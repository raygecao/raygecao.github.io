[{"categories":["整理与总结","踩坑记录"],"content":"取自《100 Go Mistakes and How to Avoid them》及实践中踩过的坑。 ","date":"2024-02-09","objectID":"/posts/go-trap/:0:0","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"问题梳理 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:0","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"代码和项目组织 变量覆盖(variable shadowing) 在内部块使用变量声明操作符:=覆盖掉外部块声明的变量。 不会更改外部变量的值，一旦跳出内部块，变量随即消亡。 应避免同一变量跨不同作用域多次声明。 func main{ a := 0 { a := 3 // 变量覆盖，如需更改外部变量a的值，需要使用=。 a++ } fmt.Println(a) // 0，内部块退出后变量值未发生变化。 } 非必要代码嵌套 应尽量减少嵌套（循环、分支）层数。 尽可能早地 return，借助调转判断(flip the condition)减少嵌套层数。 借助 return、continue、break 等避免不必要的 else 子句。 init() 函数的顺序 包初始化后，先执行常量与全局变量的声明，再执行 init() 函数。 不同包的 init() 函数的执行顺序与包依赖顺序一致。 同一文件下 init() 函数按定义顺序执行，同包不同文件下的执行顺序存在不确定性。 我们的代码逻辑不应该依赖 init() 的顺序，避免调整包依赖关系导致非预期的行为。 init() 可用于引入副作用，一般通过引入匿名包完成，如 db driver、pprof。 init() 函数存在错误处理受限、单元测试受限、状态依赖全局变量等问题。 Getters 与 Setters 不是 Go 语言的惯用法。 可以进行一些封装，如读时转换、写时校验、并发控制等行为。 避免接口污染 接口污染指代码中采用了不必要的抽象，使其难以理解。 接口抽象会增加单元测试的难度。 接口越大抽象性越差，越难以复用，可以适当组合细粒度的接口创建更高级的抽象。 接口应该被无意发现，而不应该被有意创建。我们在没有明确的原因时不应创建接口，除非我们确定需要它。 一些使用接口抽象的行为可以考虑使用泛型替代以提升可读性。 接口使用的限制 避免将接口定义在生产方，将是否抽象的决定权交给使用方。 尽量返回具体类型，避免限制使用方使用特定类型的抽象。 尽量避免使用any接口，因其可读性差，并且无法在编译时暴露出错误。 泛型 可以自定义接口限制泛型参数类型，如 compariable，如果定义类型是基础类型且包含~前缀，表明其支持底层为特定类型的 alias 类型。 类型参数不能用于方法参数，可以通过将类型参数应用于 receiver，或者使用反射来实现方法的泛型。 类型内嵌 类型内嵌是一种组合，而不是继承。 如果内嵌多种类型，则这些类型不应存在相同签名的方法，否则会产生冲突。 类型内嵌主要是为了简便性。如果不希望把内嵌类型的全部方法提升到顶层类型时，不要使用内嵌方式。 类型内嵌可能引入一些不符合预期的行为，如内嵌的类型声明了 JSON Marshaler 等接口，那么顶层类型将无法使用默认的 JSON 序列化行为。 函数配置传递 Go 没有定义默认函数的方法。 使用普通的结构体作为参数，默认值字段可能无法区分是用户传入了默认值还是没传入对应的字段值。 Builder 模式可以通过构建方法链传递相关的配置，但这种方法无法及时返回错误，不得不推迟校验。 函数选项方式是 Go 的惯用法，核心流程如下： 定义一个未导出的 options 结构。 定义一个处理 options 的方法类型 Option。 定义一组配置 options 的具体实现。 函数参数传入 Option 不定参数，在内部定义一个 options 并迭代执行 Option 以固定相应的配置。 type options struct { // 第一步 port *int } type Option func(options *options) error // 第二步 func WithPort(port int) Option { // 第三步 return func(options *options) error { if port \u003c 0 { return errors.New(\"port should be positive\") } options.port = \u0026port return nil } } func DoSomething(opts ...Options) error { // 第四步 var options options for _, opt := range opts{ if err := opt(\u0026options); err != nil{ return err } } // use options } 项目结构与规范 Go 提出了项目结构的标准，这提供了一个通用的布局，可以让开发者避免花很多时间组织项目，但并不是强制规范。 包名的定义应该是有意义的，尽量简短，有意义的单个小写字母单词。避免使用 utils, common等包名。 最小化导出的内容以减少包间耦合，可以通过 internal 目录或者未导出元素进行限制。 一些需要序列化的结构体字段需要定义为可导出的。 尽量避免包名与变量名冲突，否则可能因为 variable shadowing 导致相应的包不可用，可以采用 package alias 或者 rename 与包同名的变量解决。 导出的元素都应该写文档，文档以注释的方式定义，但与内部注释的内容完全不同。函数文档应记录做什么，而注释记录怎么做。 包文档通常在 doc.go 里添加。 需要采用 gofmt, goimports, golint, gofumt 等方式进行代码格式化与规则检查。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:1","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"数据类型 基础数据类型 0开头的数字会被解析为八进制，通常采用 0o 前缀显式定义八进制。 需要考虑整数溢出的行为，避免出现两个大正数相加溢出得到了个负数，可以使用 math/big 处理大数操作。 浮点数使用 == 进行比较通常会不精确，通常通过判断两个浮点的 delta 值是否足够小以判定两个浮点数是否相等，如 assert.InDelta(t, math.Pi, 22/7.0, 0.01)。 切片 切片由一个指向底层数组的指针、长度和容量三个结构组成。 如果切片 append 元素时，底层数组已经满了，Go 会创建另一个具有二倍容量的数组，并将已经存在的元素拷贝过去再插入新元素。 避免低效率切片操作： 切片底层数组更换时，会引入拷贝大量元素和 GC 的开销，应尽量减少切片扩容的操作。 切片长度已知时，可以在初始化时显式声明数组长度 make([]int, 1000) 初始化切片。 如果知道大致长度时，可以显式给个大致的容量值 make([]int, 0, 1000)，避免出现频繁的扩容操作。 空切片与 nil 并不相等，但其容量都为0，为避免错误的判断条件，尽量保证生产方返回 nil，使用方使用切片长度 len(s) == 0 进行判断。 子切片与原切片共享底层数组，可以采用完整切片表达式 s[low:high:max] 来声明切片的容量。 可以采用 copy() 或者通过 append([]int{}, s...) 方式进行切片元素深拷贝。 经常踩的坑： 子切片 append 元素覆盖，即子切片的末尾未触及底层数组的边界时，往子切片 append 元素时，会覆盖底层数组对应的元素，从而影响其他切片的值。在刷题时进行 dfs 传入切片回溯时会经常出现此类问题，可以采用切片深拷贝的方式解决。 copy 方法拷贝元素数目是 src 与 dst 的最小的长度，且 dst 是第一个入参，src 是第二个入参。 切片会导致底层数组不会被 GC，导致内存一直不会被释放。可以通过拷贝元素的方式替代短切片。 切片作为入参时采用的是值拷贝，即如果给切片变量赋新值则不会改变原切片；如果给切片中的元素赋新值，相当于改变底层数组中的元素值，会对原切片造成影响。 切片并行 append 会产生 data race，并行给不同 index 赋值不会产生 data race。 Map Map 底层由 buckets 组成，且 bucket 数目不会减少。 Key/Value 的值如果超过 128 字节后，Go 会存储一个指向 key/Value 的指针。 Map 是并发不安全的，即便操作不同的 key 也会产生 data race。 写 map 前需要保证 map 已被初始化，为了避免扩容产生的拷贝开销，建议初始化 map 时指定容量，如 make(map[int]int, 10)。 值为 struct{} 类型的默认值不分配内存空间，如果想要实现 Set 的功能时，建议采用空结构做为值类型。 Map 迭代的顺序是不确定的。 避免在迭代过程中向 map 中插入元素，这会为后期的迭代引入不确定性，如果希望边迭代边插入，可以考虑拷贝一个新的 map，确保迭代和插入是两个完全隔离的 map。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:2","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"控制结构 循环 range 循环是对目标进行拷贝，如果对 range 的元素进行修改，则只会更新拷贝而不是元素本身。如果希望更改生效的话，需要保证目标值是指针类型，或者采用索引复制的方式进行更新。 range 只会在 loop 前执行一次表达式，并对其进行深拷贝迭代。不要在 range loop 中为切片添加元素，避免产生非预期的行为。 循环变量是个指针，在迭代过程中会对元素迭代地进行引用。在并发访问循环变量的闭包时会访问到不确定的元素。解决办法是采用变量拷贝或者将闭包改造成函数传参。 loop 时不要使用 defer 命令，因为在 loop 结束后 defer 并不会执行，只有在函数返回时才会执行，long-running loop 会导致严重的内存泄露。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:3","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"字符串 字符串是不可变的，所有对字符串的更新操作都是拷贝，而不是原地更新。 一个 rune 是一个 unicode 码字。 len(str) 返回的是 bytes 的长度，而不是 runes 的长度。 使用 index 循环( for i:=0; i\u003clen(s); i++ )迭代的是每个 byte，使用 range 循环( for i := range s )迭代的是每个 rune。 TrimLeft/TrimRight 移除所有与给定字符串匹配到的每个字符；TrimPrefix/TrimSuffix 移除的是匹配的完整字符串。 使用 + 连接字符串的效率很低，可以采用 strings.Builder 进行连接以提升性能。 通过显式类型转换对 []byte 与 string 转换的效率很低，可以采用 bytes 包直接操作 []byte（所有strings 包提供的函数都有），减少二者的相互转换。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:4","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"函数和方法 值接收者和指针接收者 对于值接收者，Go 拷贝值将其传到方法中，对对象的修改只在方法内生效，原始对象值不变。 对于指针接收者，Go 拷贝指针到方法中，对接收者的修改会影响到原始对象。 使用指针接收者的场景 需要修改接收者的方法。 接收者包含无法被拷贝的字段。 对象 size 很大，使用指针接收者可以避免庞大的拷贝开销。 使用值接收者的场景 要求接收者不可变。 接收者类型是 map/function/channel。 接收者是基础类型(int/string/float64)。 当不确定使用那种方式时，无脑选择指针接收者。 返回值参数 核心目的是增强代码可读性。 声明返回类型参数时，可以返回裸的 return，但这不是使用返回值参数的理由，也不推荐这种方式返回。 使用裸 return 时，要注意 variable shadowing 的问题。 方法是函数的一个语法糖，接收者作为函数的第一个参数。 func Bar(foo *Foo) string { return \"bar\" } // 上述函数的语法糖。 func (foo *Foo) Bar() string { return \"bar\" } 空指针与空接口不一样，空指针具有类型值，因此不等于 nil，空指针可以作为接收者正常调用。 参数传递文件名是一个糟糕的方式，因为其难以进行单测，且可复用性差。建议采用 io.Reader, io.Writer替换。 defer 参数执行时机 defer 参数为值变量时，会立即执行，不会感知函数返回前的赋值。 defer 参数为闭包时，在函数执行完成前的赋值会被 defer 感知到。 defer 参数为函数时，实参变量会立即执行，不会感知函数返回前的赋值。 func main() { i := 0 defer fmt.Println(\"传递变量：\", i) // 传递变量： 0 defer func(){ fmt.Println(\"传递闭包：\", i) // 传递闭包： 1 }() defer func(i int){ fmt.Println(\"函数实参：\", i) // 传递实参： 0 }(i) i++ } ","date":"2024-02-09","objectID":"/posts/go-trap/:1:5","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"错误管理 Error wrapping 常见用例 添加额外的上下文。 标记一个 error 为特定的错误。 Go 1.13 可以使用 %w 标记 wrapping。 引入潜在的耦合，因其允许使用方接入到 source error。 错误判断 判断错误值，使用 == 进行精准判断，如if err == io.EOF。 递归判断错误类型，使用 error.As 递归地 unwrap error，判断某个 error type 是否出现在错误链上。其第二个参数一定需要是指针类型，否则会 panic。 递归判断错误值，使用error.Is 递归地 unwrap error，判断某个 error value 是否出现在错误链上。 本质上打印 error 日志也是处理错误的一种方式，因此返回错误与打印日志二者只选其一，避免对错误的多次处理（打印）。 为增强代码可读性，不要直接忽略返回的错误，而是将其显式分配到一个空标识符上(_)，以表示我们不关心这个错误。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:6","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"并发控制 并发与并行 并发是同时处理多个任务，但同一时间最多只做一件，通过调整任务的顺序提升多任务处理效率。 并行是同时做多件事，需要利用多个核。 Go scheduling Goroutine 是应用级线程，初始大小为 2KB，并按需扩展。 Goroutine 调度由 Go runtime 完成，不需要切换内核线程上下文。 GMP 原理 全局队列、work stealing。 P 的数量与 GOMAXPROCS 一致，M 的数量一般更多一些。 Go 1.14之后支持了抢占式调度，即使某个 goroutine 一直处于非阻塞状态，也不会饿死其他 goroutine。 由于 goroutine 切换也存在开销，因此一些本身开销较小的逻辑并发处理后反而会更慢。 Mutex vs Channel Go 提倡通过通信来共享内存，但这并不意味着鼓励用 channel 替代 mutex。 如果想共享状态，建议使用 mutex 排他地接入到共享资源上。 如果想使用信号同步或者并发顺序处理，建议使用 channel 进行通信。 Data race vs race condition Data race 多个 goroutine 同时接入到一个相同的资源，并且其中至少存在一个写操作。 可以通过原子操作、使用 mutex 排他接入或者使用 channel 通信机制进行同步接入。 Race condition 行为受事件发生的顺序影响而不受控，存在不确定性。 Context 机制适用于并发控制，级联控制，超时控制，跨边界传值等场景。 并发控制在实践中注意的问题 当 goroutine 开始时，需要明确何时他会停止，以避免其持有的资源发生泄露。 避免在 goroutine 中使用循环变量，可以通过以下两种方式获取循环值。 创建一个本地变量。 使用函数传参替代闭包。 select 并不像 switch 一样按顺序匹配子句。select 会随机从满足条件的 case 中挑选一个操作，这主要考虑避免饿死其他 channel 的场景。 channel 用作信号同步时，建议使用 chan struct{} 传递空结构体，以减少内存分配。 nil channel 可以永远不会被 select 选中，因此在同时操作多个 channel 时，如果其中某个已关闭，可以将其赋值 nil 以移除对应 case。 buffered channel 用处十分有限，最常用的是工作池与并发限速两个场景。同步一般使用 unbuffered channel。 切片 append, map 并发读写会引起 data race。 sync.WaitGroup 用作同步控制 不应在 goroutine 里面执行 Add(1)，避免提前结束。建议使用以下两种方式： goroutine 数目预先可知时，建议在 for 循环前执行 Add(N)。 goroutine 数目未知时，建议在 for 循环内，goroutine 前执行 Add(1)。 Done() 应该在 goroutine 退出前执行，推荐使用 defer。 如果希望在并发时，某个 goroutine 失败后终止此次并发过程，建议使用 errgroup 尽早终止。 如果希望向多个 channel 重复发送通知，可以使用 sync.Cond，不过一般这种场景可以被 context 机制覆盖。 channel 关闭方应为唯一的发送方，不应由接收方关闭。当有多个发送方时，需要同步机制以决定何时由谁来关闭。 永远不要复制 sync 类型值，避免复制出多个实例破坏排他性。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:7","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"标准库 time time.Duration 是 int64 类型，其值的单位为ns。在使用时，用包内自带的度量常量避免出错（如 time.Seconds * 2）。 time.After 创建的资源会在超时后自动释放，但在超时前，所占用的内存无法释放。要避免在循环中使用引起大规模内存泄露。 time.Timer 提供了Reset(time.Duration) 方法重置超时时间，可以解决 time.After 潜在的内存泄露问题。其底层实现也利用了 time.After。 json 类型嵌套时，当内嵌的类型实现自己的 json.Marshaler 接口时（如 time.Time），会提升到父类型上，从而使得父类型不满足默认的序列化规则。 值为 any 类型的 map 在 json.Unmarshal() 时会按 json 默认的数据类型反序列化，而数字在 json 中都为浮点数类型，所以对应字段 any 所涵盖的实际类型为 float64。 sql sql.Open() 不保证会建立连接。对于某些 driver 而言，仅创建客户端句柄，真正连接会在使用时延迟建立。如果想确认数据库是 reachable 的， 可以在 sql.Open() 生成的 *sql.DB 上调用 Ping() 方法。 *sql.DB 并不表示单个的数据库连接，而是个连接池。建议对连接池参数进行配置，以提升性能： SetMaxOpenConns：设置最大打开连接数。在生产环境下需要设置，以确保底层数据库能处理得过来。 SetMaxIdleConns：设置最大空闲连接数，默认值为2。需要根据请求数据库的并发量调整此配置。 SetConnMaxIdleTime：设置连接最大空闲时间。应用面临大量突发请求时，应将其配置较小的值，避免大量空闲连接驻留在内存中。 SetConnMaxLifetime：设置连接最大打开时间。当连接到负载均衡器时，需要设置生存时间，以期对负载进行再均衡。 如果 SQL 声明需要重复执行，应该使用 prepared statement (db.Prepare) 以提升效率与安全性。 在迭代获取 sql 结果时，需要判断迭代过程是否发生了错误，通过检查row.Err()是否为空来判断。 sql.Rows 使用完毕后需要显式 Close 掉，避免连接泄露而无法放回连接池中。 http http.Response.Body 在使用完毕后需要显式 Close 掉，避免内存泄露。 http.Error 执行后并不会终止后续处理，因此需要手动添加 return。 不应使用默认的 http client，因其没有设置任何 timeout。 Http client 与 server 所支持的 timeout 如下: Http client timeout 分类 Http server timeout 分类 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:8","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"测试 测试金字塔自底向上为单元测试、集成测试、e2e测试。 可以使用 build flag（如 //go:build integration）筛选出执行哪些测试。指定 tag 时，声明对应 build tag 的测试文件及没有声明 build tag 的文件会被执行。 可以使用 t.Skip() 跳过某些特定的测试，其粒度为函数级，而 build flag 的粒度是文件级。 -race 参数可以检测出函数是否存在 data race，但其性能开销较大，应尽在本地测试或者在 CI 中添加。 对于不存在依赖关系的测试函数，可以使用 t.Parallel() 并发执行提升效率，执行顺序为先依序执行顺序测试，再执行并发测试。 单元测试的规范是表格驱动。 可以借助 httptest 包对 http server 进行模拟测试；借助 iotest 对 IO 进行模拟测试。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:9","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"优化 大多数场景下，代码可读性强要比优化后更复杂、不易理解的代码更好。 按重要性排序：准确性 \u003e 清晰性 \u003e 简洁性 \u003e 高性能。 CPU cache 主要包括 L1、L2、L3 三级，分别在逻辑核内独享、在物理核间共享、在 CPU 间共享。 Cache line 优化使得每次读取变量时会读取一整块，主要利用了局部性思想： 空间局部性：附近的内存位置会被访问到，如数组访问。 时间局部性：相同内存位置会被重复访问，如变量访问。 二维数组行优先遍历比列优先遍历性能好，可以使用空间局部性进行解释。 结构体字段未对齐时会引入 padding，占用额外的内存空间，建议结构体字段按照类型大小进行降序定义以实现数据对齐。 堆上内存分配的开销较大，需要通过 GC 来释放空间；栈可以自动清理，是单个 goroutine 独有的本地空间。要尽量避免在堆上分配内存。 逃逸分析是指编译器确定变量分配到堆上还是栈上。常见的分配到堆上的 case 包括： 函数返回指针，指向其内部的变量（sharing up）。 全局变量。 发送到 channel 上的指针。 尺寸过大的本地变量。 尺寸未知的本地变量，如 s := make([]int, n)。 切片 append 触发底层数组重新分配。 io.Reader 传入切片而不是返回切片，目的之一是避免非必要逃逸到堆上的情况。 如果需要频繁分配相同类型的对象，建议使用 sync.Pool 节省重建对象的开销。 内联发生在复杂度低的函数上，可以移除函数调用的开销，允许编译器做更深层次的优化。 ","date":"2024-02-09","objectID":"/posts/go-trap/:1:10","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"后向兼容性 避免产生 breaking change 的宗旨：只添加，不修改，不删除。 函数的兼容性 函数/方法签名增加可变参数可以保证函数/方法调用兼容性，但无法保证后向兼容。如函数作为另一个函数的参数，方法实现了某个接口等。 如果函数希望支持 context，需要新增一个带 context 参数的函数，并将相关逻辑移到新函数里，原函数调用新函数避免代码重复。如db.Query()/db.QueryContext()。 如果函数预期未来会添加很多配置，可以使用如下方式： 使用配置结构体定义函数。如func Dial(network, addr string, config *Config) (*Conn, error)。 使用options可变参数定义函数。如func Dial(network, addr string, opts ...Option)(*Conn, error)。 改造成配置结构体的方法。如func (*ListenConfig) Listen(ctx context.Context, network, address string) (Listener, error) 接口的兼容性 如果不想让别人实现你的接口，可以在接口中添加一个未导出的方法。 如果接口的定义方希望新增一个方法，则必须创建一个新的接口，避免在原接口上直接添加，破坏兼容性。 如果接口的使用方接口希望新增一个方法，可以使用类型断言的方式判断传入接口的具体类型是否扩展了新方法。如 package tar type Reader struct { r io.Reader } func (r *Reader) Read(b []byte) (int, error) { if rs, ok := r.r.(io.Seeker); ok { // 判断 Reader 是否实现了 io.Seeker 接口。 // Use more efficient rs.Seek. } // Use less efficient r.r.Read. } 结构体兼容性 对于导出的结构体，添加新字段和删除未导出的字段一般不会破坏兼容性。 在一个可比较的结构体中，添加一个不可比较的字段会破坏某些场景的兼容性，如使用==判等，结构体值用作 map 的 key。 为了使定义的结构体不可比较，以避免上述的不兼容性，可以嵌入一个函数字段，如 type doNotCompare [0]func() type Point struct { doNotCompare X int Y int } ","date":"2024-02-09","objectID":"/posts/go-trap/:1:11","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"实践反思 ","date":"2024-02-09","objectID":"/posts/go-trap/:2:0","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"init函数与全局变量 背景描述 命令行工具希望添加一个调试 flag 来开启 http 嗅探中间件，以在 RoundTrip 中打印 request 与 response 信息。 具体实现 命令行工具库 cobra 通过 init() 函数将用户输入的 flag 绑定全局变量中。而我们试图使用该变量开关创建全局 http.Client。所有的 http 请求使用该 client 发起。 问题及原因 该 flag 未生效，无法控制 RoundTrip 的类型，因为 http client 的初始化早于 init() 函数，client 在变量绑定前就已完成初始化，因此使用的是该 flag 的默认值。 解决方法 将获取该 http client 的方式由全局变量改成函数获取，在请求发起时可以确保init()执行完毕，flag 成功绑定，可以正常控制是否开启嗅探中间件。 ","date":"2024-02-09","objectID":"/posts/go-trap/:2:1","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"浮点数 Quota 调度 背景描述 在容器资源调度时，资源 Quota 粒度为浮点数。 具体实现 判断剩余资源是否大于申请资源，如果大于则进行资源分配并对申请资源进行扣除，如果小于则分配失败。 type Resource struct { Quota float64 } leftResource := getLeftResource() // 获取剩余可用资源 if leftResource.Quota \u003c request.Quota { // 判断剩余 quota 是否够分配 return fmt.Errorf(\"no quota left\") } leftResource.Cost(request.Quota) // 扣除申请的 quota 问题及原因 当资源理论上恰好够完全分配时，实际调度中存在调度到最后一个容器时资源不足的情况。比如总 quota 为1，申请的资源为 50 container * 0.02 quota。 由于浮点数存储及操作都只是近似的，因此在做严格比较时，会出现非预期的情况，即调度到容器时，剩余的 quota 略小于申请的 quota。 浮点数非精准计算举例如下： func main() { a := 0.1 b := 0.2 fmt.Println(a+b) // 0.30000000000000004 fmt.Println(0.1 + 0.2) // 0.3 } 解决方法 方法一：使用近似比较替换严格比较，确保误差小于 quota 分配的分度值。 // 定义浮点数比较方法 const float64EqualityThreshold = 1e-4 // 调度的分度值为 0.001，此处只要比幅度值小即可。 func Equal(a, b float64) bool { return math.Abs(a-b) \u003c= float64EqualityThreshold } func Greater(a, b float64) bool { delta := a - b return delta \u003e= float64EqualityThreshold } // ... if Greater(request.Quota, leftResource.Quota) { // 判断剩余 quota 是否够分配 return fmt.Errorf(\"no quota left\") } 方法二：将浮点数字段改造成 Quantity 类型。 Quantity 提供了浮点数比较的方法，同时也支持解析基本量级单位，既增强了可读性，又缩减了浮点数使用的场景。如 0.01 =\u003e 10m。 考虑到重构会带来兼容性上的问题，我们实际采用了方法一。但作为调度模块而言，使用 Quantity 更科学，更通用。 ","date":"2024-02-09","objectID":"/posts/go-trap/:2:2","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"Track Map 爆内存 背景描述 对于视频流人脸检测，我们对每一帧进行检测，并将同一人脸框加入到一个 track 中以进行后续择优。 具体实现 我们对于检测出的每个人脸，构造一个 track，使用 map 存储所有的track。生成一个 uuid 作为 key 进行索引。当该物体离开视频后，处理后删除掉相应的 track。 tracks := make(map[string]*Track) // 创建 track if dect.ID == \"\" { uuid := genUUID() dect.ID = uuid Track := NewTrack(dect) mu.Lock() tracks[uuid] = tracks mu.Unlock() } // Omit Adding Face to Track // Omit Processing Track // 删除 Track if dect.Done(){ delete(tracks, dect.ID) } 问题及原因 应用运行一定周期后出现 OOM，且反复出现。 由于 map 删除 keys 后并不会完全释放掉所占内存，而目标数目相对比较庞大，track 的创建与删除的规模较大，引起了内存泄露。 解决方法 定期拷贝 track map，频繁创建删除的 map 会被 GC 掉。 t := time.NewTicker(1 * time.Hour) defer t.Stop() go func () { for { \u003c- t.C mu.Lock() m := make(map[string]*Track) for k, v := range tracks{ m[k] = v } tracks = m mu.UnLock() } }() ","date":"2024-02-09","objectID":"/posts/go-trap/:2:3","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"Protobuf 客户端与服务端的兼容性 背景描述 客户端与服务端使用 gprc gateway 进行通信。 具体实现 客户端使用 protojson 进行反序列化到 proto 结构体中。 import jsonpb \"google.golang.org/protobuf/encoding/protojson\" // .... err := jsonpb.Unmarshal(result, \u0026resp) // .... 问题及原因 服务端在 proto 中新增一个字段后上线，旧版本客户端再请求时出现不兼容，报错 proto: (line X:Y): unknown field \"someField\"。 此序列化方法在校验时，发现 json 中存在，但 proto 中不存在的字段会抛出 unknwon field 异常，除非显式指定 DiscardUnknown Options 默认的 Unmarshaler 居然会破坏后向兼容性，实乃大坑。 解决方法 添加 DiscardUnknown 选项。 import jsonpb \"google.golang.org/protobuf/encoding/protojson\" // .... m := jsonpb.UnmarshalOptions{DiscardUnknown: true} err = m.Unmarshal(result, \u0026resp) // .... ","date":"2024-02-09","objectID":"/posts/go-trap/:2:4","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"切片 append 覆盖 背景描述 刷算法题：求给定整数集合S的所有子集。 具体实现 使用递归处理切片的方法： func subsets(S []int) [][]int { // 省略特判逻辑 sort.Ints(S) last := len(S) - 1 subResults := subsets(S[:last]) results := subResults // 解决方案1 // results := make([][]int, len(subResults)) // for i := range subResults{ // results[i] = append(results[i], subResults[i]...) // } for _, sub := range subResults { results = append(results, append(sub, S[last])) // 解决方案2 // results = append(results, append(append([]int{}, sub...), S[last])) } // 省略排序逻辑 return results } 问题及原因 当用例为 S=[9,0,3,5,7]时，返回结果出错。 求子集算法中未通过的用例 可以看到，存在两个子集中重复出现了9，而排序后9是最后一个元素，因而初步怀疑是数组 append 覆盖的问题。 为了验证这一猜想，在 for 循环的前后打印了 subResults 的结果，最后一轮的结果如下： before [[] [0] [3] [5] [7] [0 3] [0 5] [0 7] [3 5] [3 7] [5 7] [0 3 5] [0 3 7] [0 5 7] [3 5 7] [0 3 5 7]] after [[] [0] [3] [5] [7] [0 3] [0 5] [0 7] [3 5] [3 7] [5 7] [0 3 5] [0 3 7] [0 5 7] [3 5 7] [0 3 5 9]] 显然 append 执行后，将 [0,3,5,7] 最后一个元素覆盖成了9。 回到代码中，我们看到 subResults 复用了上一轮递归返回的切片，并在循环中对这些切片进行了 append 操作。当遍历到 [0,3,5] 时，此时底层数组为 [0,3,5,7]，对前者 append 9 时，由于 length \u003c capaticy，7会被9覆盖掉。本质原因是上一轮迭代的结果与本次迭代中存在复用了同一底层数组的切片。 解决方法 为了避免 append 覆盖问题，需要避免上次迭代的切片与本次迭代的切片复用同一底层数组。 代码段中的注释给出了两种解决方案：解决方案1将上一轮迭代的切片进行深拷贝；解决方案2将本轮迭代的切片进行深拷贝。两个方案都使得某一侧的切片结果分配到另一个底层数组中。 ","date":"2024-02-09","objectID":"/posts/go-trap/:2:5","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"io.Reader 读取不完整。 背景描述 使用 oras 将多个文件组织成 OCI Artifacts 传到 OCI Distribution 上。 具体实现 在从 registry 上拉取 image manifest 时，会先根据 image index 获取到 manifest descriptor，这个 descriptor 记录着 manifest 的位置及大小。 创建指定大小的切片，并传入到 http.Response.Body.Read() 中以获取 image manifest。 问题及原因 当 image manifest 过大时，从获取切片中进行 unmarshal 解析时，报错 invalid character '\\x00' in string literal，即未获取到完整的 manifest。 因为在 Read() 时不保证一次将所有的内容都读到切片中，当 Reader 内容过大无法一次读出时，会出现一次读取的内容不完整的现象。 解决方法 使用 io.ReadFull() 替代 Reader.Read()，以保证可以将 Reader 中相应尺寸的内容完整读到切片里。具体解决方法为：https://github.com/oras-project/oras-go/pull/142 ","date":"2024-02-09","objectID":"/posts/go-trap/:2:6","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["整理与总结","踩坑记录"],"content":"引用 《100 Go Mistakes and How to Avoid them》 Keeping Your Modules Compatible ","date":"2024-02-09","objectID":"/posts/go-trap/:3:0","tags":["go"],"title":"Golang 陷阱盘算","uri":"/posts/go-trap/"},{"categories":["工作项目"],"content":"为提升调度任务的规模，突破基于 etcd 存储状态进行调度的性能瓶颈，采用内存任务管理对调度器进行重构。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:0:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"背景 任务调度器(Scheduler)是大规模业务系统的重要组成部分，其设计对系统的吞吐量、任务处理耗时、系统的稳定性等方面有着重要影响。 通常，Scheduler 需要考虑系统资源限制、任务优先级、任务抢占、调度公平性、调度延时等因素，需要针对不同业务系统作出权衡。 对于分布式任务调度系统来讲，还需要考虑高可用、调度数据一致性、崩溃恢复等问题。 我们的业务系统是图像识别系统，经历了如下发展阶段： 业务类型从最初的视频流人脸解析扩展到图片流人脸解析，扩展至基于视频流和图片流的人体、车辆解析，再扩展至脸人车绑定解析。 接入规模从单机单卡16路视频流到单机一百路图片流接入，再到单机8卡千路图片流接入，又到集群万路图片流接入。 业务场景从原始的在线视频流解析到离线视频流的解析，再到离线视频流切片解析。 业务类型和规模的扩张要求我们对现有的 Scheduler 进行重构。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:1:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"原始任务调度器 我们调度的维度主要是 GPU 资源。在最初任务量少，类型单一的场景下，为了使系统更轻量，我们采用了基于 etcd 的调度策略。之所以采用 etcd，是因为我们使用 etcd 用作服务发现及分布式选主，早期的目标是在不引入额外组件的前提下完成调度。 原始任务调度架构 具体策略如下： 解析服务(Worker)在启动前，将其信息注册到 etcd 中，主要包含其所包含的GPU quota。 Worker 启动一个异步进程，通过 etcd election.Campaign() 选择一个主 Worker 执行调度程序。 调度程序定期从 etcd 中拉取 Worker 列表与任务列表进行比较，结合节点状态、任务状态对任务进行资源分配、任务转移等调度操作。 Worker 启动解析处理进程，定期从 etcd 中拉取分配给自身但未处理的任务进行处理，并对执行状态进行更新。 整个调度过程与 Kube-scheduler Pod 调度方式有一些类似之处，但存在以下差别： Worker 与任务的获取均采用拉取全量的方式，未采用 watch 机制。 没有单独的调度组件，调度程序与解析程序在一个进程内。 缺少缓存机制与 patch 更新机制，状态更新都是更新完整结构到etcd中。 随着接入路数规模扩大、解析类别增多，旧调度器的痛点突出，下面由重至轻列举一些： 频繁的状态更新导致 etcd 性能大幅恶化，出现频繁的连接超时，影响到服务发现。 参与 campaign 的节点过多时，当 etcd 性能不稳定时会偶发选主卡住的问题。 调度的延时过大。 未考虑网络中断的问题，在 etcd 不稳定的情况下，调度结果不一致（资源超配、误扣、同一任务被多个 worker 执行）的问题频现。 调度代码难以维护且扩展性差，新类型解析任务出现时，不得不将调度核心逻辑拷一份进行修改，维护多份类似的代码。 尽管期间我们通过很多方法缓解上述问题（如调整重试间隔，增加外部gc等），但随着路数进一步增加，系统性能明显变差，因此需要对任务调度架构进行重构。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:2:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"重构目标 为解决原有 Scheduler 的痛点，我们提出如下的重构目标： 彻底缓解 etcd 压力，避免对选主、服务注册发现产生任何影响。 调度路数显著提升，支持百台服务器集群十万路图片流的调度。 Scheduler 自身的扩展性要大幅增强，新增解析类型时可以在零学习成本下接入。 Scheduler 高可用、崩溃恢复要得以保证。 Scheduler 与 Worker 的数据一致性得以保证，调度需满足业务场景需求（优先级、抢占、排队、重试等）。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:3:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"重构实现 我们主要采用以下几种方式对 Scheduler 进行重构： 任务元数据的存储从 etcd 转移到 postgresql 中以缓解 etcd 的压力。 Worker 元数据依然保留在 etcd 中，但通过 watch 机制获取全量结果并缓存在内存中。 状态数据仅维护在内存中。 Scheduler 改造成一个独立的微服务，通过 campaign 实现高可用。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"核心流程 重构调度架构 Worker 向 etcd 注册 GPU 资源及可处理任务的类型。 Scheduler 接收来自用户创建的任务，并将任务元数据进行落盘。 Scheduler 通过任务队列的方式在内存管理状态，找出所有未分配的任务，并结合 Worker 剩余资源进行调度，更新状态机产生相应的调度 action（如 Create/Delete）。 Scheduler 批量执行调度 action 以提升并发度。 Scheduler 定期向所有 Worker 发起心跳请求，一方面确认节点是否正常工作，另一方面获取分配到 Worker 上的任务状态以更新状态机。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:1","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"元数据与状态 任务的数据结构可以分为元数据与状态： 任务元数据：包括调度配置与任务处理配置，元数据需全部持久化。 调度配置：如占用资源、任务类型、任务优先级、restartPolicy 等。 任务处理配置：如视频流/图片流的 url、告警阈值、布控范围等。 任务状态：包括调度状态与处理状态，离线任务的状态需要持久化。 调度状态：包括任务被分配的 Worker 标识以及对应的资源标识（如 GPU index）。 执行状态：任务当前的执行状态，包括 PENDING、RUNNING、SUCCESS、ERROR 等。 根据不同的任务执行状态，Scheduler 分别维护了 PENDING、RUNNING、ERROR 三个任务队列配合以下状态机实现任务调度与管理。 任务执行状态机 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:2","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"离线任务与在线任务 根据业务需要，实时任务和离线任务处理存在如下差异： 实时任务无法调整倍速，而离线任务可以调整倍速解析，倍速会映射到 GPU 资源占用上。 实时任务的优先级高于离线任务，实时任务可以抢占离线任务，离线任务无法抢占实时任务与离线任务。 实时任务支持暂停解析的功能，离线任务不支持暂停的功能，离线任务执行状态中 SUCCESS 与 ERROR 状态为终态，不会被重试拉起。 实时任务一旦失败后会被退避重新拉起。 类比 K8s workload，离线任务相当于 Job，实时任务相当于 restartPolicy=None 的 Deployment。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:3","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"Servant Facade 我们使用 Servant Facade 封装了复杂的任务管理流程，增强调度系统的可扩展性，便于接入新类型的任务。 每个 Worker 只需要传入 runnerFactory 到 Servant Facade 并注册到 GRPC Server 上即可实现对应的任务管理功能。 Servant Facade 主要封装了以下方法： type AbstractServant interface { // CreateTask 创建一个任务。 CreateTask(context.Context, *CreateTaskRequest) (*Task, error) // DeleteTask 删除一个任务。 DeleteTask(context.Context, *DeleteTaskRequest) (*empty.Empty, error) // ListTaskStatus 列出所有task的status. // Scheduler通过周期性调用该方法实现心跳及状态反馈。 ListTaskStatus(context.Context, *ListTaskStatusRequest) (*ListTaskStatusResponse, error) // Register 注册 servant 处理的任务类型，其包含的 GPU 资源信息以及构建 runner 工厂方法。 Register(taskType TaskType, GPUResources []*GPUResource, f RunnerFactoryFunc) // Start 启动 runner 并接收请求。 Start() error } Servant Facade 相当于 Scheduler 向 Worker 进程中注入的 Sidecar，承接来自 Scheduler 的全部流量并将相应的任务 hook 到 runner 中。这解耦了 Worker 任务管理和任务执行的逻辑，对 Worker 开发者屏蔽了复杂的任务管理流程。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:4","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"故障转移 Scheduler 会通过 watch 的方式发现 Worker 节点的上下线，如果 Worker 因崩溃等原因使得注册到 etcd session 中断，Scheduler 会感知到 Worker 节点的下线，从而将分配到该节点的任务放入 ERROR 队列中等待重试。 如果 Worker 与 Scheduler 产生了网络分区，此时 etcd session 还在，调度节点无法通过服务发现感知到节点异常。这时 Scheduler 会通过连续几次心跳超时来将该 Worker 标记下线，该 Worker 上分配的任务放入 ERROR 队列里。 当某个节点连续多次分配任务失败时，逐步降低其调度权重，并在分配成功时恢复。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:5","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"调度一致性 在分布式系统中，上述网络分区导致的故障转移会导致数据处理出现不一致的情况，最经典的是 At-Least-Once vs At-Most-Once。 当出现网络分区导致 Scheduler 与 Worker 断连时，Scheduler 会将任务从该 Worker 中转移到其他 Worker 中。由于之前的 Worker 还正常工作，因此在此期间会出现重复解析(At-Least-Once)的情况。 同样的网络分区场景下，如果 Worker 也感知到与 Scheduler 断连（比如定期检查是否收到心跳），并在断连后停止所有正在处理的任务，这会由于以下两种情况而造成至多解析一次(At-Most-Once)： Worker 早于 Scheduler 感知到断连，因此在 Scheduler 重新调度任务前便终止了任务。 可用资源不够，导致部分任务无法被调度处理，这些任务会变成等待调度的状态。 由于我们项目很多是单 Worker 现场，因此我们选择了 At-Least-Once 策略。该策略使得 Worker 仅需要被 Scheduler 驱动，简化了系统的复杂度。 无论是上述哪个策略，当 Worker 网络恢复后，调度结果总能恢复到 Exactly-Once 的情况。但由于 Runner 的设计是高度自由的，数据不一致仍然无法完全避免。 因此 Scheduler 会起一个异步 matcher 线程定期地对 Scheduler 维护的 RUNNING Queue 与 Worker 中收集到的任务列表进行比较，以对以下不一致的问题进行校正： 某个任务在 Scheduler 中，但不在相应的 Worker 任务列表中。 某个任务不在 Scheduler 中，但在某个 Worker 任务列表中。 某个任务在多个 Worker 列表中出现。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:6","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"高可用 Scheduler 本身无状态，采用冷备方式通过 election.Campaign 进行故障切换。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:7","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"崩溃恢复 Scheduler 重启/切主时会根据 postgresql 中存储的元数据恢复任务，并通过发起一次心跳收集各任务的分配状态及运行时状态，结合二者构建出任务队列完成恢复。 为保证恢复数据的一致性，恢复过程中无法接受新的请求。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:4:8","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目"],"content":"重构效果 调度路数大幅提升，调度延时不随调度路数增加而显著增长，mock 测试调度 QPS 能达到十万量级。 etcd 后续未出现异常，解决计算节点（选主）过多时，依赖 etcd campaign 导致的选主卡住问题。 ","date":"2024-01-27","objectID":"/posts/task-scheduler/:5:0","tags":["platform"],"title":"任务调度器重构","uri":"/posts/task-scheduler/"},{"categories":["工作项目","前端"],"content":"Next.js + NextAuth.js + Material UI + Materio template ","date":"2024-01-07","objectID":"/posts/frontend-practice/:0:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"Next.js Next.js 是 Vercel 推出的 React Web 框架，提供全栈应用构建的快速解决方案，官方描述其包含 Routing、Rending、Data Fetching、Styling、Optimizations、Typescripts 等核心 features。下面根据我们的实践经验描述几个核心特性。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"全栈式解决方案 官方脚手架构建出的 Next.js 项目包含前后端代码，可以单独启动前/后端服务，也可以同时启动前后端服务。 后端服务使用 Node.js 编写，前端使用 JavaScript/TypeScript 编写，前后端代码在一个项目中管理，避免了前后端分开维护带来的困难。 我们之前为了解决 monorepo 前后端代码跨域访问的问题，做了大量 proxy 及 API endpoints 合并的工作。而该框架使前后端 API 在同一个域内，完美解决跨域访问的问题。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:1","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"客户端渲染 + 服务端渲染 单页应用（SPA）在 React 开发中流行了很长一段时间，此类应用组件渲染完全发生在客户端，使用上更灵活，也更加易于调试，但其存在首屏加载速度慢，不利于 SEO 等问题。 客户端渲染（CSR）模式下，客户端需要请求页面、js/css 文件、动态数据等内容进行渲染，无论是网络请求还是渲染开销都比较大，导致首屏加载速度变慢。另外客户端渲染通常挂一个空的 html 元素，将动态数据渲染到此元素中，从网页源码中无法获取到网站内容，因此不利于 SEO。 服务端渲染（SSR）很好地解决了上述两个问题，服务端将首屏的 html 文件发给客户端，客户端直接展示就可以，这大大减少了首屏加载慢的问题。另外客户端获取到的是静态 html 文件，易于获取网页内容进行 SEO。但其也存在耗费后端资源，与前端耦合度较高的问题。所以通常情况下首屏加载选择 SSR，之后建议使用CSR。 CSR 与 SSR 在使用时要注意 JavaScript 与 Node.js 支持的库的差异，比如 React 相关库，浏览器相关 API 只能在客户端渲染使用，filesystem 等只能在服务端渲染中使用。 Next.js 将 CSR 与 SSR 巧妙地结合起来，对于静态内容（如首屏展示内容等）采用SSR，对于用户交互采用CSR。 Next.js 包含两种模式，一种是旧版本的 Page Router 模式，另一种是新版本主推的 App Router 模式。二者使用上最大的差异在于如何声明不同的渲染方式： App Router 默认都是服务端渲染，如果需要使用客户端渲染，需要显式指定 use client。 Page Router 通过 getServerSideProps 等函数声明服务端渲染所需的请求，函数外部全部为客户端渲染。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:2","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"中间件 Next.js 提供了中间件，对 HTTP Request 进行预处理/后处理。在实践中，我们使用中间件实现了后端鉴权功能，以保证所有到后端的请求都带上 Authorization Header。 另一个比较常见的使用中间件场景是认证。当用户未登录访问安全限制页面时，会跳转到相应的登录页，如 NextAuth Middleware。 Next.js 中间件最大的坑在于不支持原生的链式中间件，我们无法（至少无法很直观地）添加多个中间件以同时支持以上两个预处理功能，真乃大坑也。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:3","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"目录结构的Router Next.js 支持 App Router 和 Page Router 两种方式，用户可以任选一种进行开发。每一种定义了一组目录路由规则，相比于 react-dom 更直观易用。 除了 Page Router 前端页面路由的功能以外，API Routes 将后端 API path 按照目录结构统一了起来，进一步提升了框架的易用性。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:4","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"Proxy Next.js 支持通过 next.config.js 配置 redirects 与 rewrites 实现前端请求代理转发的能力。由于前端请求的 API 可能有一部分是由外部服务（external service）提供的，如果直接访问会出现跨域问题，将对应的 API rewrite 到相应的 external API 可以解决此问题。 使用 proxy 遇到一个坑：当 proxy 一个下载文件的 request 时，由于下载的文件会很大，proxy 不知在哪里加了个 30s timeout，无法使下载顺利完成。翻了翻配置源码找到 ExperimentalConfig.proxyTimeout，但并未按预期生效。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:1:5","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"NextAuth.js NextAuth.js 是专门为 Next.js 应用提供认证的工具，具备灵活、易用、安全等特点。后文以 Next.js 的 Page Router 为例说明 NextAuth.js 的用法。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"添加Providers 通常的认证有两种方式，一种是自行维护登录名和密码的 basic auth 方式，另一种是借助第三方认证系统的 oauth 方式。对于常用的认证方式，NextAuth.js 提供了相应的 provider，因此集成 NextAuth.js 的第一步就是根据特定的方式选择 providers，如选择支持 facebook 与 google 的第三方认证，可以通过以下方法： import NextAuth from 'next-auth' import FacebookProvider from 'next-auth/providers/facebook' import GoogleProvider from 'next-auth/providers/google' export default NextAuth({ providers: [ // OAuth authentication providers... FacebookProvider({ clientId: process.env.FACEBOOK_ID, clientSecret: process.env.FACEBOOK_SECRET }), GoogleProvider({ clientId: process.env.GOOGLE_ID, clientSecret: process.env.GOOGLE_SECRET }), ] }) 不同 provider 需要做不同的配置，常见的 provider 分为三类： Oauth Provider： 通常需要声明第三方提供的 ID 与 Secret。 Email Provider： 通常需要添加 email server。 Credentials Provider：通常需要配置用户名、密码等用户填入的信息，以及相关的验证逻辑。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:1","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"身份认证 NextAuth.js 支持客户端和服务端的身份认证。 客户端认证 客户端认证使用 useSession() 方法： // 前端组件 import { useSession, signIn, signOut } from \"next-auth/react\" export default function Component() { const { data: session } = useSession() if (session) { return ( \u003c\u003e Signed in as {session.user.email} \u003cbr /\u003e \u003cbutton onClick={() =\u003e signOut()}\u003eSign out\u003c/button\u003e \u003c/\u003e ) } return ( \u003c\u003e Not signed in \u003cbr /\u003e \u003cbutton onClick={() =\u003e signIn()}\u003eSign in\u003c/button\u003e \u003c/\u003e ) } 使用前需要在 pages/_app.tsx 中添加 SessionProvider： import { SessionProvider } from \"next-auth/react\" export default function App({ Component, pageProps: { session, ...pageProps }, }) { return ( \u003cSessionProvider session={session}\u003e \u003cComponent {...pageProps} /\u003e \u003c/SessionProvider\u003e ) ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:2","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"服务端认证 服务端认证使用 getServerSession() 方法： import { getServerSession } from \"next-auth/next\" import { authOptions } from \"./auth/[...nextauth]\" export default async (req, res) =\u003e { const session = await getServerSession(req, res, authOptions) if (session) { res.send({ content: \"This is protected content. You can access this content because you are signed in.\", }) } else { res.send({ error: \"You must be signed in to view the protected content on this page.\", }) } } 如果使用 JWT 进行认证，服务端认证也可以使用 getToken() 方法获取 token 来确认用户身份。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:3","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"前端统一认证跳转 我们应用的场景中，所有前端页面需要用户认证后才可以访问，如果每个页面加一个 useSession() 判断，则工作量太大，因此希望可以添加前端页面未认证自动跳转到登录界面的功能，上文提到了 NextAuth 中间件可以做到这件事，但我们使用了中间件来添加 Authorization Header。因此选择在 _app.tsx 添加一个强制性认证的 HOC： // pages/_app.tsx const App = (props: ExtendedAppProps) =\u003e { // ... return ( \u003cSessionProvider session={session}\u003e \u003cAuth\u003e \u003cSettingsProvider\u003e \u003cSettingsConsumer\u003e {({ settings }) =\u003e { return \u003cThemeComponent settings={settings}\u003e{getLayout(\u003cComponent {...restProps} /\u003e)}\u003c/ThemeComponent\u003e }} \u003c/SettingsConsumer\u003e \u003c/SettingsProvider\u003e \u003c/Auth\u003e \u003c/SessionProvider\u003e ) } const Auth = ({ children }: { children: React.ReactElement }) =\u003e { // if `{ required: true }` is supplied, `status` can only be \"loading\" or \"authenticated\" const { status } = useSession({ required: true }) if (status === 'loading') { return \u003cLoading/\u003e } return children } export default App ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:4","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"访问外部服务 上文说过 NextAuth.js 在前后端提供了获取解码后的 JWT 的方法，而我们的应用中，大量后端 API 在另一个应用（external backend）中提供，因此我们需要通过 Raw JWT 来与 external backend 交互以验证用户身份。 getTokn() 提供了 raw: true 属性标识获取Raw JWT。我们可以在访问由 external backend 提供的 API 时，使用 Next.js 中间件添加 Authorization Header 的预处理。 // middleware.ts import { NextResponse } from 'next/server' import type { NextRequest } from 'next/server' import { getToken } from 'next-auth/jwt' const secret = process.env.NEXTAUTH_SECRET // This function can be marked `async` if using `await` inside export async function middleware(request: NextRequest) { const jwt = await getToken({ req: request, secret: secret, raw: true }) if (!jwt) { return NextResponse.next() } const requestHeaders = new Headers(request.headers) requestHeaders.set('Authorization', `Bearer ${jwt}`) return NextResponse.next({ request: { headers: requestHeaders } }) } export const config = { matcher: '/api/v1/external/:path*' } 由于 NextAuth.js JWT 默认使用 JWE 的编码方式，与 external backend 解析的方式不一致，因此需要自定义jwt编解码： // pages/api/[...nextauth].ts export const authOptions: AuthOptions = { //... jwt: { encode: async params =\u003e { const code = await new SignJWT(params.token ?? {}) .setProtectedHeader({ alg: 'HS256' }) .setExpirationTime('1d') .sign(new TextEncoder().encode(params.secret.toString())) return code }, decode: async params =\u003e { const { token, secret } = params if (!token) return null const { payload } = await jwtVerify(token, new TextEncoder().encode(secret.toString())) return { accessToken: '', ...payload } } }, ","date":"2024-01-07","objectID":"/posts/frontend-practice/:2:5","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"Material UI Material UI 是 Google 推出的 React 组件库，其设计方式简洁扁平，相比 Ant Design 更加轻量，提供更灵活的 CSS 定义方式，适合技术平台的前端开发。 Material UI 的灵魂在于定制化，涵盖 theme，component 等方面。尤其是在定制化组件时，提供了 Base UI, styled components 等手段，为构建组件提供了便捷。 为了快速搭建起一个前端框架，模板是必不可少的，我们使用了开源的 materio 模板，该模板采用 Next.js 12 + Material UI v5 搭建，提供登录、表格、卡片、表单等页面。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:3:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"常用组件 下面记录几个技术平台常用的组件。 代码Diff组件 import React from 'react' import { createPatch } from 'diff' import * as Diff2Html from 'diff2html' import 'diff2html/bundles/css/diff2html.min.css' const CodeDiff: React.FC\u003c{ fileName: string oldStr: string newStr: string fullContent?: boolean oldHeader?: string newHeader?: string }\u003e = ({ fileName, oldStr, newStr, fullContent, oldHeader, newHeader }) =\u003e { const patch = createPatch(fileName, oldStr, newStr, oldHeader, newHeader, fullContent ? { context: 9999 } : undefined) const diffHtml = Diff2Html.html(patch, { renderNothingWhenEmpty: true, matching: 'lines' }) return \u003cdiv id='code-diff' dangerouslySetInnerHTML={{ __html: diffHtml }}\u003e\u003c/div\u003e } export default CodeDiff Markdown展示组件 import React from 'react' import Typography from '@mui/material/Typography' import { styled } from '@mui/material/styles' import Divider from '@mui/material/Divider' import Link from '@mui/material/Link' import Checkbox from '@mui/material/Checkbox' import Table from '@mui/material/Table' import TableBody from '@mui/material/TableBody' import TableCell from '@mui/material/TableCell' import TableContainer from '@mui/material/TableContainer' import TableHead from '@mui/material/TableHead' import TableRow from '@mui/material/TableRow' import ReactMarkdown from 'marked-react' import CodeBoard from './CodeBoard' import Paper from '@mui/material/Paper' const Wrapper = styled('div')({ '\u0026 .MuiTypography-h4': { marginTop: '0.2em' }, '\u0026 .MuiTypography-h5': { marginTop: '1.5em', marginBottom: '1em' }, '\u0026 .MuiTypography-h6': { marginTop: '1.5em', marginBottom: '1em' }, '\u0026 p': { marginBottom: '0.4em' }, '\u0026 .MuiDivider-root': { marginBottom: '0.4em' }, '\u0026 code': { fontSize: '0.9em' } }) const genKey = (e: unknown): string =\u003e { const ee = e as { elementId: number } return `markdown-${ee.elementId}` } const fixLang = (lang?: string): string =\u003e { if (!lang) { return 'text' } return lang.split(' ')[0] } const renderer: () =\u003e Parameters\u003ctypeof ReactMarkdown\u003e[0]['renderer'] = () =\u003e ({ code(snippet, lang) { return ( \u003cPaper key={genKey(this)} variant='outlined' sx={{ m: 1 }}\u003e \u003cCodeBoard code={snippet as string} language={fixLang(lang)} title={''} /\u003e \u003c/Paper\u003e ) }, link(href, text) { return ( \u003cLink key={genKey(this)} target='_blank' rel='noopener' href={href}\u003e {text} \u003c/Link\u003e ) }, paragraph(children) { return ( \u003cTypography key={genKey(this)} variant='body1' gutterBottom={false} paragraph={true}\u003e {children} \u003c/Typography\u003e ) }, heading(children, level) { switch (level) { case 1: return ( \u003cTypography key={genKey(this)} variant='h4' gutterBottom={true} component='h1'\u003e {children} \u003c/Typography\u003e ) case 2: return ( \u003cTypography key={genKey(this)} variant='h5' gutterBottom={true} component='h2'\u003e {children} \u003c/Typography\u003e ) case 3: return ( \u003cTypography key={genKey(this)} variant='h6' gutterBottom={true} component='h3'\u003e {children} \u003c/Typography\u003e ) case 4: return ( \u003cTypography key={genKey(this)} variant='subtitle1' gutterBottom={true} component='h4'\u003e {children} \u003c/Typography\u003e ) case 5: return ( \u003cTypography key={genKey(this)} variant='subtitle2' gutterBottom={true} component='h5'\u003e {children} \u003c/Typography\u003e ) default: return ( \u003cTypography key={genKey(this)} variant='caption' gutterBottom={true} component='h6'\u003e {children} \u003c/Typography\u003e ) } }, image(href, text, title) { return ( \u003cimg key={genKey(this)} src={href} alt={text} title={title === null ? undefined : title} style={{ maxWidth: '100%' }} /\u003e ) }, listItem(children) { return ( \u003cTypography key={genKey(this)} gutterBottom={false} variant='body1' component='li'\u003e {children} \u003c/Typography\u003e ) }, checkbox(checked) { return \u003cCheckbox key={genKey(this)} size='small' disabled={true} sx={{ p: 0 }} checked={checked as boolean} /\u003e }, table(children) { return ( \u003cTableContainer key={genKey(this)}\u003e \u003cTable size='small' sx={{ display: 'inline-block' }}\u003e {children} \u003c/Table\u003e \u003c/TableContainer\u003e ) }, tableHeader(children) { return \u003cTableHead key={genKey(this)}\u003e{children}\u003c/TableHead\u003e }, tableBody(children) { return \u003cTableBody key={genK","date":"2024-01-07","objectID":"/posts/frontend-practice/:3:1","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"踩坑记录 Next.js proxy 存在30s超时时间的设置，且不易溯源。 Next.js 除了通过环境变量文件声明，后端服务也可以直接使用系统环境变量，如果前端需要获取某个敏感变量时，可以借助后端 API Routes 传递。 NextAuth.js 自定义登录页时，Auth HoC 会引起页面循环重定向。 页面切换后，react-hook-form 需要显式执行 reset()，才能使得基于 state 的表单默认值发生变化。 不要将组件的 props 作为组件内 state 的 initial value，否则 props 变化后，state 不会发生变化。 React hook 使用之前不能有判断语句，state/props 使用上不符合预期时，优先考虑 useEffect() 能否解决。 ","date":"2024-01-07","objectID":"/posts/frontend-practice/:4:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目","前端"],"content":"References hydration in React React Antipatterns: Props In Initial State NextAuth Pages Infinite redirection loop with wrapper when using custom sign-in page ","date":"2024-01-07","objectID":"/posts/frontend-practice/:5:0","tags":["platform"],"title":"React前端快速构建","uri":"/posts/frontend-practice/"},{"categories":["工作项目"],"content":"容器调度通过一系列规则将容器分配到各个节点上，包括容器资源需求，节点的负载情况，容器、节点间的亲和性等。 该交付平台调度模块参照 Kube-Scheduler 框架，并对相应的流程进行简化，针对相关业务对一些调度插件进行特化，从而实现专用的调度器。 ","date":"2023-11-11","objectID":"/posts/scheduler/:0:0","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"Kube-Scheduler Kube-Scheduler 是 K8s 容器平台的调度器模块，用于将 Pod 放置到合适的节点上，相应节点上的 Kubelet 监听新调度的 Pod 并运行。 Kube-Scheduler 通过 K8s 的监测（Watch）机制来发现集群中新创建且尚未被调度到节点上的 Pod。调度器会将所发现的每一个未调度的 Pod 调度到一个合适的节点上来运行。 Kube-Scheduler主要包括调度（Schedule）和绑定（Binding）两个阶段。其中，调度由预选（Predicates）和优选（Priorities）两个核心过程构成。 调度阶段为 Pod 选择一个合适的节点，是顺序执行的。 绑定阶段将调度结果提交给集群。是并发执行。 无论是在调度还是绑定过程中，如果发生错误或者 Pod 无法成功调度，那么 Pod 就会被重新放回调度队列，等待重新调度。 ","date":"2023-11-11","objectID":"/posts/scheduler/:1:0","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"调度框架 调度框架的核心是定义一组扩展点，用户可以实现扩展点定义的接口来接入自己的调度逻辑。调度框架在执行调度工作流时，遇到对应的扩展点时，将调用用户注册的扩展。 调度框架中的扩展点有些可以改变调度程序的决策方法，有些扩展点只是发送一个通知。 Kube-Scheduler调度插件框架 QueueSort 扩展用于确定 Pod 调度顺序，用以决定调度的优先级。 Pre-filter 扩展用于对 Pod 进行预处理，并对集群做一些基本检查。 Filter 扩展用于实现预选逻辑，过滤掉不符合要求的节点。 Post-filter 扩展用于实现可选节点列表相关的通知机制。 Scoring 扩展用于实现优选逻辑，按照规则从可选节点列表中选出得分最高的节点。 Normalize scoring 扩展用于对得分进行修正。 Reserve 扩展可以获得节点为该 Pod 预留的资源，避免 Pod 绑定发生资源冲突。 Permit 扩展用于最终准入、推迟 Pod 与节点的绑定。 Bind 扩展用于将 Pod 绑定到节点上，结合 Pre-bind 扩展对绑定进行预处理，结合 Post-bind 扩展对绑定结果进行通知。 扩展的调用顺序为： 如果某个扩展点没有配置对应的扩展，调度框架将使用默认插件中的扩展。 如果为某个扩展点配置且激活了扩展，则调度框架将先调用默认插件的扩展，再调用配置中的扩展。 ","date":"2023-11-11","objectID":"/posts/scheduler/:1:1","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"完整Pod调度过程 客户端通过 API-Server 的 API 或者 kubectl 工具创建 Pod 资源。 API-Server 收到请求后将数据持久化到 etcd 中。 Kube-Scheduler 监控 API-Server，将未分配 host 的 Pod 放入到调度队列中，并按优先级进行排序。 从调度队列中不断弹出 Pod，开始完整的调度周期（预选、优选、绑定）。并将绑定的结果提交到 API-Server，持久化到 etcd 中。 Kubelet 监控 API-Server，发现新增了调度到本 Node 的 Pod，执行创建 Pod 的操作。 ","date":"2023-11-11","objectID":"/posts/scheduler/:1:2","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"交付平台 该私有化交付项目存在以下特点： 平台底座容器平台基于自研，不是通用的 K8s。 项目应用数较少，集群的规模较小，一般为几台到十几台服务器，因此需要最大限度提升资源利用率。 交付的应用主要对 GPU、NVMe 资源有强要求，并且需要通过 Quota 机制复用此类资源。 项目中存大量应用共生与极个别的应用互斥的要求。 交付项目中，所有服务不可被驱逐，要么全部调度成功，要么全失败。 交付项目中，需保证应用实例依赖关系，即当数据库服务可以对外服务时，才启动业务服务。 交付平台部署的服务中，存在平台级应用（如计算平台），用户基于计算平台可以更改计算实例，即某些服务可以直接对接底座容器平台。 借鉴 Kube-Scheduler，并结合交付项目的特点，我们对调度框架进行了简化与改造。 ","date":"2023-11-11","objectID":"/posts/scheduler/:2:0","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"统一调度 由于交付规模较小，并且实例部署不存在抢占，我们采用串行的统一调度（类似于gang scheduling）方式进行调度。 统一调度意味着每次都基于全量的服务实例进行调度。为了简化交付操作复杂度，交付平台最核心的目标是一键部署，这意味着一次安装操作可以将所有应用解析出来进行调度，这种全局调度便于找到最优的部署策略，避免逐个应用调度引起频繁的驱逐腾挪。 新安装一个实例时，所有已部署的实例同样跟着参与调度。对于有状态的应用，不应改变其调度结果（因为集群并没有将存储进行虚拟化）；对于无状态的应用，尽量保证其调度结果，当新的实例所需的资源不够时，会尝试腾挪此类应用以合并资源碎片。 这种统一调度可以将流程简化为sort-\u003efilter-\u003escore-\u003ebind。不通过预分配行为来腾挪资源，利用 sort 阶段将重资源的实例顺序提前的方式简化资源腾挪。 ","date":"2023-11-11","objectID":"/posts/scheduler/:2:1","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"专用调度插件 为满足交付项目的需求，我们主要定制了以下插件： 基于 GPU 显存与类型的调度插件。 基于 NVMe 按 label 进行共享的调度插件。 此外我们简化了 Pod 亲和性调度策略： 对于服务共生，我们抽象出 App Sidecar 概念来确保两个应用的实例可以调度到同一节点上。 对于服务互斥，我们采用灵活地植入 node label 的方式避免应用部署到同一节点上。 ","date":"2023-11-11","objectID":"/posts/scheduler/:2:2","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"部署顺序 K8s 支持通过 init containers 来确保一个 Deployment 中的若干的 container 按指定的顺序部署。在微服务架构下，显然无法将几十上百个 container 放在一个Deployment 中，因此需要一个更完善的机制保证应用的部署顺序。 kapp 将一组资源视作应用，其提供了两个特别有价值的功能特性： 依赖感知部署。通过定义两个 annotations kapp.k14s.io/change-group 与 kapp.k14s.io/change-rule 对一组资源进行排序，后者定义对前者的依赖，只有保证依赖的change-group所有声明方就绪后，才部署当前资源。 配置变更实时生效。通过将 ConfigMap 版本化的方式，让关联的应用可以在其变更后自动重启。 参考 kapp 依赖感知的实现，我们通过以下策略实现服务按序部署： 将服务间的依赖层级映射为 rank。 按 rank 启动实例，结合健康检查，确保某一 rank 的所有实例就绪后启动下一 rank 的服务。 ","date":"2023-11-11","objectID":"/posts/scheduler/:2:3","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"可扩展性 由于交付平台会部署计算平台，而计算平台需要调整计算实例的个数，因此需要参与调度部署的流程。这意味着调度部署需要和编排解耦，计算平台需要利用调度器提供的 api/sdk 来触发实例调度与部署。这就无法避免引入了高层平台对底层调度模块的引用耦合，这驱使我们将系统架构由显式调用风格切换到以数据共享的风格上。 ","date":"2023-11-11","objectID":"/posts/scheduler/:2:4","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"架构演进 为增加平台的可扩展性，我们将交付平台的架构由同步架构演进成异步架构。 ","date":"2023-11-11","objectID":"/posts/scheduler/:3:0","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"同步架构 同步架构采用显式调用风格，架构图如下： 同步部署架构 用户输入系统配置，选择相应的应用提交到交付平台上。 交付平台通过编排工具，从包管理器中选择相应的服务模板，进行配置渲染，生成一组应用 Services。 编排后的 Services 传递给调度器进行资源分配，并根据副本数、AppSidecar 切分成若干 Instances，部署到自研平台。 上层计算平台通过调用交付平台的 schedule api 创建计算实例。 p.s. 此架构下 Service 类似于 K8s Deployment，Instance 类似于 Pod。 ","date":"2023-11-11","objectID":"/posts/scheduler/:3:1","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"异步架构 异步架构采用以资源为中心的异步处理方式，使用 K8s Operator 实现编排、调度、部署的解耦。 异步部署架构 交付平台启动器将其所依赖的服务（API-Server, Service-Controller, Instance-Controller）部署到容器平台上。 用户输入系统配置，选择相应的应用提交到交付平台上。 交付平台通过编排工具，从包管理器中选择相应的服务模板，进行配置渲染，生成一组应用 Services，提交到 API-Server。 Service-Controller 监控 API-Server 中的 Service CR，拆分成 Instances 并进行统一调度，随后提交到 API-Server。 Instance-Controller 监听 Instance CR，并部署到容器平台上。 上层计算平台通过提交 Service CR 创建计算实例。 异步架构的优势： 使用 K8s 声明式架构，便于向云原生演进，可以无缝地兼容 Helm chart。 增强平台扩展性。调度不在局限于交付平台，而更贴近容器平台，这样在交付平台之上可以扩展出高动态性的计算平台，按需生成计算实例调度部署到容器平台上。这也是架构改造的核心原因。 编排、调度、部署相解耦。异步的处理方式避免了用户长时间的等待，简化各组件的网络中断重试、高可用机制的实现。 ","date":"2023-11-11","objectID":"/posts/scheduler/:3:2","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"引用 https://kingjcy.github.io/post/cloud/paas/base/kubernetes/k8s-scheduler/ ","date":"2023-11-11","objectID":"/posts/scheduler/:4:0","tags":["platform"],"title":"私有化交付平台——调度部署篇","uri":"/posts/scheduler/"},{"categories":["工作项目"],"content":"编排就像搭积木，将零散的服务拼接成完整的系统。 ","date":"2023-07-12","objectID":"/posts/orchestration/:0:0","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"服务编排 在大型微服务系统中，服务编排至关重要。服务编排泛指生成一组服务描述文件的方式，诸如docker-compose.yaml，K8S object file等都是通过编排一组文件来描述服务的部署规格。 为提升系统配置的灵活性和组件可复用性，编排过程通常会变为使用一组系统配置渲染服务模板的方式，如helm chart。这样开发者只需将一组服务规格模版给到用户，用户便可以提供一组配置来实现所需的系统部署行为。 由于编排的过程会产生系统中所有服务的描述文件，因此在一个多版本、依赖关系复杂的系统中，编排还需要具备依赖解析等能力以获取相应的服务模板的版本。 ","date":"2023-07-12","objectID":"/posts/orchestration/:1:0","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"包管理器 微服务架构使组件发版变得更轻量、更敏捷，发版的频率大幅提升。具体表现为发布的服务数目变多、每个服务的版本激增。为了在众多的服务及海量的版本中挑选出满足系统功能需求、服务依赖约束、API兼容性的版本，包管理器应运而生。无论是语言类包管理器（如npm、pip、go module）还是应用类包管理器（如apt、helm），核心的功能便是提供规范的版本管理并屏蔽掉依赖解析、版本选择等复杂的流程。 ","date":"2023-07-12","objectID":"/posts/orchestration/:2:0","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"版本管理 本平台采用通用的semantic version 2.0.0（semver）来作为组件包版本的规范。semver提供了如下规则以透过修改相应的版本号向大家说明做了哪些修改。 semver的版本定义基本形式为：MAJOR.MINOR.PATCH，其中 MAJOR为主版本号，当你做了不兼容的 API修改时，递增MAJOR。 MINOR为次版本号，当你新增向下兼容的功能时，递增MINOR。 PATCH为修订号，当你做了向下兼容的问题修正时，递增PATCH。 更完整的semver形式为：MAJOR.MINOR.PATCH-PRE+META PRE为预发布号，当某个版本正式发布前，会通过预发布进行测试或给一小批用户进行试用体验，一般定义为alpha，beta这种表示当前功能成熟度或测试进行的阶段。具有PRE的版本为prerelease，否则为release。 META为元信息，仅用作版本的标识信息，一般定义为发布日期等。 版本管理会更多关注版本的排序，通过版本排序找到最新发布的版本、最新稳定版本、某个MAJOR版本下的最新稳定版本等。 semver定义的排序规则总结如下： MAJOR、MINOR、PATCH按序进行比较，一旦某位比较出了大小，版本大小便据此确定。如果都一致，则根据PRE比较大小。 主版本号相同时，prerelease版本小于release版本，两个prerelease版本的顺序按字PRE字母序排序。 META不影响排序。 ","date":"2023-07-12","objectID":"/posts/orchestration/:2:1","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"依赖管理 包管理器主要职责在于选出能让用户选择的包正常运行的所有依赖包的版本，其最核心功能是依赖解析。 依赖声明 首先，包开发者需要在包的描述文件中声明一组依赖及版本约束，我们看一个npm package.json依赖定义方式： { // .... \"dependencies\": { \"@mui/lab\": \"^5.0.0-alpha.70\", \"@mui/material\": \"^5.4.3\", \"mdi-material-ui\": \"^7.1.0\", \"react\": \"17.0.2\", \"react-dom\": \"17.0.2\", }, \"devDependencies\":{ // .... } } 通常情况下，某个组件声明依赖是为了使用其部分能力，比如调用其API辅助完成自身的功能。但在交付的视角下，将组装关系视作依赖关系可以提升交付效率，实现整个产品的一键安装。 所谓组装关系，指的是使用一个虚拟包关联一系列的组件包，将关联的组件包声明为依赖关系，包管理器可以按依赖解析的规则选择这些组件的版本。通常虚拟包并不包含任何真实负载（不是一个真正的组件），它只用来关联系统所需的包，其中的代表是archlinux的meta package。这一方式不仅使得安装复杂应用变得更加简便，也使得包的扩展性、复用性大幅提升。 我们从业务的角度定义了三类包： 组件：可以真正部署的包，如redis，粒度最小。 模块：组装一部分具备某一功能组件的包，如storage模块组装了redis与mysql组件，粒度适中。 产品：一个完整的业务产品包，组装若干模块、组件，粒度最大。 通过引入虚拟包的概念， 我们可以对组件/模块/产品三个维度进行复用与部署，大幅提升交付效率。 包的复用关系 版本约束 描述文件需要包含依赖包名及对其的版本约束。版本约束主要用作兼容性保证，以^5.4.3为例，它表示依赖的版本号必须满足\u003e=5.4.3 \u0026\u0026 \u003c6.0.0，不能选择5.0.0，因为可能缺少它所依赖的某些功能；也不能选择6.0.0，因为它所依赖的API可能存在breaking change，与依赖方的调用的api不兼容。 与semver不同，版本约束并没有一个标准，我们采用了与helm一致的规范。此规范与npm等包管理器版本约束不一致的点在于对prerelease的选择上，即版本约束的形式为release时，不会匹配到任何prerelease。这种形式并未完全按照版本排序规则匹配版本，而是结合顺序与稳定性语义进行选择。其将稳定版的选择区分开，更适合生产环境下的版本选择，例如\u003e1.0.0会匹配到1.0.3，但无法匹配到1.0.3-beta。 版本选择 通常情况下，依赖版本约束通常为一个范围，而不是一个固定的版本，这样做的好处是便于升级更新。这也意味着在包管理器仓库里，可能存在多个版本满足约束条件，那么包管理器该选择哪个版本呢？ 依赖的版本选择也是一个见仁见智的决策。比如稳定性优先的GO MVS会选择满足所有版本约束的最小版本，其宗旨为既然大家都不需要用最新版本，那就没有必要使用最新的版本。而与之相对的是，大多数阵营会选择满足约束的最新版本，这样做的好处是选择最新的版本可以修复很多已知的问题，也可能会添加一些很cool的功能。他们唯一需要关心的是避免选择一个不兼容的版本，他们会选择相信所有开发者都遵循semver。约束大多定义为^X.Y.Z，这通常表明我测试时依赖版本选择的是X.Y.Z，但只要依赖的的主版本号不变（没有breaking change）就可以放心选择最新的版本。 我们采用的是第二个方案，但我们突出了对prerelease的考量，按照semver的规范，我们不希望在交付过程中选择不稳定的版本，而一些研发/测试场景又希望选择这些版本，这正是我们选择Masterminds/constraints约束规则的原因。 最大版本选择策略 当然这种自动的版本选择有时不能满足我们的需求，比如组件C满足条件的最大版本为1.2，但这个版本由于加入了一些功能后，导致了性能所有下降，需要回退到1.1。我们当然可以将约束收紧为~1.1，但是如果C在后来的某个版本修复的问题，我们还需要手动地把约束改回去。更常见的方式是提供人工更新某个组件版本的方式，就像go get C@1.1、npm update C@1.1一样。 版本锁定 尽管大家希望尽量使用最新的版本，但更需要关注的点在于版本选择的确定性，尤其是对于交付平台，保证交付产品的确定性是第一要义。 所谓确定性，指的是某个版本的包，多次安装/部署的状态是一致的。在软件开发的早期时代，最流行的说法便是：\"在我的机器上可以正常运行\"，当然这与软件的可移植性相关，但是如果两个运行环境中部署的包不一致，其行为大概率不合预期，而这个问题在微服务架构下会被放大。 举个例子，比如某个被依赖的组件发了新的版本或者某个版本被修改后覆盖掉，那么按照最新版本选择策略部署依赖该组件的产品时，这个组件就会相较于之前部署的组件有所差别。这时我们会说：\"同样的产品版本，之前部署就没问题，现在部署就有问题了\"。这正是不确定性带来的问题，这个问题对于运维人员是一场灾难，因为他们会在\"找不同\"的流程上痛苦挣扎。 为了保证部署的确定性，我们需要需要如下两个规则： 项目一旦部署后，之后所有部署的组件版本不应发生变化。 每个组件版本的内容不应发生变化。 这就意味着版本选择仅发生在首次构建中，一旦产品确认后，依赖的版本不应发生变化。我们需要通过版本锁定来保证这件事，通常的做法是加一个lock文件，将首次构建选择的版本及内容hash记录下来，之后的每次部署只使用lock里的版本并校验版本是否发生了变化。 像npm、go module等包管理器都存在lock文件（package-lock.json、go.mod/go.sum），确保每次部署都可以选择一致的版本，并且这个版本的hash未发生变化。 ","date":"2023-07-12","objectID":"/posts/orchestration/:2:2","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"模板渲染 模板渲染是一种提供灵活性数据更新的重要手段。模版渲染通常用在网页构建中，将展示的内容框架构建成模版，并使用实时的数据渲染模板，形成动态变化的网页。 在系统部署中，业务配置决定了部署形态的多样性，这些配置包括资源相关的配置（副本数、最大使用内存容量）和功能相关配置（是否开启聚档功能，索引类型是内存索引还是磁盘索引）。为提升部署的灵活性，通常将应用的规格文件作为模版，而将系统配置作为渲染模版的数据。 ","date":"2023-07-12","objectID":"/posts/orchestration/:3:0","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"模版引擎 模版引擎的种类繁多，每个模版语言都有各自适用的场景，也有相应的短板场景。支持合适的模板语言，是我们提升研发体验的关键。 模板引擎主要分为两大阵营： 文本型模版：直接以文本形式展示出规格文件的结构并注入模板变量，模版渲染更像是完形填空，此派别代表为jinja，go template。这类模板编写声明式文件时结构很直观，但在书写诸如判断、循环、计算等逻辑时格外复杂。 程序型模版：此类模版语言通常类似于编程语言，将规格文件进行结构化改造，以写代码的形式处理渲染过程，此派别可以使用任意的脚本语言（如Bazel Starlark），模版引擎相当于对应语言的运行器。此类模版可以很容易处理复杂的计算逻辑，但无法以直观的方式展示出规格文件的结构。 在本平台出现之前，我司编排工具大多使用python脚本生成，这推动了我们选择starlark模版引擎，以降低研发人员切换编排系统的成本。此外，为兼容helm chart，我们也支持了go template模版引擎。 go template模版 vs starlark模板 ","date":"2023-07-12","objectID":"/posts/orchestration/:3:1","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"配置管理 对应服务级交付平台，配置管理是决定交付效率最重要的因素。在一个大型应用系统中，配置管理主要面对如下挑战： 配置过多：大型系统可以包含上百个服务，每个服务又可能包含若干业务配置，导致交付人员面对近千个配置。 配置联动：由于系统中服务交互关系复杂，更改某一服务配置的同时要求更改其他服务的配置，这种联动关系要求配置变更按照某种规则同时发生，如果只更改一处而未更改其他的地方，可能存在无法预期的行为。 配置传递：服务A依赖服务B，并且希望控制服务B的部署行为时，配置需要从依赖方传递到被依赖方。比如应用根据自身一致性要求决定其依赖的kafka选择相关的复制策略。 配置依赖：服务A依赖服务B，并且希望获取B的相关信息时，配置需要从被依赖方反向传递到依赖方。比如某个应用依赖了mysql，并且希望获得mysql的endpoints。 在交付平台的配置系统中，一个由二八定律衍生的矛盾是：大量的配置让使用者无从下手 vs 过少的配置无法配置完整的功能。因此我们在精简配置的同时，需要保留对那些使用频率很少的配置变更的能力，即便修改这些配置很复杂。就好比变长编码会给低频符号较长的码字，但不会遗漏任何一个可能出现的符号编码。 隐藏配置过多的问题 针对配置过多的问题，我们首要任务是筛选出系统中高频变化的配置，这些配置通常会随着私有化环境的不同而变化，这类配置通常为： 限制系统整体资源的配置，如cpuLimit等。 控制系统功能的配置，如enableNotification等。 控制系统扩展性、迁移等运维配置，如replica等。 对于开发配置、调试配置、低频业务配置要尽量避免直接出现在交付系统使用者面前，这些配置一般会以高级选项的方式隐藏在某处，只有当系统破天荒地需要调整他们时，才会把他们翻到台面上。 支持配置联动 在Helm中，从父chart向子chart传值是很常见的，但是对于复杂系统来讲，helm的传值方式并不实用。因为配置通常具有联动性，比如某个系统中应用A的副本数必须与B的副本数完全一致，而其他的系统中不存在这样的限制。这就意味着在该系统中，修改A的副本数的同时也要修改B的副本数。这种限制给交付人员增加了额外的心理负担的同时，也引入了很强的易错隐患。配置联动性在大型系统中会被无限放大，设想一下，在一个100个微服务的系统中，如果只有固定的五种交付模式，那么只向用户暴露一个五选一的enum，而系统中服务的配置完全交给联动性，那么我们的交付流程会是多么的简洁！ 另一方面，对于配置使用方而言，整个系统通常是一个黑盒。用户并不关心散落在各个服务上的配置，他们只关心系统暴露出来的配置，这就意味着交付系统需要隐藏配置在服务间传递的细节。交付平台在配置下发时，可以按照一定的规则进行加工并传递到相关的服务上。 总体来说，配置联动将灵活运用配置加工与传递，并为用户呈现出简洁的配置管理。 配置联动场景 配置双向传递 配置传递包含两个方向，除了上述提及的由父组件向子组件的配置传递以外，还应对从子组件向父组件的配置导出予以支持。我们称前者自顶向下的传递为正向配置传递，后者自底向上的传递为反向配置传递。 反向配置传递也是一个很常见的需求，比如对接第三方应用时，他们需要调用某一系统的api，而这个系统需要将系统的入口地址告知给第三方应用。这时，通常由gateway入口服务将自身的endpoint通过反向传递的方式暴露给系统。通常情况下，被依赖方都需要将自己的endpoint以反向配置传递的方式暴露给依赖方，只是借助域名管理系统，这种方式会显得无足轻重。但对于那些希望获取依赖组件运行时配置的场景，反向配置传递是最简单的解决方案。 我们将双向配置传递及模版渲染过程建模为如下处理机： 某组件从父组件传递过来的pVal经由combo函数加工、传递到子组件上，完成配置的正向传递。 子组件将声明的配置cVal传递给该组件。 该组件将来自父组件的配置、子组件的配置、自身默认的配置以及用户传入的配置合并起来渲染模版，生成对应的服务编排文件。 根据该组件的配置声明，将cVal传递给其父组件，完成配置的反向传递。 单个组件配置建模 将每个处理机单元按照包依赖关系连接成管道，便形成了完整的编排解析器。在配置传递的过程中会不断地进行模版渲染以生成编排文件，最终可以编排出整个系统。 依赖链路配置传递流程 ","date":"2023-07-12","objectID":"/posts/orchestration/:3:2","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"总结 服务编排以声明式的方式简化了微服务系统的部署，但生产环境下的交付平台远不能向docker compose一样只管理一个compose文件就行。这要求服务编排需要考虑包的编排复用、版本管理、依赖管理、模板编写、业务配置等诸多方面。只有做到编排足够灵活，私有化交付才可能高效进行。 该平台参考了go module、npm等包管理器的设计，将服务封装成一个个编排模板，并对其进行版本管理；结合依赖管理，该平台支持一键编排部署的能力；为了让我们的研发人员可以平滑地从旧的编排工具迁移到我们的交付平台上，我们扩展了编排模板引擎；参考helm的依赖传递模型，该平台提供了灵活的配置传递机制，并在此基础上增添了配置多播、配置加工等魔法，极力简化系统配置，隐藏配置作用细节，降低交付平台使用者的心智负担。 ","date":"2023-07-12","objectID":"/posts/orchestration/:4:0","tags":["platform"],"title":"私有化交付平台——编排篇","uri":"/posts/orchestration/"},{"categories":["工作项目"],"content":"打造面向私有云的包管理器，降低研发成本，提升交付效率。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:0:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"私有化交付 私有化是ToB、ToG类业务的主要落地方式，而私有化交付旨在将业务系统提供给客户，即将公司研发的业务系统部署到客户现场的私有云上，以在其内部提供服务。 通常，私有化交付步骤包括私有场景下环境的构建、业务服务配置、业务系统部署、业务系统运维等。其前置条件为业务系统的研发与系统的质量测试。 私有化交付平台将上述交付流程集成到一个平台上，客户可以在这一平台上完成系统的配置、部署、运维、更新等操作，以提升交付效率。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:1:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"私有化困境 私有化交付模式下的一个核心痛点是公司必须维护业务系统的多个版本，无法通过统一的”上线”流程将所有现场的版本都更新到最新版本。这是因为： 大量客户现场存在定制化功能，很难将这些定制化功能移植到一个通用的最新版本上。 某些版本间存在不兼容变动，涉及数据迁移等操作，而众多的客户现场会放大相关人工成本。 很多私有化项目以满足合同为首要目的，只要系统不出大问题，甲方不会主动要求升到最新版本；而乙方也不会为了取悦客户而耗费人力对客户现场的系统进行升级。 此外，根据公司的业务性质和技术形态，还存在如下的挑战： 很多客户现场的网络无法连接到互联网，比如对安全性要求较高的公网系统、政府系统等。无法通过远程的方式连到客户现场进行操作，只能通过人工出差或远程指导的方式运维，效率较低。 在容器平台百花齐放的早期时代，很多公司会选择自研容器部署平台。而当前K8S已成为云原生容器平台的标准，向云原生演进过程中，无法避免容器平台的迁移。此外一些私有化场景可能会要求由其提供诸如K8S集群用以部署业务应用，因此研发及部署需要考虑面向底座平台的可扩展性。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:2:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"微服务困境 微服务架构因其职责单一、灵活性强、可以独立扩展、支持多种语言的优点，成为云原生核心技术之一，并广受开发者的拥趸。但是对于规模较大的系统而言，尤其对于私有化场景下的微服务部署而言，其面临大量挑战： 部署工作量大。大型系统一般会有近百个微服务，每个服务又有数十个配置，这种规模的系统部署起来工作量极大。如果扩散到多个私有化场景，在没有高效的部署工具的帮助下将会是一场灾难。 服务间依赖关系错综。微服务能替代原有的单体结构的核心在于服务间通信，这就带来的服务依赖的问题，比如业务应用会依赖数据库服务。这种依赖会影射到API兼容性、部署顺序等限制，给微服务管理带来挑战。 服务版本管理困难。在某个版本的系统中，每个服务的版本都是固定的，以确保每次交付的系统都是具有确定性的。但私有化场景下经常会存在一些升级需求，比如更新数据库版本以修复某个安全漏洞、更新业务服务版本以增加某个功能。这就意味着系统可能存在很多新旧版本混合的服务。QA很难对所有组合进行集成测试，这就意味着服务升级需要保证兼容性，即版本的定义需要满足一定的规范，比如semantic version。 系统发布风险高。在大规模的微服务下，每次系统发布需要协调各个服务的开发方，一旦一个服务没有按时ready，发布便存在延期的风险。 服务复用率低。公司内存在多个产品线，因私有化交付无法在公司内提供统一的基础设施，很多产品线不得不维护各自的基础设施。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:3:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"平台设计理念 为解决私有化交付中版本管理难、部署难、发布难的问题，同时为了提升研发效率、发布效率、可维护性，本平台提出了如下设计理念： 我们需要一个公司级的软件包管理平台，提供统一的版本发布、打包和依赖管理机制。提高基础组件的复用率，解决软件之间依赖混乱等问题，同时降低因版本升级、维护不确定性带来的心智负担。 我们需要一个高效的部署工具，理想情况下可以做到一键配置并部署一个大型系统。 参考云原生理念中的声明式API和不可变基础设施，我们需要一个保证确定性的部署工具，一个环境下的多次部署行为应该是结果确定的，不应存在“惊喜”。 我们需要一个通用的编排底座，规范协议和接口，以节省掉一个产品就要一套编排工具带来的重复工作。 我们需要屏蔽掉底层不同容器平台（比如Docker Swarm和K8S）带来的差异，降低交付人员的心智负担，更无感地迈向云原生生态。 我们需要一个服务管理工具，为升降级、扩缩容、部署配置更新、备份等功能提供通用框架，而不是每个团队各自开发一套。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:4:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"Why Not Helm? Helm作为K8S的包管理器，为云原生应用部署提供了便捷。使用Helm可以很便捷地将一个复杂的应用部署到K8S上。就像npm一样，管理着多个版本的同时解析复杂的依赖关系，通过一键安装提升用户体验。 Helm可以满足我们对交付平台的绝大多数要求，但是存在如下几个限制： Helm专用于K8S平台，适配其他容器平台需要通过operator等方式，复杂度较高。 Helm chart目前仅支持go template模板语言，对于复杂逻辑的编写十分不友好。 Helm通过subchart实现依赖管理机制，此机制不支持递归依赖解析，必须将所有依赖拍平，无法实现模块级产品复用。 Helm chart对依赖更新的限制较高，更新必须从顶级chart进行，对于细粒度升级场景不友好。 Helm chart的values传值机制不够灵活，通常我们希望仅在顶级chart上暴露常用的配置，将其传递给所依赖的组件。chart具备向单个依赖传值的能力，但是并不具备单个value向一组依赖传值的能力（比如通过一个mode配置控制应用部署的形态，而globals values会破坏chart通用性）。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:5:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"核心Features 支持starlark模板语言定义组件包，语法类似python，降低入门门槛。 支持深层依赖管理和版本解析，便于复用模块级组件。 支持更灵活的配置传递机制，如配置加工，配置多播传递等。 支持多维度的升级策略，如单个服务升级、模块化多服务升级、系统全量服务升级等。 原生面向自研容器平台，提供配套的调度和部署逻辑。同时提供相应的chart wrapper以兼容面向K8S的部署能力。 基于OCI的打包能力，将系统的编排、镜像、数据包高效地搬运到私有化环境。 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:6:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["工作项目"],"content":"系列文章 私有化交付平台——编排篇 私有化交付平台——调度部署篇 私有化交付平台——打包篇 私有化交付平台——升级篇【TODO】 ","date":"2023-07-04","objectID":"/posts/delivery-platform/:7:0","tags":["platform"],"title":"私有化交付平台——概述篇","uri":"/posts/delivery-platform/"},{"categories":["anecdotes"],"content":"一个IT运维的传奇故事。 ","date":"2023-06-26","objectID":"/posts/phoenix/:0:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["anecdotes"],"content":"概述 《凤凰项目，一个IT运维的传奇故事》描述了一个IT运维部副总裁比尔上任第一天就面临了工资核算系统崩溃的问题。在之后的两个月内，接连出现安全性审计整改、凤凰项目被迫强制上线导致多系统异常等灾难，用户信任度大幅下降，企业生存状态岌岌可危，甚至面临整个IT部被外包的局面。 在本书“扫地僧”埃瑞克的帮助下，主人公比尔及其部下积极完善变更控制流程，协同开发部、IT运维部和安全部，贯彻落实“三步工作法”。将DevOps与敏捷开发的思想引入到新项目“独角兽”并逐步扩散到整个IT部门，力挽狂澜，大幅提升了企业竞争力。 本书以小说的形式描写IT部门遇到的重大事故、问题解决不顺利、部门间推诿埋怨、新流程制度引入困难到逐渐好转、IT变更流程标准化等历程。情节跌宕，让IT从业者深感共鸣。书中将IT运维与工厂车间制造相类比，强调了生产瓶颈、流程标准化、产品交付周期与预期收益等对IT部门的启示，令人茅塞顿开。 其中总结几个触动比较大的点。 ","date":"2023-06-26","objectID":"/posts/phoenix/:1:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["anecdotes"],"content":"约束点理论 木桶的盛水量并不取决于筒壁上最高的木板，而是取决于最短的那块。约束点（瓶颈）限制了产能，限制了部署效率，本书中首席工程师布伦特便是整个IT部的约束点。很多项目只有他了解，因此绝大多数的项目交付时，或多或少地需要他参与其中。他的不可替代性以及满负荷工作让他无法完成凤凰项目的相关工作，引爆了本书的主要矛盾。 正如创建约束理论的艾利·高德拉特告诉我们，在瓶颈之外的任何地方作出的改进都是假象。在瓶颈之后作出任何改进都是徒劳的，因为只能干等着瓶颈把工作传送过来。而在瓶颈之前作出的任何改进则只会导致瓶颈处堆积更多的库存。因而，正确的决定应围绕对约束点的优化展开。 书中列举出一个流经布伦特的30分钟的工作受约束点限制需等待63小时，并附上等待时间与资源使用率的关系图。对于约束点而言，其使用率接近满负荷，因此新流经约束点的工作势必会等待很长时间，从而拖慢整体效率。 等待时间与资源利用率对比图 值得庆幸的是，比尔很快地意识到这一点，迅速冻结流经约束点的除凤凰项目之外所有项目，以便让约束点处积压的任务得以纾解。在项目解冻时，对约束点处的项目予以限流，保证其对核心项目（凤凰）的支持程度，并将可以提升约束点性能的监控项目优先级调高，大幅缓解了约束点处的限制，为IT流程的有序进行奠定了基础。 ","date":"2023-06-26","objectID":"/posts/phoenix/:2:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["anecdotes"],"content":"部门协作 造成书中IT部门的混乱的”功臣“无疑是糟糕的部门协作了。开发部、IT运维部、安全部看上去各司其职，但却相互限制，事故出现后相互推诿、相互抱怨，是一场“不协调的婚姻”。以IT运维部的视角举例：比一名开发人员更危险的就是开发人员和信息安全部门的人联手。这样的组合把给我们添乱的动机、手段和机会都弄齐全了。 这种协作的矛盾在我看来核心原因是其职能边界过分独立。开发部将相关开发代码交给运维部；运维部整合开发包、配置、环境进行部署；安全部事后打安全补丁。瀑布模型下分明的流水线并没有提升产品交付效率，反而放大上游的错误，引入下游干预产生的负面影响。书中列举中几个协作失调的案例： 安全部一个数据库字段变更导致工资核算系统崩溃，运维部耗费大量时间排查原因。 开发部将所有变更申请标为急或者紧急变更，扰乱运维部的处理优先级。 开发部版本/制品管理混乱，运维部拿到错误/陈旧/不完整的开发包及配置，导致部署失败。 运维部对生产环境引入了某个更改，没有同步到开发、QA环境，导致测试通过的项目在生产环境中部署失败。 过于独立的职能边界导致了庞大的部门间工作流转开销，成为拉长部署周期的元凶。 受生产车间将约束点“烘房”与“喷漆室”两个工作中心合并提升产能的启迪，部门协作的方式也进行相应的调整，部门间不是彼此独立的上下游关系，而是各阶段通力合作，互相渗透。运维部提供一组通用的搭建环境的方案（IaC），开发部在代码提交后便可以自行验证、部署（CI/CD）；安全部也不用再事后对部署安装安全补丁，而在一开始便对开发过程和生产环境提出安全标准（DevSecOps），并在开发、部署等阶段对潜在的漏洞进行攻击检测。各部门不再仅陷于自己部门的目标，而着眼于企业目标，缩短产品发布周期，提升企业核心竞争力。 这正是DevOps诞生的最佳时机。 ","date":"2023-06-26","objectID":"/posts/phoenix/:3:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["anecdotes"],"content":"三步工作法 书中提出的三步工作法正是敏捷开发的原型： 第一工作法是关于从开发到IT运维再到客户的整个自左向右的工作流。通过小批量的规模和工作间隔，最大化系统流量。 第二工作法是关于价值流各阶段自右向左的快速持续反馈流，放大其效益以确保防止问题再次发生，更快地发现和修复问题。 第三工作法是关于创造公司文化（敏捷无疑是一种公司文化）。要勇于尝试、加强演练，不断给系统施加压力，从而不断强化习惯并加以改进，因为对任何流程或技能来说，训练成习惯， 习惯成精通 。系统里要经常出些故障，长此以往，再遇到困难就没有原来那么痛苦了。 三步工作法需要我们计划发布内容，把控发布节奏，保障代码总是处于可部署的状态的同时，通过CI/CD、自动化测试加速发布与部署效率；此外还需对已发布的系统进行监控，快速获取反馈并进行持续优化改进。 ","date":"2023-06-26","objectID":"/posts/phoenix/:4:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["anecdotes"],"content":"总结 本书阐释了开发部和IT运维部之间长期的核心冲突为何会导致整个IT组织及其所服务企业的失败。如果不加以抑制，冲突会拖延开发部的上线时间，并在功能发布期间导致时间更长、问题更多的部署，增加1级严重级别服务中断的数量，而IT运维部则要做越来越多的计划外工作，难以清偿技术债务。 而扭转乾坤的力量正是敏捷与DevOps。围绕约束点制定计划优先级以最大化生产效率；推进企业的信息化集成以减少线上事故，统一IT资源；实施短平快的产品发布以跟上市场节奏。 在敏捷文化与DevOps盛行的今天，我们很少能知道自己究竟避开了哪些灾难。但就其演进的历程来看，技术早已为首要的价值创造过程，并已成为绝大多数公司获得客户的越来越重要的(而且常常是基础性的)一种手段。 ","date":"2023-06-26","objectID":"/posts/phoenix/:5:0","tags":["思辨"],"title":"读《凤凰项目》有感","uri":"/posts/phoenix/"},{"categories":["整理与总结"],"content":"One place for everything. Accessible to everyone. ","date":"2023-04-22","objectID":"/posts/dev-portal/:0:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"什么是开发者门户 开发者门户（developer portal）通常是一个网站或在线平台，旨在为开发者提供有关软件开发和应用程序构建的信息和资源。这些门户通常包括软件组件、资源、环境、工具、示例代码和文档等内容，这些内容可以帮助开发者更轻松地构建应用程序和软件。通过提供这些资源和工具，开发者门户可以帮助开发者缩短开发周期，减少出错频率并提高软件的质量。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:1:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"为什么需要开发者门户 当公司管理的组件规模足够大时，开发团队经常会面临如下的问题： 这个组件的源码在哪里？负责人是谁？本周oncall是谁？ 这个组件的使用文档在哪里？ 哪些组件会调用这个组件的API？ 这个服务组件有哪些依赖？它又被哪些组件所依赖？ 这个组件还在建设中吗？还是已经应用到产品环境中了？抑或是已经过时了？ 这个服务组件部署在哪些环境上？用的是什么版本？ 正如Spotify提出的速度悖论（Speed Paradox）所言：随着公司发展速度变快，软件生态会变得更加复杂、更加碎片化，反而会限制其发展速度。 幸运的是，开发者门户可以将这些资源集中管理起来，提升开发效率。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:2:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"开发者门户的核心优势 减少资源管理成本，提高生产力：开发者门户将每个软件与其使用资源（如文档、API、基础设施、依赖组件、release版本、部署环境等）关联起来。开发者可以快速找到所需的资源，减少软件管理成本。 推进责任制，增强协作：每个组件都有负责人，可以快速地响应软件相关问题，有助于组件在公司范围内的复用，避免重复造轮子，解决组件孤岛的问题。 一站式管理，提升组件开发效率：自助服务支持在不依赖外部团队的前提下快速构建、配置、管理组件，避免了繁杂的工作流程，提升研发效率。 完整生命周期管理，提升组件质量：开发者门户管理着软件的完整生命周期。模板脚手架提供了软件构建的最佳实践，使得软件在开发之初就能符合相关规范；强大的可观测性可以展示软件的完整性、成熟度、运行时关键指标，以多维度push负责人对组件进行持续打磨，提升组件质量。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:3:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"开发者门户涉及的领域 软件目录：展示所有软件和资源的列表。 追踪微服务、系统库、项目、环境、依赖等资源。易于管理大规模微服务，梳理调用关系，推动组件间复用。 作为软件资源的唯一source of truth，与metrics、自助服务、自动化工作流打通。 Scorecard：展示组件健康程度、可靠性、成熟度等指标，提供软件质量可见性。常用的指标如下： 运维成熟度 SLO 测试覆盖率 Oncall 健康程度（告警情况） 应用健康度 云设施健康度 DORA metrics 发版频率 变更应用到生产环境的时间 故障恢复时间 变更故障率 产品checklist（Operational Readiness） 自助服务：让开发者在不依赖外界帮助的前提下，可以自己构建和配置软件。 Scaffolder：开发者门户提供多种创建软件的模板，可以帮助开发者自助创建出一个软件框架。通常脚手架的提供方会集成该软件技术领域的业界规范与最佳实践，使软件在创建阶段就有较高的质量。 自动化集成：开发者门户提供对常见平台（如Github、Terraform等）的自动化集成。开发者可以按需进行配置，而这些过程只需要填填表单、点点按钮即可。 自定义前端组件：开发者门户提供工具与向导支持用户自定义资源表单、向导、步骤等前端组件。只需要提供schema、rules便可以构建出风格统一的前端界面。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:4:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"开发者门户代表产品 目前开发者门户产品主要是Spotify开源的Backstage；基于Backstage扩展的SaaS产品Frontside、roadie等；以及公有云No-Code开发者门户Port、Cortex等。 本节简单对Backstage和Port和核心特性进行讨论。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:5:0","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"Backstage Backstage将服务所涉及的资源（包括代码、文档、环境、基础设施和负责人）聚拢在一处，从而使完整的开发链路流程化，统一开发体验。 核心特性 软件目录 软件目录将软件资源汇集在一处管理，每个团队都可以看到全局的资源视角。 软件目录记录着资源丰富的元信息，让开发者可以更了解软件的特性。 软件资源不仅限于服务，还包括库、网站、环境、pipeline等类型资源，以及他们的依赖关系。 每个资源都有负责人，避免了软件孤岛的出现。 组件间依赖图 软件模板 集成了组织内研发的最佳实践，规范地构建出一个软件应用。 以自助服务的方式在界面上创建一个代码仓库，并注册到软件目录中。 使用模板创建组件 文档中心 以doc-like-code的方式编写文档，并可以实时预览效果。 文档与代码在一处管理，无需额外建站，易于维护。 文档与服务放在一起，避免一个产品一个独立的站点，易于查找。 backstage创建的文档站点 定制化搜索 按需定制索引，在门户范围内全局搜索。 搜索可以扩展到第三方平台（如stack overflow等）。 管理K8s服务 专注于服务，而不是集群。 优化部署、服务、日志等资源的展示。 UI与本地集群、云提供商无关，切换丝滑。 产品特点 开源，需要二次开发。 强大的插件生态大大简化外部平台集成。 资源定义格式采用K8s Object格式，易于理解。 数据模型灵活性受限，仅支持component, API, resource, system, domain 和 group六种通用资源类型。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:5:1","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"Port Port是一个No-Code开发者门户，构建一个单source-of-truth平台，以自助服务的方式对资源进行管理，并提供免费版与企业版两种方案。 核心特性 软件目录 自定义数据模型。 自定义视图与部件。 强大的资源关系图。 开发者自助服务 资源脚手架。 界面操作构建前端表单及向导。 外部通用平台一键集成。 RBAC准入控制 细粒度的资源权限控制，避免越权操作。 基于权限的自助工作流。 集成SSO。 审计日志记录门户事件流。 资源粒度的权限控制 用户操作资源审计 Scorecards 创建一个质量文化，跟踪软件的成熟度、环境健康程度、DORA指标、产品就绪状态等状态。 成熟度和健康度评分版 自动化工作流 快速搭建DevOps工作流，并将结果集成到软件目录中。 丰富的订阅机制，触发通知、告警、垃圾清理等行为。 产品特点 专业团队提供定制化方案。 公有云上提供的SaaS平台，开箱即用。 No-Code风格，重前端，主打自助服务。 以蓝图描述数据模型，可以灵活定制。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:5:2","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["整理与总结"],"content":"产品对比 backstage vs port详细地比较了二者支持的特性及使用的侧重点。 ","date":"2023-04-22","objectID":"/posts/dev-portal/:5:3","tags":["portal"],"title":"探秘开发者门户","uri":"/posts/dev-portal/"},{"categories":["工具党"],"content":"无需科学上网，可以复用OpenAI账号的Chat Web。 ","date":"2023-04-22","objectID":"/posts/chatweb/:0:0","tags":["tools"],"title":"安利一款好用的ChatGPT Web","uri":"/posts/chatweb/"},{"categories":["工具党"],"content":"项目介绍 近来发现一个好用的ChatGPT网页应用。 具有以下优势： 可以使用vercel一键部署一个chat页面，也可以一条命令在本地部署，操作十分方便。 无需连vpn即可访问，便于与不会科学上网、没OpenAI账号的广大亲朋好友分享使用。 可以通过密码认证的方式共享构建者的账号，也可以自己配置OpenAI API Key复用此UI。 以markdown方式导出聊天记录，无需额外的浏览器插件。 示例：以Markdown导出聊天内容。 # 开发门户分析工具 ## 来自你的消息: 开发者门户中，scorecard有什么用？ ## 来自 ChatGPT 的消息: 开发者门户中的scorecard可以用来衡量和监控开发者在开发过程中的整体表现。它可以跟踪开发者在某个时间段内的代码提交数量、错误率、代码质量、代码审查通过率等指标，帮助团队管理者或上级领导更好地评估开发者的表现和绩效，并及时发现和解决问题。通过scorecard的监控和评估，开发团队可以更好地协同工作，提高项目交付质量和效率。 ## 来自你的消息: 如何开发一个scorecard? ## 来自 ChatGPT 的消息: 开发一个scorecard需要以下几个步骤： 1. 确定需要衡量的指标。这些指标应该与团队或项目的目标相关，并可以帮助衡量开发者的绩效。例如，代码质量、代码提交数量、错误率等。 2. 确定指标的权重。根据指标的重要性来赋予不同的权重，在计算总分时需要考虑这些权重。 3. 设计评分标准。根据指标的权重和分值范围来设计评分标准。例如，代码质量评分可以根据静态代码分析的结果，分为A级、B级、C级等，每个级别赋予不同的分值。 4. 开发评估工具。可以通过开发评估工具来自动化收集和评估指标。例如，代码质量可以通过静态代码分析工具来评估；代码提交数量可以通过版本控制工具来统计。 5. 计算总分并生成scorecard。根据指标的权重和评分标准，计算每个开发者的总分，并生成scorecard。scorecard可以是一个表格或图表，可视化地展示开发者的绩效。 ","date":"2023-04-22","objectID":"/posts/chatweb/:1:0","tags":["tools"],"title":"安利一款好用的ChatGPT Web","uri":"/posts/chatweb/"},{"categories":["工具党"],"content":"注意事项 密码谨慎泄露，避免太多人消耗你的token。 本地搭建时，需要指定OpenAI proxy避免无法直连，使用docker本地部署可以使用如下命令： docker run -it --rm -p 3000:3000 \\ -e OPENAI_API_KEY=\u003cYOUR OPENAI KEY\u003e \\ -e CODE=\u003cYOUR PASSWARD\u003e \\ -e BASE_URL=\"api.openai-proxy.com\"\\ yidadaa/chatgpt-next-web ","date":"2023-04-22","objectID":"/posts/chatweb/:2:0","tags":["tools"],"title":"安利一款好用的ChatGPT Web","uri":"/posts/chatweb/"},{"categories":["工具党"],"content":"Demo site 官方：https://chat-gpt-next-web.vercel.app 墙内：https://chat.raygecao.cn ","date":"2023-04-22","objectID":"/posts/chatweb/:3:0","tags":["tools"],"title":"安利一款好用的ChatGPT Web","uri":"/posts/chatweb/"},{"categories":["探索与实战"],"content":"嫌一句句地问ChatGPT太麻烦？试试AutoGPT吧！ ","date":"2023-04-20","objectID":"/posts/autogpt/:0:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"背景 近来ChatGPT引爆了生成式AI的浪潮，基于ChatGPT的应用喷涌而出。ChatGPT提供一种交互式的聊天方式回答使用者提出的问题，通常这种答案不够全面，需要提供多轮提示才可以获得满足预期的结果。 这存在几个限制： 需要大量的人工介入。 非专业人士很难给出专业的提示。 很难给出很全面的提示。 为了可以一次性获得更全面的内容，AutoGPT粉墨登场。 ","date":"2023-04-20","objectID":"/posts/autogpt/:1:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"AutoGPT是什么 AutoGPT实际上是个自主人工智能代理，它的目标是make GPT-4 fully autonomous。 下面总结了对AutoGPT的一些定位： 它可以通过自主地链接思维来实现特定的目标，尝试将目标分解为子任务，并使用互联网和其他工具进行自动循环以实现它。 它可以自我提示并生成完成任务所需的每个必要提示。 它依赖于AI代理来根据预定义的目标和规则做出决策并采取行动。 它拥有互联网访问权限，能够读写文件，并且具备短期和长期记忆力，以理解已经完成的工作。 它最令人印象深刻的一点是其自主反思和改进行为的能力。这是通过一个反馈循环实现的，该循环使用计划、批评、行动、阅读反馈和再次计划，就像拥有编码技能的个人教练一样。 ","date":"2023-04-20","objectID":"/posts/autogpt/:2:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"AutoGPT vs ChatGPT ChatGPT以交互式的方式回答使用者提出的问题，从而实现一个完整目标。而AutoGPT通过对目标进行分解，并不断进行反馈迭代，从而自主地解决整个目标。 ChatGPT交互方式 AutoGPT交互方式 从工作原理上看，ChatGPT有点类似于DNS迭代查询，而AutoGPT类似于DNS递归查询。 ","date":"2023-04-20","objectID":"/posts/autogpt/:3:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"AutoGPT的体验 目前基于AutoGPT的产品比较多，主流的是Auto-GPT，但其并没有提供一个易用的用户界面，因此使用cognosys进行体验。截止目前（2023.04.19），cognosys可以免费试用。 从体验上来看，AutoGPT主要将目标拆分为若干个子任务，分析返回的结果并进行追问，形成完整的目标结论。 其拆分子任务的策略也具有一定的规律： 搜集数据 分析数据 给出方案 而反馈循环一般是围绕上一轮结果中一些关键点开展，比如风险控制、可行性分析、步骤细化等。 下面将从两个案例分析AutoGPT自学习能力，为节省篇幅，省略回答内容。 ","date":"2023-04-20","objectID":"/posts/autogpt/:4:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"案例一：如何构建开发者门户？ 第一轮：任务拆解 研究现有的开发门户并分析它们的特性和功能，以确定最重要和最相关的内部门户。 确定组织对开发门户的具体需求和要求，包括用户角色、访问控制、协作工具以及与现有系统的集成。 设计和开发内部开发门户，使用以用户为中心的方法确保可用性和可访问性，并与具有代表性的用户组一起测试门户以确保满足他们的需求和期望。 第二轮：方案细化 创建可集成到内部开发门户以增强其功能和可用性的潜在第三方应用程序列表。 制定培训用户如何有效使用内部开发门户的计划，包括创建用户指南和举办培训课程。 对内部开发门户进行安全审计，以确保其符合数据保护和访问控制的行业标准。 可以看出AutoGPT会按照上述策略拆分成子任务，并根据子任务返回的结果进行反馈追问，使得目标的解决方案更全面。 ","date":"2023-04-20","objectID":"/posts/autogpt/:4:1","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"案例二：如何把大象放进冰箱？ 第一轮：任务拆解 识别和分析一只普通大象的尺寸和尺度，以确定需要多大的冰箱才能容纳一只大象。 制定一个逐步计划，以安全和人道的方式将大象运输到冰箱中，并考虑潜在的障碍和挑战。 为整个过程创建详细的成本分析，包括合适的冰箱、运输以及任何必要的设备或人员的成本。 第二轮：成本预估 研究并确定可容纳大象的合适冰箱的最佳选择，并确定其相关成本。 进行市场调查以确定最具成本效益的大象运输方式并确定相关成本。 为运输过程中可能出现的意外费用和不可预见的费用制定预算。 第三轮：风险评估 考虑到大象的体型和健康状况，研究并确定安全和人道地镇静运输大象的最佳方法。 识别和分析运输过程中可能出现的潜在障碍和挑战，例如狭窄的道路或桥梁，并制定应急计划来解决这些问题。 进行风险分析以识别潜在危险并制定减轻危险的策略，确保大象和所有参与运输过程的人员的安全。 第四轮：可行性保障 进行可行性研究以确定将大象运输到冰箱中的伦理和实际影响，同时考虑动物福利和公众认知。 确定并与合格的专业人员团队合作，包括兽医和运输专家，以确保大象的安全和人道运输。 制定沟通计划，告知利益相关者（包括公共和动物福利组织）运输过程，并解决他们可能有的任何疑虑或问题。 针对此问题，ChatGPT给出了如下的答案。 打开冰箱门。 把大象的四条腿一只一只地放进冰箱里。 仔细压紧大象的身体，推进冰箱里。 关上冰箱门，确保大象完全被封闭在里面。 给出“标准答案”背后的原因值得我们思考，当然只能简单的猜一猜： 它能听理解人类的玩笑话。 它觉得这是不可能完成事件所以回答的比较敷衍。 他无法详细回答这么宽泛的问题呢。 ","date":"2023-04-20","objectID":"/posts/autogpt/:4:2","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"使用总结 AutoGPT以自主型AI代理的方式不断进行自强化，减少了人工干预。它进一步将互联网与AI结合起来，从一个应答式的AI工具向提供通用解决方案的AI应用迈进。相信在不久的将来，它会渗透到各行各业。 ","date":"2023-04-20","objectID":"/posts/autogpt/:5:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"引用 Why You Need To Know About AutoGPTs AUTO-GPT VS CHATGPT: HOW DO THEY DIFFER AND EVERYTHING YOU NEED TO KNOW AUTOGPT: THE AI THAT CAN SELF-IMPROVE IS SCARY! INTRODUCTION TO AUTOGPT 封面图引自 Auto GPT vs ChatGPT: What’s the difference? ","date":"2023-04-20","objectID":"/posts/autogpt/:6:0","tags":["tools"],"title":"AutoGPT初体验","uri":"/posts/autogpt/"},{"categories":["探索与实战"],"content":"本文简述Terraform HCL语言；提供基于Docker与AWS平台供应基础设施的demo；并对多环境管理进行探索。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:0:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"模板引擎 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:1:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"文件结构 Terraform识别以下几类特殊文件： *.tf ：资源声明文件，用于声明所要创建的资源。 *_override.tf：资源后处理文件，用于覆盖已存在资源的某些部分。 .terraform.lock.hcl：锁定provider的版本。 terraform.tfvars：给定已声明变量一组特定值。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:1:1","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"HCL语法 HCL全称为HashiCorp Configure Language，是由HashiCorp专门为Terraform设计的声明式配置语言，用来描述如何管理一组的基础设施。 HCL通用语法块为： \u003c块类型\u003e \"\u003c资源类型\u003e\" \"\u003c资源名\u003e\" { # Block body \u003c属性名\u003e = \u003c属性值\u003e # Argument } 其中 块类型包含以下几种常见类型： 块类型 作用 terraform 声明provider版本及来源 provider 配置provider resource 定义并配置资源对象 data 获取外部资源 variable 定义模块输入变量 output 定义模块输出变量 locals 定义本地变量 module 定义模块引用 资源类型及Arguments由provider定义。 资源名由用户自定义，用来在引用模块内资源，因而避免相同资源同名的问题。并非所有块类型都需要额外的资源名（如variable、output、provider、locals等） 以创建一个redis docker container为例体验一下HCL： # 声明docker provider terraform { required_providers { docker = { source = \"kreuzwerker/docker\" version = \"~\u003e 3.0.1\" } } } # 配置provider，必需，即使没有arguments provider \"docker\" {} # 创建redis docker image resource resource \"docker_image\" \"redis\" { name = \"redis:5.0.5-alpine\" keep_locally = true } # 创建redis docker container resource，引用上一步创建的docker image id resource \"docker_container\" \"redis\" { name = \"storage\" image = docker_image.redis.image_id command = [\"--requirepass\", \"dea1452fe9133ea28e60b25f70fa93c43bcfeca9648d0cb470a473f563c91af6\"] } ","date":"2023-03-05","objectID":"/posts/terraform-practice/:1:2","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"隐式依赖 HCL中通过资源引用来声明隐式依赖，即R1的某些输入参数通过引用R2资源的输出结果建立起了R1对R2的隐式依赖。Terraform在进行资源创建时，会先创建R2再去创建R1，删除顺序与创建顺序相反。 上例中docker_container.redis.image引用了docker_image.redis.image_id，即docker_container.redis依赖于docker_image.redis。 $ terraform apply --auto-approve # ...... some resource plan output ...... docker_image.redis: Creating... docker_image.redis: Creation complete after 0s [id=sha256:ed7d2ff5a6232b43bdc89a2220ed989f532c3794422aa2a86823b8bc62e71447redis:5.0.5-alpine] docker_container.redis: Creating... docker_container.redis: Creation complete after 1s [id=583c145b78d31bc28d8c993087c9bbd077be2dd6c60f51da6f78299880958ae7] $ terraform destroy --auto-approve # ...... some resource plan output ...... docker_container.redis: Destroying... [id=583c145b78d31bc28d8c993087c9bbd077be2dd6c60f51da6f78299880958ae7] docker_container.redis: Destruction complete after 1s docker_image.redis: Destroying... [id=sha256:ed7d2ff5a6232b43bdc89a2220ed989f532c3794422aa2a86823b8bc62e71447redis:5.0.5-alpine] docker_image.redis: Destruction complete after 0s ","date":"2023-03-05","objectID":"/posts/terraform-practice/:1:3","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"模块 为了增强功能块复用，Terraform提供了强大的模块化能力。 模块通常包含一组一起使用的资源，通常在一个目录下定义多个tf文件以实现一个模块。 模块通常分为两类： 根模块：在工作目录（执行terraform CLI时所在的目录）下的模块。 子模块：被其他模块引用的模块，子模块可以是自定义的本地模块，也可以使用registry中第三方提供的模块。模块引用可以嵌套，但嵌套深度不应过深。 模块之间的交互主要有两种方式： 父模块通过配置子模块属性向子模块传递input variables。 子模块通过output variables向父模块返回相关的配置值供其引用。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:1:4","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"Terraform in Docker Terraform可以构建、修改及移除Docker基础设施。Docker provider提供了network、volume、image、container等资源，以提供对Docker操作的完整支持。 Terraform Docker Tutorial详细描述了如何部署一个nginx docker container，配置与上例类似。 我们以部署一个带存储后端（redis）的留言板（guestbook）为例，探索使用Terraform部署两个具有依赖关系的docker container。 该场景资源依赖关系可以描述为下图： docker资源的依赖关系图 具体实现可参考示例代码，工作方式与docker compose极为相似。 然而Docker资源属于上层组件，Terraform并不具备管理优势，其核心价值在于公有云基础设施供应。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:2:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"Terraform in AWS 通常，云提供商为了满足多样的商业需求，会提供与计算、存储、网络相关的多种功能服务。这些厂商提供了易用的UI界面及api供用户管理相关的资源，但存在以下问题： 服务种类众多，每个服务又包含数十种配置，给用户带来大量的学习成本。 流程繁杂，用户需要按顺序地配置、启动服务，并进行资源关联，易错且低效。 单从创建一个强隔离性的nginx ec2 instance而言，用户不得不手动创建vpc、subnet、internet gateway、route table等网络服务以及ec2、security group、key-pair等计算资源。构建过程需要对这些资源手动创建并正确配置，清理过程同样需要手动移除各个服务。如果考虑在云上维护多个环境，那管理成本无法估量。 Terraform将我们从繁杂的基础设施管理中解救出来： 声明式配置简化我们在界面上点点点的操作，并降低出错的概率。 资源按依赖顺序并发构建，大幅提升基础设施的部署速度。 模块化供应屏蔽了大量内部细节，比如aws_vpc module将vpc、subnet、internet gateway、route table等资源封装起来。 示例代码展示了构建一个guestbook ec2 instance的方法，并将相应的公网ip输出出来。 $ terraform apply --auto-approve # ..... some resource plan output ...... aws_key_pair.ssh: Creating... aws_vpc.my-vpc: Creating... aws_key_pair.ssh: Creation complete after 1s [id=dev_instance_key] aws_vpc.my-vpc: Creation complete after 3s [id=vpc-04f4f4619c774d27d] aws_internet_gateway.my-igw: Creating... aws_subnet.my-subnet: Creating... aws_security_group.my-sg: Creating... aws_internet_gateway.my-igw: Creation complete after 0s [id=igw-00d00c50080474895] aws_subnet.my-subnet: Creation complete after 0s [id=subnet-060ff42389821a4e4] aws_route_table.my-route-table: Creating... aws_route_table.my-route-table: Creation complete after 2s [id=rtb-0c150344ed47e3c8b] aws_route_table_association.a-rtb-subnet: Creating... aws_route_table_association.a-rtb-subnet: Creation complete after 1s [id=rtbassoc-0c8c6b265e4333216] aws_security_group.my-sg: Creation complete after 4s [id=sg-0ace2bfadd865994b] aws_instance.my-server: Creating... aws_instance.my-server: Still creating... [10s elapsed] aws_instance.my-server: Still creating... [20s elapsed] aws_instance.my-server: Still creating... [30s elapsed] aws_instance.my-server: Creation complete after 35s [id=i-0fee61317795aab3b] Apply complete! Resources: 8 added, 0 changed, 0 destroyed. Outputs: server-ip = \"13.231.202.24\" 通过curl 13.231.202.24:3000可以访问留言板应用。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:3:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"多环境管理 一般的产品迭代过程分为需求研发、质量测试、功能上线等阶段。在不同的阶段，我们通常需要维护一套独立的环境以达到当前阶段的目的，比如通常我们需要维护dev、stage、prod三个环境。Terraform可以帮助我们简化多环境管理，越来越多的多环境管理的最佳实践被提了出来。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"基于Terraform workspace管理 Terraform workspace用于描述存放在同一backend的state文件。某些backend支持存放多个state文件，因此支持通过多个workspace进行环境管理。 Terraform支持workspace子命令，用以列举、创建、切换workspace。一个workspace对应着一个环境，环境的切换对应于workspace的切换，初始仅存在一个default workspace。 为支持多环境使用不同的配置，Terraform提供了特殊的插值表达式terraform.workspace，Terraform在解析配置文件时，会将此标识替换成当前的workspace名字。下例展示了两种常见的用法： resource \"aws_instance\" \"my-server\" { ami = data.aws_ami.amazon-linux-image.id instance_type = \"t2.micro\" associate_public_ip_address = true subnet_id = aws_subnet.my-subnet.id vpc_security_group_ids = [aws_security_group.my-sg.id] availability_zone = var.available_zone key_name = aws_key_pair.ssh.key_name tags = { # 使用workspace区分实例标识 Name = \"${terraform.workspace}-inst\" } # 使用workspace切换执行脚本 user_data = file(terraform.workspace == \"dev\" ? \"guest1.sh\" : \"guest2.sh\") } 示例代码展示了使用多个workspace创建不同的ec2实例，每个workspace管理的基础设施相互独立。（此例workspace并未提交到VCS中，workspace管理于local backend） 使用workspace对多环境管理可以最大程度上复用配置代码，并且是Terraform官方提供的特性。 workspace的局限性 workspace管理的多环境必须共用同一backend，并不是所有backend都可以用workspace进行管理。官方提供了支持多workspace的backend。此外workspace不适用于存在证书、接入控制隔离要求的环境管理。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:1","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"基于多分支管理 基于git branch对多环境管理是十分常见的策略，大量企业使用不同的release分支管理产品的版本。这最大程度上保证了版本的隔离性，避免不同版本的特性相互污染。但另一方面放弃了代码复用的能力，尤其是当维护的分支规模较大时，一个通用的bugfix会引起cherry-pick风暴。 为支持不同环境使用不同的配置，通常将不变的内容以模板的形式存放在Terraform配置文件中，将可变的配置声明在variable.tf中，不同分支通过维护一组特殊的配置值（记录在terraform.tfvars中）来控制环境的配置。 示例仓库创建了dev分支与prod分支分别管理dev环境与prod环境，二者的区别在于对服务监听端口及应用镜像的tag配置不同。 如何提升复用能力？ 使用分支进行多环境管理本质上是对配置文件进行版本控制，可以通过配置与模板分开管理以提升代码复用能力。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:2","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"基于Terragrunt管理 Terragrunt是由Gruntwork.io开发的Terraform代码管理工具，其核心宗旨是提供一层thin wrapper实现DRY(Don’t Repeat Yourself)。其核心思想是将不变的配置模板与可变的配置值分开管理，以维护多环境的特异性配置，在设计上与kustomize异曲同工。 使用Terragrunt通常将配置模板和配置值放到不同的目录进行管理，如分别在modules、live目录下： live ├── dev │ └── docker │ └── terragrunt.hcl └── prod └── docker └── terragrunt.hcl modules └── docker ├── main.tf ├── output.tf └── variable.tf 在配置模板中，声明所需的输入变量： $ cat modules/docker/variable.tf variable \"external_port\" { description = \"External port for expose guestbook service\" type = number } variable \"guestbook_image_tag\" { description = \"The tag for the guestbook image\" type = string } 在patch文件中，通过维护不同的参数文件来控制环境的配置： terraform { source = \"../../../modules/docker\" } inputs = { external_port = 14000 guestbook_image_tag = \"v2\" } 在相应的配置目录下执行terragrunt apply即可实现对应环境的部署。参考完整示例代码。 Terragrunt CLI本质是将分离的配置与模板组合在一起，再去执行对应的Terraform CLI，这也是Terragrunt会被定位为Terraform的thin wrapper的原因。 Terragrunt 结合了workspace与multi-branch管理的优势并解决了二者存在的问题。但是其作为一个外部工具引入了额外的学习成本，并且目前在Terraform Cloud中无法使用。 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:3","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"三种环境管理方式的比较 Workspaces Branches Terragrunt Minimize code duplication ■■■■■ □□□□□ ■■■■□ See and navigate environments □□□□□ ■■■□□ ■■■■■ Different settings in each environment ■■■■■ ■■■■□ ■■■■■ Different backends for each environment □□□□□ ■■■■□ ■■■■■ Easy to manage multiple backends □□□□□ ■■■■□ ■■■■■ Different versions in each environment □□□□□ ■■□□□ ■■■■■ Share data between modules ■■□□□ ■■□□□ ■■■■■ Work with multiple modules concurrently □□□□□ □□□□□ ■■■■■ No extra tooling to learn or use ■■■■■ ■■■■■ □□□□□ Works with Terraform Cloud ■■■■■ ■■■■■ ■□□□□ 黑块越多表示越具优势。引自 https://blog.gruntwork.io/how-to-manage-multiple-environments-with-terraform-32c7bc5d692 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:4","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"多环境与Devops 在DevOps的实践领域中，一种高度自动化的方式是通过pipeline贯穿多个环境，从而实现代码直接上线的能力。 比如下面的案例中，一套代码控制多个环境使用，但通过一条pipeline将环境连接起来，从而实现自动化部署、测试、上线的能力。这依赖了Terraform基础设施构建的一致性。 pipeline横贯多个环境 具体的工作流如下： 向VCS提交代码。 通过CI将VCS中的模块拷贝发布到制品仓库中。 CD检测到有新版本的制品发布，会应用到测试环境，并执行自动化测试。 测试通过后，自动地（或人工触发）应用到线上环境。 infra自动化上线流程 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:4:5","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"使用Terraform Cloud实现自动化部署 Terraform Cloud是Hashicorp为支持Terraform工作流的平台，提供自动化管理基础设施的能力。其具备权限控制、密钥管理、托管状态文件等能力。 Terraform Cloud使用流程如下： 接入VCS provider，核心是向VCS中注入TC webhook。 创建workspace，可以指定具体的触发条件。 配置变量。包括tf文件中声明variable，Access Token相关的环境变量。 首次运行。官方明确指出：A workspace with no runs will not accept new runs from a VCS webhook. You must queue at least one run manually。 之后， 便可以根据workspace创建的触发条件完成自动化部署。 运行状态列表 ","date":"2023-03-05","objectID":"/posts/terraform-practice/:5:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["探索与实战"],"content":"参考文档 Terraform config language tutorial Terraform docker tutorial Terraform aws tutorial Terraform docker provider Terraform aws provider How to manage multiple environments with Terraform Terragrunt kubernetes kustomize 初体验 Using Pipelines to Manage Environments with Infrastructure as Code ","date":"2023-03-05","objectID":"/posts/terraform-practice/:6:0","tags":["IaC"],"title":"Terraform实践","uri":"/posts/terraform-practice/"},{"categories":["整理与总结"],"content":"Infrastructure as code is the process of managing and provisioning computer data centers through machine-readable definition files, rather than physical hardware configuration or interactive configuration tools. ","date":"2023-03-03","objectID":"/posts/iac-intro/:0:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"传统基础设施管理方案 传统的基础设施构建及运维通常由IT运维团队手动维护，这存在以下问题： 交互式管理方式会带来与人相关的问题。 容易引入human error，基础设施在整个系统的最底层，一些误配可能会降低系统的可用性。 手动管理基础设施步骤较多，人工操作效率低。 不同运维人员有着不同的经验背景、细心程度，难以保证基础设施一致性。 手动管理缺乏合适/统一的审计手段。当基础设施出现故障时，很难定位到当前环境经过了哪些操作、该问题是由哪一步操作引起的。 在云原生的环境下，基础设施的规模会极为庞大，手动管理基础设施会增加运维团队的负担与成本。 基础设施会成为开发团队与IT运维团队的壁垒，研发团队无法快速构建出一个应用部署环境，必须依托于运维团队去构建；而运维团队又会付出大量的时间去应付各个研发团队基础设施运维需求。团队耦合度高，降低了产品迭代效率。 为了解决基础设施构建难、配置难、追溯难等问题，IaC将基础设施代码化，以替代传统的手动构建方式。 ","date":"2023-03-03","objectID":"/posts/iac-intro/:1:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"IaC 概述 IaC利用human-readable配置文件管理基础设施，将基础设施的管理转化为代码的维护。IaC保证了基础设施供应的一致性，大大减少了人工成本及人为错误操作带来的风险。 IaC的强大体现在以下几个方面。 ","date":"2023-03-03","objectID":"/posts/iac-intro/:2:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"参数化 在产品迭代的不同阶段通常会维护多个环境，比如在开发阶段、QA阶段、上线阶段分别对应着dev、stage、prod等多个环境。这些环境上的基础设施是极为相似的，但通常略有不同。 比如dev环境一般会使用较新的版本、较新的技术、以及较为少量的资源Quota；而prod环境一般会使用更为稳定的版本以及更多的资源Quota。 我们当然可以维护多个配置文件与各个环境一一对应。但这一方面引入了大量的管理开销，另一方面由于存在大量代码冗余，每个通用的修改要复制很多次。 为了提供更灵活的配置能力与更强的复用能力，IaC通常会设计成参数化形式。即将不变的、安全的部分做成模板，将可变的、不安全的部分作为配置值渲染到模板中。这将多个配置文件的管理转换成单个模板文件及多个参数文件的管理。 使用参数化的IaC，可以将研发团队与运维团队进一步解耦。运维团队只需要提供一份经过充分测试及高度文档化的配置模板，研发团队可以对其按需进行参数化配置构建基础设施，这大大提升协作效率。 ","date":"2023-03-03","objectID":"/posts/iac-intro/:2:1","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"IaC on VCS VCS（git）为IaC提供版本管理的能力，具备如下优势： 支持多人协作。 便于追溯配置变更。 对基础设施进行版本化管理，易于实现不可变基础设施。 是实现DevOps、GitOps工作流的基础。 利用VCS的特性降低基础设施管理的复杂度。如基础设施操作权限映射到VCS的权限控制、使用ChatOps实现相关的审批流程等。 不可变基础设施 基础设施的升级采用安装新版本-\u003e切换流量-\u003e卸载旧版本的方式，避免原地升级不完全引起的状态不一致。 ","date":"2023-03-03","objectID":"/posts/iac-intro/:2:2","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"IaC in DevOps IaC使得基础设施的构建与运维变得自动化，结合CI/CD/Pipeline可以对基础设施进行自动化构建、校验、发布与部署，减少人工干预，加速产品迭代速度。 With the rise of DevOps and SRE approaches, infrastructure management becomes codified, automatable, and software development best practices gain their place around infrastructure management too. On one hand, the daily tasks of classical operations people changed and are more similar to traditional software development. On the other hand, software engineers are more likely to control their whole DevOps lifecycle, including deployments and delivery. —— gitlab IaC makes the environment part of a software release, managed as part of the CI/CD pipeline —— aqua ","date":"2023-03-03","objectID":"/posts/iac-intro/:2:3","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"使用场景举例 IT运维团队提供一份通用的配置模板，DEV团队可以快速在测试环境上构建出一套基础设施。 DEV团队可以以提交MR的形式对基础设施变更（如扩容）进行申请，由IT运维团队进行审批。 基础设施在不同环境（dev/stage/prod）下的迭代演进。如在dev环境下进行研发，经stage环境验证，通过后部署到prod环境。IaC保证同一版本基础设施在不同环境下部署的一致性。 ","date":"2023-03-03","objectID":"/posts/iac-intro/:3:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"Terraform Terraform 是HashiCorp推出的一款IaC产品，并具备以下特性： 定义一种HCL声明式语言，用以定义基础设施供应方式。 提供多种provider对接多个云平台，provider通过集成云平台的api将code转化为infrastructure。 定义了写配置 -\u003e 构建执行计划 -\u003e 执行部署工作流 Terraform架构图，引自terraform intro Terraform具备以下优势： 支持模块化，以提升代码复用率。模块的版本化有助于实现不可变基础设施，减少基础设施升级的复杂度。 声明式配置语言屏蔽了基础设施部署细节，保证了部署的一致性。 依赖管理机制可以保证基础设施按正确顺序部署。 使用VCS track配置的变更，通过state文件track实际部署的变化。 Terraform内部维护资源的依赖关系，可以并发进行资源部署，提升基础设施安装速度。 即可以管理低级组件（计算实例、存储、网络等），又可管理高级组件（DNS、SaaS等）。 插件式provider便于扩展，易于将terraform扩展到自研平台。 为强化基础设施的管理，HashiCorp还提出了terraform cloud，用以支持： 基础设施自动化供应 安全性管理 密钥管理 远程状态管理 私有化registry RBAC 审计、成本预估、大规模团队协作等能力 ","date":"2023-03-03","objectID":"/posts/iac-intro/:4:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["整理与总结"],"content":"参考文档 Infrastructure as code wikipedia What is Terraform? Infrastructure as Code and DevOps: DevOps Automation Reloaded Gitlab Infrastructure management 封面图引自 https://www.cisco.com/c/en/us/solutions/cloud/what-is-iac.html ","date":"2023-03-03","objectID":"/posts/iac-intro/:5:0","tags":["iac"],"title":"基础设施即代码 (IaC)","uri":"/posts/iac-intro/"},{"categories":["探索与实战"],"content":"https改造的核心工作是申请证书，本文重点探索内网应用的证书申请。 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:0:0","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"背景 近期在内网开发了个http协议的web应用，在分配子域名时，由于根域名配置了HSTS，在浏览器中访问过根域名后无法访问http协议的子域名，因而需要将访问该服务的协议改造成https。 改造成https有两个主流的方案：应用直接改造和使用https代理。本例以部署在localhost为例，并建立local.raygecao.cn指向127.0.0.1的A记录。 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:1:0","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"应用直接改造 将应用协议改造成https是最直接的一种方案，改造的核心工作是申请一个SSL证书。 一旦有了证书，应用的改造便极为简单，下面以golang为例展示https应用的改造： func main() { handler := http.HandlerFunc( func(writer http.ResponseWriter, request *http.Request) { writer.Write([]byte(\"Hello World\")) }) /* http log.Panicln(http.ListenAndServe(\":8080\", handler)) */ // https log.Panicln(http.ListenAndServeTLS(\":8080\", \"cert.pem\", \"key.pem\", handler)) } ","date":"2023-01-02","objectID":"/posts/go-https-cert/:2:0","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"内网CA颁发证书 对于内网环境，主流的方案使用内网CA颁发的证书。我们以本地CA模拟内网CA颁发证书的流程： 创建本地CA $ openssl genrsa -out ca.key 2048 # 创建CA key $ openssl req -x509 -new -nodes -key ca.key -days 730 -out ca.crt -subj \"/CN=test\" # 创建自签名证书，有效期为两年 使用本地CA签署证书 $ openssl genrsa -out server.key 2048 # 创建服务端key $ openssl req -new -key server.key -out server.csr -subj \"/CN=local.raygecao.cn\" # 创建CN为local.raygecao.cn的csr $ openssl x509 -req -in server.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 365 # 根据csr创建经本地ca签名过的证书，有效期为一年 server.key与server.crt即为该server的私钥与证书。 验证 使用curl验证时，会发现此证书并未被信任： $ curl https://local.raygecao.cn:8080 curl: (60) SSL certificate problem: certificate has expired More details here: https://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. HTTPS-proxy has similar options --proxy-cacert and --proxy-insecure. 使用浏览器访问会给出风险提示： 因为本地的CA并未被加入信任证书列表，所以浏览器会给出安全风险提示。解决方法是手动将其加入到信任证书列表，或者手动指定证书机构： $ curl https://local.raygecao.cn:8080 --cacert ca.crt Hello World 由于证书指定了CN为local.raygecao.cn，因如果以其他的域名访问此应用会报错：\u0016 $ curl https://localhost:8080 --cacert ca.crt curl: (51) SSL: certificate subject name 'local.raygecao.cn' does not match target host name 'localhost' 使用内网CA颁发证书可以加强对颁发证书控制的灵活度，比如证书有效期设置，证书的吊销与续约；此外还可以减少向公网CA申请证书的成本。但需要一个完善的同步机制以确保此CA可以被任意内网环境所信任。 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:2:1","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"公网CA颁发证书 如果内网中CA同步机制不完善的话，为了可以正常访问，可以向公网CA去申请证书。目前全球权威证书机构有很多，我们以Let’s Encrypt这个非盈利性证书机构为例获取证书。 Let’s Encrypt使用ACME标准来验证我们对域名的控制权威性，主要手段有二： http验证：Let’s Encrypt 会向ACME 客户端提供一个令牌，然后ACME 客户端将在web服务器的路径上放置指定文件，如果Let’s Encrypt在 web 服务器上找到了放置于正确地点的正确文件，则该验证被视为成功，便可以继续颁发证书。 dns验证： Let’s Encrypt 为 ACME 客户端提供令牌后，客户端将创建从该令牌和您的帐户密钥派生的 TXT记录，并将该记录放在 _acme-challenge.\u003cdomain\u003e 下。 然后 Let’s Encrypt 将向 DNS 系统查询该记录。 如果找到匹配项，便可以继续颁发证书。 由于Let’s Encrypt无法访问内网的web服务器，因此我们只能使用dns验证的方式。 首先我们需要安装ACME客户端，官方推荐certbot，申请流程为： $ sudo certbot -d \"local.raygecao.cn\" --manual --preferred-challenges dns certonly # some output... Please deploy a DNS TXT record under the name: _acme-challenge.local.raygecao.cn. with the following value: jvHmur0iIcTleblY1wrAHMM7TdUOjInFKu6p_Wtlk6o 配置完相关的DNS TXT记录后，便可以获得相关的证书及key： Successfully received certificate. Certificate is saved at: /etc/letsencrypt/live/local.raygecao.cn/fullchain.pem Key is saved at: /etc/letsencrypt/live/local.raygecao.cn/privkey.pem This certificate expires on 2023-04-02. These files will be updated when the certificate renews. 由于大多数浏览器都信任了Let’s Encrypt，因此整个内网环境都可以顺利访问此https应用。 注意 Let’s Encrypt 证书的有效期为90天，并且不可调整，官方推荐每60天自动续期一次。 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:2:2","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"使用https代理 除了原地改造应用的协议之外，一种比较间接的方式使用使用https反向代理。反向代理有多种，如haproxy，nginx，caddy等。其中caddy主打automatic HTTPS特性，因此我们使用caddy作为https反向代理。 caddy的自动化https特性旨在可以自动地将http应用代理成https，核心的原理是caddy会自动向Let’s Encrypt申请证书并定期进行续约（caddy实现了ACME客户端）。 但遗憾的是由于这种自动化https使用的是http验证方案，而内网服务又无法使用此验证方案，因此需要使用上述方式手动申请证书，并配置到caddy中。配置文件Caddyfile如下： local.raygecao.cn: { tls /etc/letsencrypt/live/local.raygecao.cn/fullchain.pem /etc/letsencrypt/live/local.raygecao.cn/privkey.pem reverse_proxy localhost:8081 } 该配置表明local.raygecao.cn会被反向代理到localhost:8081，并且禁用auto https，而使用外部证书。 开启一个http server监听8081端口，并执行caddy run --watch。由于浏览器大多信任了Let’s Encrypt，因此使用浏览器可以安全访问。 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:3:0","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["探索与实战"],"content":"改造应用 vs https代理 指标 改造应用 https代理 侵入性 有 无 证书动态更新 不支持 支持 性能 较高 较低（多一层转发） 引入外部依赖 否 是 ","date":"2023-01-02","objectID":"/posts/go-https-cert/:4:0","tags":["tls"],"title":"内网应用配置https","uri":"/posts/go-https-cert/"},{"categories":["踩坑记录"],"content":"gin路由注册要求是互斥的，engine.NoRoute可以提供默认路由的能力， 使用gin NoRoute对grpc gateway进行路由时却发生了正常返回结果但伴有404status code的问题。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:0:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"背景 某存储服务实现了两组grpc-gateway分别对外提供artifact服务与package服务，并使用Gin框架进行路由。 func main() { router := gin.Default() mux := runtime.NewServeMux() // 注册artifact与pkg ServiceHandler到mux上 // ... // 匹配/v1/*的path路由到mux中 router.Any(\"/v1/*path\", gin.WrapH(mux)) if err := router.Run(*flagListen); err != nil { panic(err) } } ","date":"2022-08-06","objectID":"/posts/gin-noroute/:1:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"需求 现需要为此服务新增openapi路由，并且同样具有/v1 prefix，路由表如下所示： route tree ","date":"2022-08-06","objectID":"/posts/gin-noroute/:2:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"实现方案 最初的想法是在最开头加上相应的route。 // ... router.GET(\"/v1/swagger/*path\", swaggerHandler) router.Any(\"/v1/*path\", gin.WrapH(mux)) //... 运行时报如下错误： panic: catch-all conflicts with existing handle for the path segment root in path '/v1/*path' gin的router在匹配路由时并不像三层路由表按定义的顺序一层一层匹配，而注册一组互斥的(method, pattern)用以全局匹配。而上述两个pattern是存在包含关系的，因此报错path conflict。由于mux内的二级路由比较多样，因此我们希望通过默认路由对mux进行路由改造。gin提供了NoRoute方法去处理匹配不上的路由，这正好可以满足此需求。 // ... router.GET(\"/v1/swagger/*path\", swaggerHandler) router.NoRoute(gin.WrapH(mux)) //... ","date":"2022-08-06","objectID":"/posts/gin-noroute/:3:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"问题描述 经过本地测试，服务可以正常返回，但上线时UI并不能正常工作。进一步测试时发现虽然api返回了正确结果，但是status code返回404，所以client无法正常工作。 $ curl -v localhost:3001/v1/artifacts # 省略Request输出 \u003c HTTP/1.1 404 Not Found \u003c Content-Type: application/json \u003c Date: Fri, 05 Aug 2022 06:04:45 GMT \u003c Content-Length: 26 \u003c * Connection #0 to host localhost left intact {\"artifacts\":[],\"total\":0}% ","date":"2022-08-06","objectID":"/posts/gin-noroute/:4:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"问题排查 上述请求返回了正确结果，因此可以确定其已经被artifact handler的逻辑正确处理，显然status code被gin置成了404。NoRoute的doc如此描述： // NoRoute adds handlers for NoRoute. It return a 404 code by default. func (engine *Engine) NoRoute(handlers ...HandlerFunc) { engine.noRoute = handlers engine.rebuild404Handlers() } 一个合理的猜测是gin默认使用200作为response status code，NoRoute在处理匹配不上的路由同时，会将默认status code置为404。 为了更深入探索此问题，做了如下三个实验 Case1：将/v1/artifacts 返回结果改造成internal error，观察status code。 Case2：给mux添加一个自定义路由/v1/artifacts/success（裸handler），总是返回200。 Case3：给mux添加一个自定义路由/v1/artifacts/fail，总是返回500。 // case1 func (m *Manager) ListArtifacts(ctx context.Context, request *v1.ListArtifactsRequest) (*v1.ListArtifactsResponse, error) { // /v1/artifacts handler return nil, fmt.Errorf(\"fail to list\") } // case2 mux.HandlePath(\"GET\", \"/v1/artifacts/success\", func(w http.ResponseWriter, r *http.Request, pathParams map[string]string) { w.WriteHeader(http.StatusOK) }) // case3 mux.HandlePath(\"GET\", \"/v1/artifacts/fail\", func(w http.ResponseWriter, r *http.Request, pathParams map[string]string) { w.WriteHeader(http.StatusInternalServerError) }) 上述请求的resp status code都是符合预期的。 # case1 $ curl -v localhost:3001/v1/artifacts \u003c HTTP/1.1 500 Internal Server Error \u003c Content-Type: application/json \u003c Date: Fri, 05 Aug 2022 06:19:14 GMT \u003c Content-Length: 50 \u003c * Connection #0 to host localhost left intact {\"code\":2, \"message\":\"fail to list\", \"details\":[]}% # case2 $ curl -v localhost:3001/v1/artifacts/success \u003c HTTP/1.1 200 OK \u003c Date: Fri, 05 Aug 2022 06:20:55 GMT \u003c Content-Length: 0 \u003c * Connection #0 to host localhost left intact # case3 $ curl -v localhost:3001/v1/artifacts/fail \u003c HTTP/1.1 500 Internal Server Error \u003c Date: Fri, 05 Aug 2022 06:21:24 GMT \u003c Content-Length: 0 \u003c * Connection #0 to host localhost left intact 上述现象基本表明在grpc ServiceHanlder中，如果处理正确，并不会显式向resp写200，但出错时会显式写特定的status code。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:5:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"源码追溯 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:6:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"gin侧 查看NoRoute配置的handler。 func (engine *Engine) rebuild404Handlers() { engine.allNoRoute = engine.combineHandlers(engine.noRoute) } 从名字来看，allNoRoute像是当匹配不到路由时会执行的handler，我们搜一下其引用位置。 func (engine *Engine) handleHTTPRequest(c *Context) { // 省略匹配路由的逻辑 // 省略HandleMethodNotAllowed处理逻辑 // ... c.handlers = engine.allNoRoute serveError(c, http.StatusNotFound, default404Body) } // ServeHTTP conforms to the http.Handler interface. func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) { c := engine.pool.Get().(*Context) c.writermem.reset(w) c.Request = req c.reset() // 置默认status code engine.handleHTTPRequest(c) engine.pool.Put(c) } func (w *responseWriter) reset(writer http.ResponseWriter) { w.ResponseWriter = writer w.size = noWritten w.status = defaultStatus // 200 } const ( noWritten = -1 defaultStatus = http.StatusOK ) ServeHTTP调用handleHTTPRequest，并且将默认status code设置为200，路由匹配失败会进入到serveError函数里。 func serveError(c *Context, code int, defaultMessage []byte) { // 默认status改成了404 c.writermem.status = code // 执行handler c.Next() if c.writermem.Written() { return } // ... } // Next should be used only inside middleware. // It executes the pending handlers in the chain inside the calling handler. // See example in GitHub. func (c *Context) Next() { c.index++ for c.index \u003c int8(len(c.handlers)) { // 调用注册的handler c.handlers[c.index](c) c.index++ } } 很明显这里将默认status code改成404，并去执行相应的handler，正常情况下handler会覆写resp里的status code，而grpc ServiceHandler看上去在执行成功后没有写200。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:6:1","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"Grpc-gateway侧 查看注册handler的逻辑，可以看到当handler返回错误后，会去调用runtime.HTTPError，而正确处理时调用runtime.ForwardResponseMessage。 func RegisterArtifactServiceHandlerServer(ctx context.Context, mux *runtime.ServeMux, server ArtifactServiceServer) error { mux.Handle(\"GET\", pattern_ArtifactService_ListArtifacts_0, func(w http.ResponseWriter, req *http.Request, pathParams map[string]string) { // ... if err != nil { runtime.HTTPError(ctx, mux, outboundMarshaler, w, req, err) return } // 调用ListArtifactHandler resp, md, err := local_request_ArtifactService_ListArtifacts_0(ctx, inboundMarshaler, server, req, pathParams) // 省略处理metadata逻辑 if err != nil { runtime.HTTPError(ctx, mux, outboundMarshaler, w, req, err) return } // 封装成http resp forward_ArtifactService_ListArtifacts_0(ctx, mux, outboundMarshaler, w, req, resp, mux.GetForwardResponseOptions()...) }) } forward_ArtifactService_ListArtifacts_0 = runtime.ForwardResponseMessage HTTPError将grpc code与http status code做了转换后，直接写到resp里，这解释了为何错误的状态码可以正常返回。 // DefaultHTTPErrorHandler is the default error handler. // If \"err\" is a gRPC Status, the function replies with the status code mapped by HTTPStatusFromCode. // If \"err\" is a HTTPStatusError, the function replies with the status code provide by that struct. This is // intended to allow passing through of specific statuses via the function set via WithRoutingErrorHandler // for the ServeMux constructor to handle edge cases which the standard mappings in HTTPStatusFromCode // are insufficient for. // If otherwise, it replies with http.StatusInternalServerError. // // The response body written by this function is a Status message marshaled by the Marshaler. func DefaultHTTPErrorHandler(ctx context.Context, mux *ServeMux, marshaler Marshaler, w http.ResponseWriter, r *http.Request, err error) { // ... // grpc code向http code转换 st := HTTPStatusFromCode(s.Code()) if customStatus != nil { st = customStatus.HTTPStatus } // 显式写入status code w.WriteHeader(st) if _, err := w.Write(buf); err != nil { grpclog.Infof(\"Failed to write response: %v\", err) } // ... } 最后看一看handler正确处理后，封装resp相关逻辑，只将handler返回的结果序列化写到resp.Body中，并没有显式写status code。 // ForwardResponseMessage forwards the message \"resp\" from gRPC server to REST client. func ForwardResponseMessage(ctx context.Context, mux *ServeMux, marshaler Marshaler, w http.ResponseWriter, req *http.Request, resp proto.Message, opts ...func(context.Context, http.ResponseWriter, proto.Message) error) { // ... if err := handleForwardResponseOptions(ctx, w, resp, opts); err != nil { HTTPError(ctx, mux, marshaler, w, req, err) return } var buf []byte var err error if rb, ok := resp.(responseBody); ok { buf, err = marshaler.Marshal(rb.XXX_ResponseBody()) } else { buf, err = marshaler.Marshal(resp) } if err != nil { grpclog.Infof(\"Marshal error: %v\", err) HTTPError(ctx, mux, marshaler, w, req, err) return } // 只marshal了结果写入了resp body，没有显式写status code if _, err = w.Write(buf); err != nil { grpclog.Infof(\"Failed to write response: %v\", err) } // ... } 这时需要考虑是否有地方可以通过options来做一些trick，从上述代码我们可以看到handleForwardResponseOptions传入了resp，猜测这里面有文章。 func handleForwardResponseOptions(ctx context.Context, w http.ResponseWriter, resp proto.Message, opts []func(context.Context, http.ResponseWriter, proto.Message) error) error { if len(opts) == 0 { return nil } // 这里的Opts实际上是mux.GetForwardResponseOptions()... // 可以在创建mux时通过runtime.WithForwardResponseOption自定义 for _, opt := range opts { if err := opt(ctx, w, resp); err != nil { grpclog.Infof(\"Error handling ForwardResponseOptions: %v\", err) return err } } return nil } 因为执行到ForwardResponseMessage一定意味着service handler执行成功了，显然可以通过ForwardResoponseOptions将200写到resp status code来解决此问题。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:6:2","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"解决方法 创建mux时指定WithForwardResponseOption向resp中写入200status code。 // ... //mux := runtime.NewServeMux() mux := runtime.NewServeMux(runtime.WithForwardResponseOption(func(ctx context.Context, writer http.ResponseWriter, message proto.Message) error { writer.WriteHeader(http.StatusOK) return nil })) // ... 测试一切正常。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:7:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"后记 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:8:0","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"ResponseWriter标准 有小伙伴指出在http.ResposeWriter接口中的Write方法里有个重要说明：If WriteHeader has not yet been called, Write calls WriteHeader(http.StatusOK) before writing the data。这表明在没有显式写状态码时，调用Write前会写个200。 // A ResponseWriter interface is used by an HTTP handler to // construct an HTTP response. // // A ResponseWriter may not be used after the Handler.ServeHTTP method // has returned. type ResponseWriter interface { // Header returns the header map that will be sent by // WriteHeader. The Header map also is the mechanism with which // Handlers can set HTTP trailers. // // Changing the header map after a call to WriteHeader (or // Write) has no effect unless the modified headers are // trailers. // // There are two ways to set Trailers. The preferred way is to // predeclare in the headers which trailers you will later // send by setting the \"Trailer\" header to the names of the // trailer keys which will come later. In this case, those // keys of the Header map are treated as if they were // trailers. See the example. The second way, for trailer // keys not known to the Handler until after the first Write, // is to prefix the Header map keys with the TrailerPrefix // constant value. See TrailerPrefix. // // To suppress automatic response headers (such as \"Date\"), set // their value to nil. Header() Header // Write writes the data to the connection as part of an HTTP reply. // // If WriteHeader has not yet been called, Write calls // WriteHeader(http.StatusOK) before writing the data. If the Header // does not contain a Content-Type line, Write adds a Content-Type set // to the result of passing the initial 512 bytes of written data to // DetectContentType. Additionally, if the total size of all written // data is under a few KB and there are no Flush calls, the // Content-Length header is added automatically. // // Depending on the HTTP protocol version and the client, calling // Write or WriteHeader may prevent future reads on the // Request.Body. For HTTP/1.x requests, handlers should read any // needed request body data before writing the response. Once the // headers have been flushed (due to either an explicit Flusher.Flush // call or writing enough data to trigger a flush), the request body // may be unavailable. For HTTP/2 requests, the Go HTTP server permits // handlers to continue to read the request body while concurrently // writing the response. However, such behavior may not be supported // by all HTTP/2 clients. Handlers should read before writing if // possible to maximize compatibility. Write([]byte) (int, error) // WriteHeader sends an HTTP response header with the provided // status code. // // If WriteHeader is not called explicitly, the first call to Write // will trigger an implicit WriteHeader(http.StatusOK). // Thus explicit calls to WriteHeader are mainly used to // send error codes. // // The provided code must be a valid HTTP 1xx-5xx status code. // Only one header may be written. Go does not currently // support sending user-defined 1xx informational headers, // with the exception of 100-continue response header that the // Server sends automatically when the Request.Body is read. WriteHeader(statusCode int) } 这看似与上述排查结论相悖。但进一步看，这个结构是个nterface，在interface内声明这个规则并不能强制约束实现方去如此做。恰好gin Context中的相关接口实现恰好打破了这个规则。 func (w *responseWriter) Write(data []byte) (n int, err error) { // 装饰了一层写特定状态码的逻辑 w.WriteHeaderNow() n, err = w.ResponseWriter.Write(data) w.size += n return } func (w *responseWriter) WriteHeaderNow() { // 如果之前没Write过，则将保存的状态码写到resp里 if !w.Written() { w.size = 0 w.ResponseWriter.WriteHeader(w.status) } } func (w *responseWriter) WriteHeader(code int) { if code \u003e 0 \u0026\u0026 w.status != code { if w.Written() { debugPrint(\"[WARNING] Headers were already written. Wanted to override status code %d with %d\", w.status, code) } // 仅更新内部状态码，待Write data时，将内部状态码直接写到resp里 w.status = code } } gin 对Write装饰了一层写内部状态码的行为，用以对默认状态码的支持。 ","date":"2022-08-06","objectID":"/posts/gin-noroute/:8:1","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["踩坑记录"],"content":"其他方案 后续跟web经验丰富的同事互动了一下，学到了两种替代方案： 方案一：预写默认状态码发生在router层，避免掀开引擎盖子。 mux := newMux() r.NoRoute(gin.WrapF(func(writer http.ResponseWriter, request *http.Request) { writer.WriteHeader(200) mux.ServeHTTP(writer, request) })) 方案二：自定义转发规则，将部分路由从gin框架内提到外部。 r := gin.Default() r.GET(\"/v1/swagger/\", func(c *gin.Context) { c.JSON(200, \"swagger\") }) mux := newMux() h := http.HandlerFunc(func(writer http.ResponseWriter, request *http.Request) { path := request.URL.Path if strings.HasPrefix(path, \"/v1\") \u0026\u0026 !strings.HasPrefix(path, \"/v1/swagger\") { // 匹配 v1/!swagger pattern的，由mux serve mux.ServeHTTP(writer, request) } else { // 匹配 v1/swagger pattern的，由gin router serve r.ServeHTTP(writer, request) } }) server := httptest.NewServer(h) defer server.Close() ","date":"2022-08-06","objectID":"/posts/gin-noroute/:8:2","tags":["go"],"title":"Gin使用NoRoute实现默认路由踩坑总结","uri":"/posts/gin-noroute/"},{"categories":["探索与实战"],"content":"Go Writer 和 Reader接口的设计遵循了Unix的输入和输出，一个程序的输出可以是另外一个程序的输入，与之类比，Reader将流中的数据写进缓冲中，是一种数据的流入；Writer将缓冲的数据写至流，是一种数据的流出。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:0:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"问题背景 ToB的业务场景中的一种常见模式是私有云部署。对于一些保密性质的机关单位，应用只能在私有网络下部署，因此将数据包、镜像等部署所需的artifact克隆到用户现场便成为私有化交付中重要的一环。 完整流程大致如下： 解析对应的应用编排包及依赖，并下载所需的镜像及数据包（artifact） 将artifacts（可能需要压缩）、编排包pack成tar包 将tar包通过离线存储设备带到现场 在现场网络环境下将tar包load到私有化集群中 项目中采用virtual FS来实现封包/解包函数，借助两个helper函数： // ArchiveFS 将fs封包成tar并写到buf中 func ArchiveFS(buf io.Writer, files fs.FS) error { tw := tar.NewWriter(buf) err := fs.WalkDir(files, \"\", func(path string, entry fs.DirEntry, err error) error { if err != nil { return err } if path == \".git\" || path == \".idea\" { return filepath.SkipDir } // don't put \"\" into tar, since it can't be parsed if path == \"\" { return nil } file, err := files.Open(path) if err != nil { return err } defer file.Close() info, err := entry.Info() if err != nil { return err } hdr, err := tar.FileInfoHeader(info, \"\") if err != nil { return err } hdr.Name = normalizeFilepathSeparator(path) if info.IsDir() { hdr.Mode = 0o755 } else { hdr.Mode = 0o644 } if err := tw.WriteHeader(hdr); err != nil { return err } // dir must be writen to tar header for index files within it if info.IsDir() { return nil } if _, err := io.Copy(tw, file); err != nil { return err } return nil }) tw.Close() if err != nil { return err } return nil } // LoadArchive 将tar解包到memory fs中 func LoadArchive(r io.Reader) (afero.Fs, error) { t := tar.NewReader(r) fs := afero.NewMemMapFs() for { hdr, err := t.Next() if err == io.EOF { break } if err != nil { return nil, err } if hdr.FileInfo().IsDir() { path := hdr.Name fs.MkdirAll(path, 0o755) fs.Chtimes(path, time.Now(), time.Now()) continue } var buf bytes.Buffer size, err := buf.ReadFrom(t) if err != nil { panic(\"tarfs: reading from tar:\" + err.Error()) } if size != hdr.Size { panic(\"tarfs: size mismatch\") } afero.WriteFile(fs, hdr.Name, buf.Bytes(), 0o644) } return fs, nil } 起初，这两个函数只是作为编排包（几KB）的封包/解包helper。后来引入了带artifact的全量打包，为了方便就复用了这两个helper函数。最终在测试时发现在模拟现场加载tar包时出现OOM的问题。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:1:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"问题分析 上述两个helper的Reader/Writer的入参是bytes.Buffer，virtual FS都是memory FS。这样就导致每调用一次helper函数时，都会在内存中新增一份全量包的拷贝。对于应用编排包而言，其大小不过几KB，完全可以忽略掉内存的变化。而当引入了镜像及数据包这类size比较大的artifact时，pack出的全量包一度达到20+GB。即使在内存中完整存在一份全量包，都可能引入OOM的风险。因此我们考虑通过对IO进行优化，避免将全量包加载到内存。 经过分析，我们的处理流程中有以下部分可以优化： Artifact的下载都是通过http call，通过ioutil.ReadAll(resp.Body)copy到bytes.Buffer中。可以考虑将artifact下载到os.File中，借助磁盘节省内存。 内存里维护memory FS存放各类artifacts，可以考虑通过物理文件系统来维护相应的结构。 FS的打包是通过上述ArchiveFS helper将FS以tar的形式写到bytes.Buffer中，可以考虑直接写到os.File中。 Artifact的上传是通过http.Post形式发送大文件，可以改造成流式传输。 FS的优化比较简单，可以直接通过tempDir来构造物理文件系统替换掉memory FS，下面着重讨论相应的IO优化。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:2:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"IO优化 选用bytes.Buffer的好处是其实现了io.Reader与io.Writer方法，可以灵活地读写。此外，其可以很方便地获取到完整的字节流，便于额外构造一个新的reader，这对于我们既要将reader写入文件，也要通过reader算md5的场景提供了便利。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:3:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"使用os.File来替代bytes.Buffer file, err := os.Create(tempPath) defer file.Close() resp, err := http.Get(url) // ignoring handle err defer resp.Body.Close() // file作为writer，放置下载后的artifact io.Copy(file, resp.Body) // file作为reader，计算MD5 hash := md5.New() io.Copy(hash, file) // ... other logic 上述会将artifact下载到tempPath下，但是计算出的md5值总为空内容的md5。 究其原因，os.File写完后不能直接读，其在write后会将offset置为写入末尾处，需要手动将offset seek到文件开头才能读取完整的文件。举个例子： a := \"text.txt\" file, _ := os.Create(a) file.WriteString(\"test\") // file.Seek(0, io.SeekStart) buf := new(bytes.Buffer) buf.ReadFrom(file) fmt.Println(buf.String()) 输出结果为\"\"，去掉第5行注释的话，输出的结果便是test。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:3:1","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"使用io.TeeReader实现双写 我们的案例中存在两种双写的场景： 将下载的内容写入到文件，同时计算md5 由于某种artifact存储方式为content-addressable，下载到文件的同时需要写入本地文件系统某个缓存目录（类比docker） 使用bytes.Buffer可以方便地获取到内容，从而构建另一个reader，完成独立的两次写入。除此之外，io package为我们提供了专门的双写方法：io.TeeReader，其实现十分简单： func TeeReader(r Reader, w Writer) Reader { return \u0026teeReader{r, w} } func (t *teeReader) Read(p []byte) (n int, err error) { n, err = t.r.Read(p) if n \u003e 0 { if n, err := t.w.Write(p[:n]); err != nil { return n, err } } return } teeReader内部wrap了一个writer，在进行read时，同时会将读出来的内容write到内部的writer中。在我们的场景下，以计算md5并写入文件为例： // reader comes from http.Body hash := md5.New() reader := io.TeeReader(reader, hash) if _, err := io.Copy(writer, reader); err != nil { return err } hashsum := fmt.Sprintf(\"%x\", hash.Sum(nil)) // handle hashsum ","date":"2022-04-20","objectID":"/posts/io-optimazition/:3:2","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"使用io.Pipe实现读写管道 io.Pipe是native go的一种实现，类似于生产者消费者模式，主要靠channel实现。在使用中，读写需要是异步的，否则会出现死锁的情况。在我们的案例中，加载全量包时会以http post的形式发送大文件。通常我们会在内存中构建body： // bytes.Buffer构造post body buf := new(bytes.Buffer) writer := multipart.NewWriter(buf) defer writer.Close() part, err := writer.CreateFormFile(\"upload\", fn) if err != nil { return err } if _, err = io.Copy(part, file); err != nil { return err } http.Post(url, writer.FormDataContentType(), buf) 当file的size很大时（我们场景会是GB量级），body占用的内存会增大，增加了OOM的风险，因此我们采用io.Pipe进行内存优化： r, w := io.Pipe() m := multipart.NewWriter(w) // 异步写入，否则会出现死锁 go func() { defer w.Close() defer m.Close() part, err := m.CreateFormFile(\"upload\", fn) if err != nil { return } if _, err = io.Copy(part, file); err != nil { return } }() http.Post(url, m.FormDataContentType(), r) 观察io.Pipe的核心实现，主要依赖于一个数据channel流式地进行数据传递： func Pipe() (*PipeReader, *PipeWriter) { p := \u0026pipe{ wrCh: make(chan []byte), rdCh: make(chan int), done: make(chan struct{}), } return \u0026PipeReader{p}, \u0026PipeWriter{p} } func (p *pipe) write(b []byte) (n int, err error) { // ignore some control logic for once := true; once || len(b) \u003e 0; once = false { select { // wrCh是数据channel case p.wrCh \u003c- b: // rdCh是读写计数channel nw := \u003c-p.rdCh b = b[nw:] n += nw case \u003c-p.done: return n, p.writeCloseError() } } return n, nil } func (p *pipe) read(b []byte) (n int, err error) { // ignore some control logic select { case bw := \u003c-p.wrCh: nr := copy(b, bw) p.rdCh \u003c- nr return nr, nil case \u003c-p.done: return 0, p.readCloseError() } } ","date":"2022-04-20","objectID":"/posts/io-optimazition/:3:3","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"一坑多踩 除了OOM的问题，在开发过程中经常出现unexpected EOF的报错。此类问题排查的切入点比较模糊，经过深入地单步调试才得以定位：项目中使用了tar.Writer进行封包，在使用tar.Reader解包时报EOF错误。这意味着这个tar包不是一个合法的tar包，使用tar命令去操作此tar包也会有类似的报错。 究其原因，tar会在封包结束后，通过Close()方法在包尾部加入一层padding： // Flush is called by tw.Close() func (tw *Writer) Flush() error { if tw.err != nil { return tw.err } if nb := tw.curr.logicalRemaining(); nb \u003e 0 { return fmt.Errorf(\"archive/tar: missed writing %d bytes\", nb) } if _, tw.err = tw.w.Write(zeroBlock[:tw.pad]); tw.err != nil { return tw.err } tw.pad = 0 return nil } 我们在封包时，没有调用Close()，也就不会再尾部添加padding。在解包时，如果没有在尾部发现这个padding，就无法定位此tar包结束的位置，因此会认为此tar包是invalid，因此抛出unexpected EOF 那么，为什么会多此踩到这个坑呢？ 除了使用了tar进行封包，我们还是用了zstd.Writer对artifact进行了压缩，同样忘记了调用Close()方法。在此总结出一条经验教训：对于ReadCloser以及WriteCloser的实现，在读/写完一定要记得调用Close()方法，否则对于前者读而言，存在内存泄露的风险；对于后者写而言，存在因缺失尾部标识而导致包格式错误。 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:4:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["探索与实战"],"content":"经验总结 本文描述了与io相关的OOM问题及unexpected EOF问题，总结了如下经验： 当待处理的内容足够大时，避免使用bytes.Buffer加载到内存中进行处理，借助磁盘或管道减少内存占用 os.File写入后直接读，需要将offset重置到起始处 使用http.Post传大文件时，可以使用io.Pipe构建body，减少内存占用 对于所有的WriteCloser的实现，写完记得调用Close()以防写出的格式不完整 配图引自 https://medium.com/learning-the-go-programming-language/streaming-io-in-go-d93507931185 ","date":"2022-04-20","objectID":"/posts/io-optimazition/:5:0","tags":["go"],"title":"Golang应用中的一些IO优化","uri":"/posts/io-optimazition/"},{"categories":["整理与总结"],"content":"Modules are how Go manages dependencies. ","date":"2022-03-06","objectID":"/posts/go-modules/:0:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"术语介绍 包（package）：相同目录下，一起编译的源文件集合，每个包会归属于某个模块 模块（module）：一组统一release并publish的包的集合 主模块（main module）：go命令被调用时所在的模块，主模块在当前目录下或者在其父目录下存在go.mod文件 依赖（dependency） 直接依赖（direct dependency）：主模块所显式import的包或者模块 间接依赖（indirect dependency）：主模块间接引入的包或者模块，未被主模块显式import。在go.mod中需要加上//indirect suffix 模块图（module graph）：模块依赖构成的以主模块为root的有向图，每条边对应go.mod中一条require命令 主版本后缀（major version suffix）：一个major version子目录用于定义一个模块的新版本，一般当模块有较大的breaking change时引入 模块代理（module proxy）：实现GOPROXY标准的webserver，go命令从proxy下载模块相关信息 模块缓存（module cache）：一个存放下载模块的本地目录，直接serve此目录可以构建一个module proxy 最小版本选择算法（mvs）：构建时用以决定所有模块版本的算法，基准为选择满足约束的最小版本 标准版本（canonical version）：由字母v加semver组成，不带meta的（+incompatible除外）版本，只有标准版本可以用于mvs ","date":"2022-03-06","objectID":"/posts/go-modules/:1:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Model Path 模块路径会在go.mod中声明，用于描述如何定位到对应的模块 模块路径是其包含的所有包路径的前缀 模块可能包含如下三部分： repo root path，即模块所在的vcs仓库 repo root path下的一个子目录，当repo root path无法定位到模块时，可能引入了一个子目录后缀 一个major version suffix，如golang.org/x/somemodule/v2 ","date":"2022-03-06","objectID":"/posts/go-modules/:2:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Model Version 遵循semver2.0规范 打破后向兼容的change需要递增major version，并将minor/patch设置为0 未打破后向兼容的change需要递增minor version，并将patch设置为0 Bugfix或者小优化可以递增patch version Pre-release会在patch后，使用-连接的部分信息。pre-release会比对应release版本号小。当存在release时，latest version不会匹配pre-release，即使此pre-release更晚发布 Metadata suffix指patch后，用+连接的部分，不用做version排序，只起到标识作用 +incompatible是用于表明兼容性的特殊metadata，标识所引的模块在迁移到module之前就已经release了major\u003e1的版本，这个metadata表明会从对应版本tag的非major version suffix目录中去找模块 模块一定要在repo root directory中，即module path与repo root directory一致 不应有go.mod文件 伪版本(pseudo-versions) 是一种特殊的pre-release形式 伪版本一般用于如下场景 无semver tag的场景，此场景版本base一般为v0.0.0 用于标识发布release tag之前的测试版本 指定branch/commit map对应的revision上无semver tag，此场景下版本base为最近的semver tag 伪版本常见形式为vX.Y.Z-yyyymmddhhmmss-abcdefabcdef，一般编码如下信息： 下一个待发布的版本vX.Y.Z 生成此伪版本的时间戳 模块仓库的revision（如git系统下的commit号） ","date":"2022-03-06","objectID":"/posts/go-modules/:3:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"MVS 操作实体是一个模块图 节点是模块名 边描述对模块所依赖的最小required version，由go.mod中require命令指定，会被replace/exclude等命令影响 mvs从主模块出发遍历图，跟踪每个模块被依赖的最大required version，这些模块及相应的版本构成了build list MVS版本选择算法 ","date":"2022-03-06","objectID":"/posts/go-modules/:4:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Workspace 功能：在运行mvs时，将磁盘上的模块添加到主模块中 由go.work声明，可以通过-workfile=off禁用；也可通过-workfile=*.work指定路径，否则会沿着当前目录及其祖先目录寻找go.work，核心命令如下： use：声明需要加入主模块的本地模块路径 replace：进行指定模块的替换，适用于workspace内的所有模块 ","date":"2022-03-06","objectID":"/posts/go-modules/:5:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Module Proxy 实现了如下GET api的http server： Path Required Description $base/$module/@v/list T 返回$module的所有已知版本（不包含pseudo-versions） $base/$module/@v/$version.info T 返回包含canonical version相关信息，用于定位模块。$version与返回的canonical version并不需要一致，但大多数情况下会保持一致；Time字段可选 $base/$module/@v/$version.mod T 返回模块指定版本的go.mod文件，如果没有此文件，则生成一个只有module命令的go.mod $base/$module/@v/$version.zip T 返回模块指定版本的源码压缩包 $base/$module/@latest F 返回latest version的info信息 为了解决uri大小写不敏感的问题，使用!m替换M 环境变量GOPROXY用于声明一组proxy url用于定位模块，其默认值为https://proxy.golang.org,direct，其中direct表明直接从源码仓库VCS去找模块 ","date":"2022-03-06","objectID":"/posts/go-modules/:6:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"定位包所在的模块 根据GOPROXY遍历proxy list，对package path进行最长前缀匹配（匹配module path+subdirectory），如果匹配到了module并且包含对应的package，则记录对应的模块 如果匹配过程中报了权限等非404/410相关的错误则抛出错误（GOPROXY以|分隔proxy list则会继续遍历） 如果此proxy所有的可能模块路径request均返回404/410，继续遍历proxy list并执行第一步 如果proxy list遍历完也未找到对应的模块，直接报错 例：GOPROXY=https://corp.example.com,https://proxy.golang.org，搜索golang.org/x/net/html所在模块： To https://corp.example.com/ Request for latest version of golang.org/x/net/html Request for latest version of golang.org/x/net Request for latest version of golang.org/x Request for latest version of golang.org To https://proxy.golang.org/ Request for latest version of golang.org/x/net/html Request for latest version of golang.org/x/net Request for latest version of golang.org/x Request for latest version of golang.org ","date":"2022-03-06","objectID":"/posts/go-modules/:6:1","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Module Cache 存放已下载的模块文件，和build cache没有关系 cache在本机所有project中共享，接入是并发安全的 cache的模块是只读的 默认path为$GOPATH/pkg/mod，可以通过 go clean -modcache将cache purge掉 cache/downloadpath下的布局是符合Module Proxy标准的，因此可以直接serve cache/download作为module proxy 注意 module cache并不十分严格符合GOPROXY协议，比如模块cache了pseudo-version的话，@v/list会返回pseudo-version。 ","date":"2022-03-06","objectID":"/posts/go-modules/:7:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"go get 底层API调用流程 Serve本地module cache作为proxy验证go get的api调用。 在域名为 web.raygecao.cn上下载6.824-test模块，并serve cache/download目录作为proxy： # web.raygecao.cn server $ go get github.com/raygecao/6.824-test go: finding github.com/raygecao/6.824-test v0.0.1 go: downloading github.com/raygecao/6.824-test v0.0.1 go: extracting github.com/raygecao/6.824-test v0.0.1 $ cd $GOPATH/pkg/mod/cache/download/ \u0026\u0026 sudo python3 -m http.server 80 Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... 在另一台机器上将web.raygecao.cn设置为GOPROXY，并获取此模块： # client $ go mod init test $ GOPROXY=http://web.raygecao.cn GOSUMDB=off go get github.com/raygecao/6.824-test 因为客户端没有此模块的cache，因此在server端观察到的api调用流程为： 模块未被cache时，server端观测的api调用 此时模块已下载并cache住，再次go get此模块时，api调用流程为： 模块被cache后，server端观测的api调用 总结完整的api调用流程如下： module path拆分，并发调用@v/listapi以及最长前缀匹配去定位模块，调用@v/list api获取所有release/pre-release，选择latest version 【cached】调用@v/infoapi获取canonical version等信息 【cached】调用modapi获取依赖从而构建build list 【cached】调用zipapi获取模块源码，从而加载相应的package @latest api用途 由于@v/list只会获取release/pre-release，当模块repo里没有release/pre-release tag时，@v/list返回结果为空，此时会尝试调用@latestendpoint搜索latest pseudo-version，依然找不见则会自动生成。 ","date":"2022-03-06","objectID":"/posts/go-modules/:7:1","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Module Authentication 验证的主体包括zip文件和mod文件，可以使得不可信的proxy提供的包变得可信，此外cache提供的模块也需要经过验证 对于zip文件，计算hash是顺序无关的，并且不受其他metadata、alignment等影响 验证时会优先从go.sum中找，如果go.sum不存在，会向checksum database query，验证通过后记录到go.sum中 checksum校验失败的一个常见的原因是人为更改repo tag ","date":"2022-03-06","objectID":"/posts/go-modules/:8:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"相关环境变量 环境变量 默认值 类型 作用 GOMODCACHE $GOPATH/pkg/mod filepath 指定下载的模块及相关文件存放的目录 GOINSECURE - 逗号分隔的glob patterns 用于匹配模块前缀，当直接从VCS中fetch匹配的模块时，可以允许其使用insecure manner（https=\u003ehttp, git+ssh:// =\u003e git://) GONOPROXY $GOPRIVATE 逗号分隔的glob patterns 用于匹配模块前缀，匹配的模块跳过query proxy，直接从VCS中获取 GONOSUMDB $GOPRIVATE 逗号分隔的glob patterns 用于匹配模块前缀，匹配的模块跳过校验流程 GOPRIVATE - 逗号分隔的glob patterns GONOPROXY与GONOSUMDB的默认值 GOPROXY https://proxy.golang.org,direct url list 声明proxy列表，当前一个proxy返回404/410状态码时，follow next（针对逗号分隔的场景） GOSUMDB sum.golang.org url 指定checksum database，当go.sum不存在且未跳过校验流程时，从此url中获取hash用于校验 ","date":"2022-03-06","objectID":"/posts/go-modules/:9:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"Tips 主模块要求有go.mod文件，但是没有go.mod文件的模块可以用作依赖，在寻找此模块时，如果模块路径与repo root path一致时，go command会生成一个只有module命令的go.mod，以保证依赖方每次构建的确定性 Go1.17相对于之前的版本在go.mod中显式地列出了所有主模块间接导入的包，而之前的版本对于间接依赖，只有在默认mvs选择的版本与被依赖的版本不一致时才会显式列出。这些额外的间接依赖的信息用于模块图剪枝及模块延迟加载 retract用法（始于go 1.16） 声明本模块的一些版本不应被依赖，一般用于误发版本/版本存在fatal bug时 被撤销的版本本身会存在，避免破坏已依赖此版本的模块构建 用法示例：当前最新版本为v0.9.1，又发布了v1.0.0且有fatal bug，则需要发布v1.0.1并进行如下声明： retract ( v1.0.0 // Published accidentally. v1.0.1 // Contains retractions only. ) 一般来讲module path跟repo root path应是一致的（不考虑major version suffix），但有些情况下模块会定义在repo root path下的子目录中，一般用于monorepo中多个组件需要独立发版的case，这其中每个组件都应有个go.mod文件 ","date":"2022-03-06","objectID":"/posts/go-modules/:10:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"References https://go.dev/ref/mod https://go.dev/blog/using-go-modules https://github.com/golang/go/blob/go1.17.7/src/cmd/go/internal/modfetch/proxy.go https://github.com/golang/go/issues/32715 https://github.com/golang/go/issues/51391 ","date":"2022-03-06","objectID":"/posts/go-modules/:11:0","tags":["go"],"title":"Go modules要点梳理","uri":"/posts/go-modules/"},{"categories":["整理与总结"],"content":"grep, sed, awk被称为linux文本处理三剑客，分别侧重于文本搜索，流式编辑和格式化文本。 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:0:0","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"grep 文本搜索工具 Schema：grep [option...] [patterns] [file...] ","date":"2022-03-05","objectID":"/posts/text-process-tool/:1:0","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Options 短选项 长选项 作用 -e pattern --regexp=pattern 指定搜索pattern，可以不显式指定 -f file --file=file 从file中获取pattern进行搜索 -i/-y ignore-case 匹配忽略大小写 -v --invert-match 匹配反选 -w --word-regexp 指定单词完整匹配，其边界为行首或者除字母、数字、下划线以外的字符 -x --line-regexp 行完整匹配 -c --count 不打印匹配行，只打印匹配行数 -L -l --files-without-match --files-with-matches 不打印匹配行，只打印完全不匹配/存在匹配的文件名 -m num --max-count=num 指定最大匹配次数 -o --only-matching 只打印完整的匹配内容，不打印匹配行 -q --quiet/silent 不打印任何内容，只以返回码来描述是否存在匹配 -h -H --no-filename --with-filename 不打印/打印匹配的文件名前缀，默认为打印(-H) -n --line-number 打印匹配的行号前缀 -A num --after-context=num 打印匹配行，及其以后的num行 -B num --before-context=num 打印匹配行，及其以前的num行 -C num -context=num/-num 打印匹配行，及其前后的各num行 -R --dereference-recursive 递归的匹配目录下所有文件，遇到符号链接正常递归 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:1:1","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Tips grep不支持换行符的匹配 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:1:2","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"sed 流编辑器，用于逐行地对输入流进行基本的文本转换 schema：sed SCRIPT INPUTFILE INPUTFILE未指定或者指定为-时，表示从stdin读取内容 默认输出为stdout，可以通过w cmd/重定向符保存到文件，可以指定-i在文件内原地修改 多个cmd可以通过;或换行来指定，或者-e指定。（a,c,i命令中不应使用;，会将其当做plain text） data buffer 模式空间（Pattern space）：数据处理空间，sed读取每行到模式空间，进行地址匹配并执行命令 保持空间（Hold space）：辅助暂存空间，在复杂处理过程中，作为数据的暂存区域 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:2:0","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Options 短选项 长选项 作用 -n --quiet/--silent 禁掉打印模式空间的内容，一般结合p命令打印处理过的行 -f --file 指定sed脚本文件路径，命令形式指定则对应选项-e，一般无需显式指定-e -i --in-place=[suffix] 原地修改文件，如指定suffix会拼接到原文件名作为原文件内容的backup -E/-r --regexp-entended 使用扩展正则表达式 -s --seperate 指定操作多个独立的文件 --posix 严格限定posix，禁用GNU扩展，便于简化可移植脚本的编写 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:2:1","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Commands 基本形式：[addr]X[options]，其中addr为地址匹配模式，X为单字符命令，options指一些命令需要的额外选项。 s命令：替换字符串 s/regexp/replacement/flags 根据regexp去模式空间找匹配，匹配成功后用replacement去替换掉regexp 特殊符号需插入\\转义 常用flag： g：所有匹配均替换 number：只替换第number个匹配 p：如果发生匹配，打印替换后的模式空间 i/I：匹配忽略字母大小写 m/M：支持多行匹配 其他常见命令 cmd 作用 d 删除模式空间内容，并开始下一轮匹配 p 匹配成功时，打印模式空间，一般配合-n选项使用 n 读取下一行替换当前模式空间的行，执行下一条处理命令而非第一条命令。一般用于周期性替换，如偶数行替换 seq 6 | sed 'n;s/./x/'。N命令在此基础上支持换行处理 {} 封装一组命令用以执行 a text 在一行后添加text i text 在一行前插入text c text 使用text替换行 y/source-chars/dest-chars 执行字符的替换，如使用0-9替换a-j：echo hello | sed 'y/abcdefghij/0123456789/' r filename 读取文件 w filename 将模式空间内容写入到文件 b label 类似于if-else的分支命令，goto label，如跳过第一行的替换：printf '%s\\n' a1 a2 a3 | sed -E '/1/bx ; s/a/z/ ; :x' 与保持空间相关的命令 cmd 作用 g 使用保持空间的内容替换模式空间的内容 G 添加一个新行，并将保持空间的内容追加到模式空间 h 使用模式空间的内容替换保持空间的内容 H 添加一个新行，并将模式空间的内容追加到保持空间 x 交换保持空间和模式空间的内容 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:2:2","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"地址匹配 行号匹配 number：第几行 $: 最后一行 first~step：从first开始，每隔step匹配 正则匹配 范围匹配 addr1,+N：匹配[addr1, addr1+N]行 addr1,~N：匹配[addr1, addr1+addr1%N]行 # examples sed '4,17s/hello/world/' input.txt # 第4-17行将所有hello替换成world sed '/apple/s/hello/world/' input.txt # 所有包含apple的行将hello替换为world sed '2!s/hello/world/' input.txt \u003e output.txt # 除第二行以外将所有hello替换为world ","date":"2022-03-05","objectID":"/posts/text-process-tool/:2:3","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Tips -i可以指定可选参数，所以其后不应有其他的短选项 sed -Ei ... =\u003e sed -E -i ... sed -iE ... =\u003e sed --in-place=E ... label用作循环处理文本，一般会配合n/N使用，如多行合并的示例：seq 6 | sed ':x; N;s/\\n//; bx;' D,G,H,N,P支持多行处理，每个命令的作用与其小写命令相同 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:2:4","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"awk 文本格式化工具，多用于格式化文本，生成报表。 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:3:0","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"基本用法 schema awk [options] 'Pattern{Action}' file 普通格式化 # $0表示完整行，$1表示第一列，$NF表示最后一列，$(NF-1)表示倒数第二列 df | awk '{print $1, $2}' 设置分隔符 # 输入字段分隔符，可以使用-F: 或者-v FS=:的形式设置 cat /etc/passwd | awk -F: '{print $1} # 输出字段分隔符，可以使用-v OFS=@的形式设置 cat /etc/passwd | awk -v FS=: -v OFS=@ '{print $1, $3}' 内置变量 变量名 作用 默认值 FS 输入字段分隔符 空格 OFS 输出字段分隔符 空格 RS 输入换行符（记录分隔符） \\n ORS 输出换行符 \\n NF 字段数目 NR 行号 FNR 文件序号 FILENAME 当前文件名 ARGC 命令行参数的个数 ARGV 命令行参数组成的数组 # 打印行号，变量在action里不应加$，加$表示获取对应列的内容 df | awk '{print NR, $2}' 自定义变量 # 使用-v设置 awk -v a=\"b\" 'BEGIN {print a}' # 在action中设置，并用分号分隔 awk 'BEGIN {a=\"b\"; print a}' ","date":"2022-03-05","objectID":"/posts/text-process-tool/:3:1","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"特殊模式 BEGIN表示在处理文本前执行action，可用于打印表头 END表示在处理文本后执行action，可用于打印表尾 df | awk 'BEGIN{print \"开始时执行\"} {print $1} END{print \"结束时执行\"}' 条件匹配 # 打印第2行，第一列的内容 df | awk 'NR == 2 {print $1}' # 如果第3行的值大于0，打印完整行 df | awk '$3\u003e0 {print}' 正则相关 # 打印/dev开头的行 # 正则模式采用awk '/regexp1/{action}'的形式 df | awk '/^\\/dev/{print}' # 打印最后一列以/dev开头的行 # 正则匹配采用awk 'var~/regexp1/{action}'的形式 df | awk '$NF~/^\\/dev/{print}' # 打印最后一列从/d开头到/p开头之间的行 # 范围模式采用 awk '/regexp1/, /regexp2/ {action}'形式 df | awk '$NF~/^\\/d/,$NF~/^\\/p/{print}' 分支动作 # 判断分支 df | awk '{ if($2\u003e0){print \"大于0\"}else{print \"小于0\"} }' # 循环分支 df | awk '{ for(i=1; i\u003c3; i++){print $i}}' df | awk '{ i=0; while(i\u003c4){print $0; i++}}' # 将每行打印4遍 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:3:2","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Tips {}意味着代码块，可以通过;将语句划归到同一代码块中 print动作如果使用,分隔两列，则输出会以输出字段分隔符将两列连接起来，如果以空格分隔，则输出会将两列直接拼接打印 awk支持三元运算符 ","date":"2022-03-05","objectID":"/posts/text-process-tool/:3:3","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"参考文档 https://www.gnu.org/software/grep/manual/grep.html https://www.gnu.org/software/sed/manual/sed.html https://www.gnu.org/software/gawk/manual/gawk.html ","date":"2022-03-05","objectID":"/posts/text-process-tool/:4:0","tags":["linux"],"title":"Linux文本处理工具","uri":"/posts/text-process-tool/"},{"categories":["整理与总结"],"content":"Iptables is the userspace command line program used to configure the Linux 2.4.x and later packet filtering ruleset. It is targeted towards system administrators. ","date":"2021-12-08","objectID":"/posts/iptables/:0:0","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"简介 Netfilter/Iptables是unix/linux自带的防火墙工具，netfilter是内核中数据包处理模块，定义了一组hook点，埋在网络协议栈中以对不同阶段的包进行处理，而Iptables是操纵netfilter的命令行工具。netfilter工作在二、三、四层，其匹配规则涵盖网卡，IP，端口等。 netfilter的作用 NAT（网络地址转换） 数据包内容修改 数据包过滤 ","date":"2021-12-08","objectID":"/posts/iptables/:1:0","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"Iptables 四表五链 Iptables四张表 filter表：负责过滤数据包； nat表：网络地址转换，如SNAT、DNAT； mangle表：修改报文，解包-\u003e修改-\u003e封包； raw表：关闭nat及数据包链接追踪机制； Iptables五个默认链 PREROUTING：刚进入网络层的数据包会先经由PREROUTING链，并根据路由判断目标地址是否为本机。由于此链为接收包的第一站，因此DNAT通常在此链上配置； INPUT：经由PREROUTING链的数据包发现目标地址为本机，会经由INPUT链进入本机，入站过滤规则通常在此链配置； FORWARD：经由PREROUTING链的数据报发现目标地址不是本机，会经由FORWARD链进行转发，通常网络防火墙会在此链配置； OUTPUT：从本机产生的数据包向外发送时会先经过OUTPUT链，通常出站过滤规则在此链配置。 POSTROUTING：经由OUTPUT/FORWARD链中流出的数据包最终经路由后会到达POSTROUTING链，以从对应的接口流出，POSTROUTING为数据包流出的最后一站，SNAT通常在此链上配置； 每条链上会有多个规则，按顺序匹配，符合规则的触发action，并停止后续的规则匹配。 iptables的默认链及拥有的功能表，引自从零开始认识 iptables 数据包由外部发往本主机进程流向为：PREROUTING -\u003e INPUT 数据包由本主机发往外部进程流向为：OUTPUT -\u003e POSTROUTING 数据包由本主机进行转发的流向为：PREROUTING -\u003e FORWARD -\u003e POSTROUTING 表与链的区别：表主要是按功能进行分类；而链是协议栈中某处提供的钩子函数。 默认策略：每条默认链都会有一个默认策略，在没有规则被匹配时会执行默认策略中的动作，而与之相关的是防火墙的两种通行策略： 黑名单机制：只有匹配到规则的数据包才可能被拒绝，没有匹配到的数据包都会被接受。将默认策略设置为ACCEPT即可实现黑名单机制； 白名单机制：只有匹配到规则的数据包才可能被接收，没有匹配到的数据包都会被拒绝。将默认策略设置为DROP即可实现白名单机制； 注意 黑白名单机制并非只能靠调整默认策略实现，设置默认策略只是实现此机制的一种方案，也可以通过其他方案来实现黑白名单机制，比如将默认策略设置ACCEPT，在链的尾部加一个DROP ALL的规则可以实现白名单机制。通常不建议将默认策略更改为DROP/REJECT，因为在此默认策略下，误flush了链会导致所有请求被拒绝。 同一链上各表执行优先级：raw \u003e mangle \u003e nat \u003e filter ","date":"2021-12-08","objectID":"/posts/iptables/:1:1","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"匹配条件 常见匹配条件 类型 iptables输出 备注 源IP source IP报头中记录，可以使单个IP地址或网段，-s指定 目的IP destination IP报头中记录，可以使单个IP地址或网段， -d指定 流入网卡 in -i指定 流出网卡 out -o指定 协议类型 prot -p指定，支持tcp/udp/icmp等 扩展匹配条件 类型 表示 备注 源端口 tcp spt:22 导入协议模块，同时指定--sport指定目的端口 目的端口 tcp dpt:22 导入协议模块，同时指定--dport 指定目的端口 字符串 STRING 导入string模块，同时指定--string指定匹配文本内容 连接数 connlimit 需导入connlimit模块，并设置连接数规则 报文速率 limit 需导入limit模块，并设置速率限制规则 状态 state 需导入state模块，通过--state对状态进行设置，通常用来判断报文是对方主动发送的还是对方回复的响应报文 使用!可以对匹配条件进行取反，扩展匹配条件通常需要通过-m导入特定的模块才可以使用。 ","date":"2021-12-08","objectID":"/posts/iptables/:1:2","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"常用的处理动作 动作 含义 ACCEPT 接受此数据包 DROP 直接丢弃此数据包 REJECT 拒绝数据包，并返回相应的通知 SNAT 源端网络地址转换，即将数据包的源IP改写为指定IP MASQUERADE 动态的SNAT，无需手动指定源IP，iptables会将数据包的源IP改写成out网卡的IP，适用于动态生成IP的场景 DNAT 目的端网络地址转换，即将数据包的目的地址改写为指定IP REDIRECT 本机端口映射，即将数据包导向另一个端口 LOG 将数据包记录在内核日志中，然后继续后续规则匹配 RETURN 结束在当前链中的规则匹配，返回到父链继续匹配 ","date":"2021-12-08","objectID":"/posts/iptables/:1:3","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"CheatSheet 选项 作用 备注 -L 获取规则列表 可接链名，不接默认为所有链，一般使用-nvL --line列举 -I/-A 在特定行/尾行插入 -I可以指定行号，默认为首行 -t 指定表名 默认为filter表 -D 删除某条规则 可以指定行号，也可以指定匹配条件+动作 -R 更新规则 须指定匹配条件和动作，建议使用删除+添加的形式 -F 清空某条链全部规则 慎用 -P 修改某条链上的默认规则 初始为ACCEPT，不建议设置为DROP/REJECT -N 创建一条自定义链 规则模块化，便于管理 ","date":"2021-12-08","objectID":"/posts/iptables/:1:4","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"增删改查基本指令示例 $ iptables -t filter -nvL INPUT --line # 查询filter表，INPUT链的所有规则并展示行号 $ iptables -t filter -I INPUT 2 -s 10.1.0.1 -j DROP # 在filter表，INPUT链第二行添加拒绝源IP为10.1.0.1的DROP规则 $ iptables -t filter -D INPUT 2 # 在filter表，INPUT链删除第二条规则 $ iptables -D INPUT 2 \u0026\u0026 iptable -I INPUT 2 -s 10.1.0.2 -j REJECT # 将filter表，INPUT链第二条规则修改为拒绝10.122.0.2 $ iptables -t filter -P FORWARD DROP # 将filter表的FORWARD链默认动作设置为DROP(默认禁止转发) ","date":"2021-12-08","objectID":"/posts/iptables/:1:5","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"自定义链 上述描述的五条链是Iptables的默认链，除此之外，iptables还支持自定义链，自定义链与默认链的操作方式相同，但不是netfilter标准hook点，因此自定义链需要附加在某个默认链上。默认链在匹配到自定义链时会将其展开进行规则匹配，类似于内联函数，我们可以类比自定义函数来看一下自定义链的优势。 函数一般需要满足单一性原则，自定义链也是将满足同一功能/归属统一业务的规则集中在一条链上管理，这样可以有效避免默认链上规则无限制的增长，便于管理与维护。 函数是可复用的，自定义链也是可以复用的，一些通用的规则可以放在一条自定义链上被多个默认链/其他自定义链引用，对复用链的更新会传播到引用链中，对同一条规则的修改无需手动操作多条链。 ","date":"2021-12-08","objectID":"/posts/iptables/:1:6","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"Docker iptables分析 为了将docker中iptables的作用展示的更透彻，示例环境启动了两个容器，详情如下： # docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f36770eceff5 ibmcom/guestbook:v2 \"./guestbook\" 3 weeks ago Up 3 weeks 0.0.0.0:19002-\u003e3000/tcp jolly_noether d8e30286aa69 ibmcom/guestbook:v1 \"./guestbook\" 3 weeks ago Up 3 weeks 0.0.0.0:19001-\u003e3000/tcp eager_dubinsky 均在默认的bridge网络下，两个容器的3000端口分别publish到19002与19001端口。 ","date":"2021-12-08","objectID":"/posts/iptables/:2:0","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"filter表 Iptables的INPUT链，OUTPUT链均为空，FORWARD设置了默认策略是DROP的白名单模式，这里着重看一下FORWARD链： filter表中forward链 Rule1引用了一个自定义链DOCKER-USER，此链只有一个RETURN动作，如果需要在此docker环境下手动添加iptables rules可以选择在此处添加。 Rule2引用了DOCKER-ISOLATION-STAGE-1链，相继引用了DOCKER-ISOLATION-STAGE-2链，这两条链也都没有实质性规则，所有的转发数据包会继续向下匹配。 Rule3表示接受从docker0流出的所有响应报文。 Rule4表示从docker0流出的数据包需要走DOCKER链，DOCKER链我们稍后分析。 Rule5表示接受从docker0流入，但不是从docker0流出的数据包，容器访问宿主机/外网等非容器网络时匹配此规则。 Rule6表示接受从docker0流入，同时从docker0流出的数据报，在默认bridge网络上的容器间通信匹配此规则。 再来看一下DOCKER链，容器相关的规则都放在DOCKER链上，便于管理及查询。 filter表中自定义DOCKER链 Rule1表示接受不是从docker0流入，但从docker0流出，并且访问的目的IP为172.17.0.2， 访问目的port为3000的tcp报文。 Rule2表示接受不是从docker0流入，但从docker0流出，并且访问的目的IP为172.17.0.3， 访问目的port为3000的tcp报文。 上述规则都没有匹配的报文，那是因为无论我们从容器内还是宿主机访问容器，都是从docker0流入、从docker0流出。如果将某个一条路由的节点添加一条路由规则，将172.17.0.0/24的destination gateway设置为本机，这样对172.17.0.3:3000的访问会经物理网卡流入，经docker0流出到容器内，会匹配上此链上的规则。说白了，其意图是只要你能将请求路由到本机，我就允许你访问默认bridge网络中容器提供的服务。 ","date":"2021-12-08","objectID":"/posts/iptables/:2:1","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"nat表 下面来分析一下nat表，OUTPUT链和PREROUTING链会引用DOCKER自定义链，其他链上规则基本为空或者都不会匹配到。我们主要关注一下POSTROUTING链和DOKCER链，先看一下POSTROUTING链： nat表中PREROUTING链 Rule1表示不是从docker0流出的，且源网段为172.17.0.0/16的数据包会被进行动态SNAT，将数据包的源IP改为流出网卡的地址，若从容器内访问外网，源IP会被更新为宿主机物理网卡的IP地址。 Rule2，Rule3乍一看像是自己访问自己的3000端口时会采用MASQ。但从容器内路由规则可以看到，容器内访问容器网络是无需路由到docker0，而直接在二层网络上转发。而容器内通过容器IP访问自身服务是无需经由docker0的，因此第2、3条的作用不明确。 再看一下DOCKER链： nat表中自定义DOCKER链 Rule1表示从docker0流入的数据包直接跳过此链，因为容器网络间的访问是不需要NAT的。 Rule2, Rule3分别表示两个容器publish了3000端口到19001端口，3000端口到19002端口。那些不是从docker0流入的数据包想要访问容器内的服务时，访问对应网卡上的19001端口的tcp请求就转发到172.17.0.2:3000；而访问19002端口的tcp请求就转发到172.17.0.3:3000。 ","date":"2021-12-08","objectID":"/posts/iptables/:2:2","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"Tips 由于iptables是按顺序匹配的，因此应该将更容易匹配的规则放在前面，以提升防火墙的匹配性能。 当存在多个匹配条件时，各条件之间为与的关系，即所有匹配条件均满足时才会执行对应的action。 配置白名单时，一般不会将默认链策略设置为DROP，这是因为如果误刷了该默认链（-F）将导致所有数据包都会被拒绝，会导致我们无法连接到服务器。一般白名单机制会将默认链策略设置为ACCEPT，并在链的最后设置DROP ALL的规则。 为提升iptables线性匹配算法，一些ACL优化算法被提出，eBPF 技术实践：高性能 ACL 采用根据匹配规则倒排，并利用bitmap来快速匹配的算法，以达到近O(1)的匹配性能。 ","date":"2021-12-08","objectID":"/posts/iptables/:3:0","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["整理与总结"],"content":"参考文献 朱双印的博客iptables部分 从零开始认识 iptables eBPF 技术实践：高性能 ACL ","date":"2021-12-08","objectID":"/posts/iptables/:4:0","tags":["网络"],"title":"Iptables概要整理","uri":"/posts/iptables/"},{"categories":["探索与实战"],"content":"Docker 通过storage driver来存储镜像层，并且将数据存储到容器层。本文主要介绍storage driver与volume的区别，以及对经典的driver：aufs, overlay, overlay2进行简单的探索。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:0:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Storage Driver vs Docker Volume Storage Driver 用于存储image layer。 用于在容器层（writeable layer）存数据。 容器层在容器销毁后丢失，无法做到持久化，无法在多个容器中共享用户数据。 一般使用CoW（copy on write）机制写容器层，初次写数据时需要从镜像层将数据copy-up到容器层进行写操作。 容器可以复用镜像层，由于CoW机制，每创建一个容器仅多创建一个很薄的容器层，可以充分提升空间效率。 Docker Volume 持久化容器产生的数据，与容器生命周期无关。 数据可以在容器间共享。 写volume性能远比写容器层的性能好，volume适合于写密集的场景。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:1:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Aufs ","date":"2021-10-16","objectID":"/posts/storage-driver/:2:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"理论 判断内核是否支持aufs driver，结果输出如下表示支持： $ grep aufs /proc/filesystems nodev aufs Aufs是一个联合文件系统，其采用union mount将linux系统上的多个目录堆叠成一个目录，一个目录代表一个branch（在docker术语中对应为layer）。 aufs layer组织形式 存储结构 diff/：每一层的内容，每层以一个独立的子目录存储。 layers/：存放layers的元信息以标识镜像层如何堆叠，每层以一个文件表示。 mnt/：挂载点，每层一个，用于向容器组装/挂载联合文件系统。 容器读文件 仅在容器层存在：直接从容器层读出。 仅在镜像层存在：沿着layer stack寻找文件，找到后读出。 在容器层和镜像层均存在：从容器层读出，镜像层相应的文件被容器层所遮盖。 容器写文件 从容器层查找文件，如果存在则直接修改。 如果不存在，沿着镜像layer stack查找文件，如果文件存在则copy-up到容器层进行修改。 如果镜像层也不存在，则直接在容器层创建文件。 容器删除文件/目录 删除文件：在容器层创建一个whiteout file，避免向下层继续寻找。 删除目录：在容器层创建一个opaque file（实测依然是whiteout file）。 Aufs性能 对容器密集型场景友好，因其能有效利用运行中的容器image，使得容器启动更迅速，减少磁盘空间使用。 能有效使用page cache。 定位文件开销大，需要沿着layer stack逐层定位。 首次写操作开销大，尤其是文件在镜像层存在时，需要copy-up至容器层。由于aufs底层存储是文件级别而非块级别，对于文件的修改需要将整个文件复制到容器层。因此文件越大，写性能越差。 最佳实践 使用ssd盘，速度远高于旋转式磁盘。 对于write-heavy负载使用volume，减少IO开销的同时可以将容器数据持久化，并且可以在多个容器中共享。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:2:1","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"实践 使用ubuntu:16.04 image对aufs driver进行探究，首先启动一个container： $ docker run -it --rm ubuntu:16.04 bash Docker存储目录结构如下： $ tree -L 2 /var/lib/docker/aufs /var/lib/docker/aufs ├── diff │ ├── 1492027998d17f2f422a0d46ed6e41a9ef59911bb13357765aeb0dc9f150ea76 │ ├── 72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a │ ├── 8065b4da5339d297dfb53f4bd7edfeba97b5c0089eda635c412943c4d7e55e81 │ ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 │ ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227-init │ └── aff7006186a6867de9cc7a75f1d31c90eb52753b9d96d6c85a3f66e78ccd465b ├── layers │ ├── 1492027998d17f2f422a0d46ed6e41a9ef59911bb13357765aeb0dc9f150ea76 │ ├── 72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a │ ├── 8065b4da5339d297dfb53f4bd7edfeba97b5c0089eda635c412943c4d7e55e81 │ ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 │ ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227-init │ └── aff7006186a6867de9cc7a75f1d31c90eb52753b9d96d6c85a3f66e78ccd465b └── mnt ├── 1492027998d17f2f422a0d46ed6e41a9ef59911bb13357765aeb0dc9f150ea76 ├── 72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a ├── 8065b4da5339d297dfb53f4bd7edfeba97b5c0089eda635c412943c4d7e55e81 ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 ├── a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227-init └── aff7006186a6867de9cc7a75f1d31c90eb52753b9d96d6c85a3f66e78ccd465b 15 directories, 6 files 可以看到存在一个-init后缀目录项，因此可以断定a5cad43是容器层，其他均为镜像层。 从挂载信息中可以验证这一点，aufs仅需挂载upperdir： $ mount | grep aufs none on /var/lib/docker/aufs/mnt/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 type aufs (rw,relatime,si=b3be613c19d0e57c,dio,dirperm1) 可以看到此挂载source内容即为ubuntu rootfs的内容： $ ls /var/lib/docker/aufs/mnt/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var 此镜像中layer stack又是如何组织的呢？ $ cat /var/lib/docker/aufs/layers/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227 a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227-init aff7006186a6867de9cc7a75f1d31c90eb52753b9d96d6c85a3f66e78ccd465b 8065b4da5339d297dfb53f4bd7edfeba97b5c0089eda635c412943c4d7e55e81 1492027998d17f2f422a0d46ed6e41a9ef59911bb13357765aeb0dc9f150ea76 72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a 可以从容器层的layers内容中看到layer stack自顶向下的组织形式为 a5 -\u003e a5-init -\u003e af -\u003e 80 -\u003e 14 -\u003e 72。 接下来验证一下容器内更改文件系统对aufs存储有什么影响。 创建文件 # 容器内 $ touch kkk # 宿主机 $ find /var/lib/docker/aufs -name kkk /var/lib/docker/aufs/mnt/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227/kkk /var/lib/docker/aufs/diff/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227/kkk 可以看见文件的创建仅发生在容器层。同理，更改文件内容也是如此，只是涉及到从镜像层到容器层的copy-up。 删除文件 # 容器内 $ rm /bin/zegrep # 宿主机 $ find /var/lib/docker/aufs -name *zegrep* /var/lib/docker/aufs/mnt/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227/usr/share/man/man1/zegrep.1.gz /var/lib/docker/aufs/diff/a5cad43027e71dcd8efaa9287a4c502a59631c108d22b5e861d9ca1878525227/bin/.wh.zegrep /var/lib/docker/aufs/diff/72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a/bin/zegrep /var/lib/docker/aufs/diff/72c62493358ebb6ae7d47717a491b7f3ff402fb338a34d9df8529c166223737a/usr/share/man/man1/zegrep.1.gz 可以看到容器中/bin/zegrep位于最底层的72c6249镜像层，容器内删除此文件并不会在镜像层删除，而是在容器层a5cad43产生一个whiteout文件——/bin/.wh.zegrep，通过whiteout文件避免沿着layer stack向下查找。如果容器内误删了某个文件，可以在对应容器层diff目录中将whiteout文件删除以将其恢复。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:2:2","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"OverlayFS OverlayFS是一种类似aufs的联合文件系统，但是速度更快，更简单。Docker基于overlayFS提供了两种存储驱动：overlay和overlay2 将两个目录以union mount的形式组织成单个目录，镜像层目录为lowerdir，容器层目录为upperdir，对外呈现一致的目录视图（容器内看到的文件系统）叫做merged。 overlayFS layer组织形式 ","date":"2021-10-16","objectID":"/posts/storage-driver/:3:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Overlay2 Overlay2 driver每一层均以一个patch的形式表示，基于内核overlayFS的multiple lower layers特性实现，不再需要硬链接，lowerdir是将镜像层所有的layer overlay起来组成。 容器层挂载信息： # mount | grep overlay overlay on /var/lib/docker/overlay2/23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/XRV25KXVQWJ42GBK5UJD7ICFVV:/var/lib/docker/overlay2/l/ZVAQQEPIP4VXNQK2LZM24JXP7H:/var/lib/docker/overlay2/l/7YTDWL6UA7JEAI5KPCZEUDIUUF:/var/lib/docker/overlay2/l/OER42RBQQTIFRQ7LZRR4IPRQWU:/var/lib/docker/overlay2/l/2MR2ULDESC4VPFHAYTSOAVXLFF,upperdir=/var/lib/docker/overlay2/23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b/diff,workdir=/var/lib/docker/overlay2/23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b/work,xino=off) 层间组织关系简单示意： overlay2 driver层堆叠示意 存储结构 diff/：本层的内容。 link：当前层的短标识。 lower：当前层的父layers，按层序排列，除最底层外有此文件。 work/：overlayFS内部使用的文件，除最底层外有此目录。 merged/：其自身及其父layer的联合目录结构，只有容器层有此目录。 l/：此目录存放短id的符号链接。 # tree -L 2 /var/lib/docker/overlay2 /var/lib/docker/overlay2 ├── 013299735d8abd2c0cc5cbad32f011be44fad600a624bc6c2f1f94b9bffa64c2 │ ├── diff │ ├── link │ ├── lower │ └── work ├── 23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b │ ├── diff │ ├── link │ ├── lower │ ├── merged │ └── work ├── 23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b-init │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── 36bf33266ffa01007cdc8db601c77fb6bd5aa5d2dc8dd35fe873e1580073db82 │ ├── committed │ ├── diff │ └── link ├── 3862629a36e3286f03a2d9234633fdb0e9301a33a7195087afd3b3f92c1464fb │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── 6d1ca51490fa03a0ebf3767e0ddc93508593fa380b542af3e5187f0ec5e2629c │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work ├── 74e31d6f05b337350eb841d01d9728852f6fb0c69a007957dcf04cdb570d8b06 │ ├── committed │ ├── diff │ ├── link │ ├── lower │ └── work └── l ├── 2MR2ULDESC4VPFHAYTSOAVXLFF -\u003e ../36bf33266ffa01007cdc8db601c77fb6bd5aa5d2dc8dd35fe873e1580073db82/diff ├── 7YTDWL6UA7JEAI5KPCZEUDIUUF -\u003e ../74e31d6f05b337350eb841d01d9728852f6fb0c69a007957dcf04cdb570d8b06/diff ├── EYKZCXLZJEZJW55DB7S5ID2HKW -\u003e ../23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b/diff ├── OER42RBQQTIFRQ7LZRR4IPRQWU -\u003e ../3862629a36e3286f03a2d9234633fdb0e9301a33a7195087afd3b3f92c1464fb/diff ├── TQQ75355OL5U7VFEC6XG7YBWE5 -\u003e ../013299735d8abd2c0cc5cbad32f011be44fad600a624bc6c2f1f94b9bffa64c2/diff ├── XRV25KXVQWJ42GBK5UJD7ICFVV -\u003e ../23ae9a63ab9f4f4c36af29f78fb64d4e943c7af9f241b696e7870d37dca0130b-init/diff └── ZVAQQEPIP4VXNQK2LZM24JXP7H -\u003e ../6d1ca51490fa03a0ebf3767e0ddc93508593fa380b542af3e5187f0ec5e2629c/diff 注意 OverlayFS中删除文件仍然使用whiteout文件来阻止向容器层以下查找文件，只是在aufs中，whiteout文件以.wh.{FILENAME}命名的普通文件呈现；而在overlayFS中，whiteout文件以原文件命名的字符设备文件呈现。 例如，从附录ubuntu:16.04的镜像分析中可以看到第三层删除了/var/lib/apt/lists下的所有文件，对应layer的diff中可以看到，这些文件都变成了字符设备文件： $ ll -a /var/lib/docker/overlay2/74e31d6f05b337350eb841d01d9728852f6fb0c69a007957dcf04cdb570d8b06/diff/var/lib/apt/lists/ total 8 drwxr-xr-x 2 root root 4096 Aug 31 09:21 ./ drwxr-xr-x 3 root root 4096 Aug 5 03:01 ../ c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial_InRelease c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial_main_binary-amd64_Packages c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial_main_i18n_Translation-en c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial_restricted_binary-amd64_Packages c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial_restricted_i18n_Translation-en c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial-updates_InReleasec--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial-updates_main_binary-amd64_Packages c--------- 1 root root 0, 0 Oct 7 13:55 archive.ubuntu.com_ubuntu_dists_xenial-updates_main_i18n_Translation-en c--------- 1 root root 0, 0 Oct 7 13:55 archi","date":"2021-10-16","objectID":"/posts/storage-driver/:3:1","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Overlay Overlay driver每一层都构筑成完整的镜像，即每一层都是从最底层到当前层overlay出的完整结构，下一层的文件以硬链接的方式出现在它的上一层，lowerdir只由镜像层的top layer组成。 容器层挂载信息： # 镜像层每层以硬链接形式共享文件 $ cd /var/lib/docker/overlay; ls -i 07236efba039eb5cb4e0b6ec010218aefd17293f8297d43c04be4a5b0fd59ab7/root/bin/ls 09195a984aeac5c05ac3c487b1bb2ffb4d57465a3dc575c13e8d4610484ae0b2/root/bin/ls 655184 07236efba039eb5cb4e0b6ec010218aefd17293f8297d43c04be4a5b0fd59ab7/root/bin/ls 655184 09195a984aeac5c05ac3c487b1bb2ffb4d57465a3dc575c13e8d4610484ae0b2/root/bin/ls # 容器层仅overlay mount了镜像层最顶层的root目录 $ mount | grep overlay overlay on /var/lib/docker/overlay/4d0852eca897d746e415bffb325ad3661f8371987082f6b6c8e9d5b9fda9abc5/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay/07236efba039eb5cb4e0b6ec010218aefd17293f8297d43c04be4a5b0fd59ab7/root,upperdir=/var/lib/docker/overlay/4d0852eca897d746e415bffb325ad3661f8371987082f6b6c8e9d5b9fda9abc5/upper,workdir=/var/lib/docker/overlay/4d0852eca897d746e415bffb325ad3661f8371987082f6b6c8e9d5b9fda9abc5/work,xino=off) 层间组织关系简单示意： overlay driver层堆叠示意 存储结构 root/：本层完整的目录结构，下层文件以硬链的形式出现在本层，所有镜像层有且仅有此目录。 merged/： 其自身及其父layer的联合目录结构，只有容器层有此目录。 upper/：本层的内容，类似于overlay2中的diff目录，但只有容器层有此目录。 work/：overlayFS内部使用的文件，只有容器层有此目录。 lower-id：lowerdir（镜像层顶层）的id。 $ tree -L 2 /var/lib/docker/overlay /var/lib/docker/overlay ├── 07236efba039eb5cb4e0b6ec010218aefd17293f8297d43c04be4a5b0fd59ab7 │ └── root ├── 09195a984aeac5c05ac3c487b1bb2ffb4d57465a3dc575c13e8d4610484ae0b2 │ └── root ├── 15afdc0d3161ffd9002a5d7714bb3566a658bfa286de82ff95a49368769e5072 │ └── root ├── 4d0852eca897d746e415bffb325ad3661f8371987082f6b6c8e9d5b9fda9abc5 │ ├── lower-id │ ├── merged │ ├── upper │ └── work ├── 4d0852eca897d746e415bffb325ad3661f8371987082f6b6c8e9d5b9fda9abc5-init │ ├── lower-id │ ├── upper │ └── work └── f6bdc701b8624ef39632441ad84efde79de50ae2bc3c637eabf0827084d547ec └── root init layer 在上述driver中，存储目录中都有一个-init后缀的目录，此为init layer，位于容器层与镜像层之间，只读。其主要包含了docker为容器准备的一些文件： $ tree /var/lib/docker/overlay/2178b65434aae442f3e71937bc6f775063a9993a3684625e982c7195052e9289-init/ /var/lib/docker/overlay/2178b65434aae442f3e71937bc6f775063a9993a3684625e982c7195052e9289-init/ ├── lower-id ├── upper │ ├── dev │ │ └── console │ └── etc │ ├── hostname │ ├── hosts │ ├── mtab -\u003e /proc/mounts │ └── resolv.conf └── work └── work upper dir中除了mtab是指向/proc/mounts的软链接之外，其他都是空的普通文件。这些文件都是Linux runtime必须的文件，如果缺少会导致某些程序或库出现异常。init layer主要是用于占坑，避免系统因缺少特殊文件而崩溃，具体内容后续进行bind mount。 由于init layer很薄并且只读，上述讨论将其予以忽略，其本身可以帮助我们快速定位到容器层的id。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:3:2","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Aufs, overlay与overlay2比较 相同点 划分容器层与镜像层，镜像层只读可以复用；容器层可写，采用CoW机制。 底层基于file-level，而非block-level，可有效利用内存，但写操作开销大，CoW效率低。当文件很大时，对其修改需要全部copy-up到容器层，即便是仅仅进行了很小的修改。 不同点 Aufs driver是按照layer stack组织镜像层的，即在定位文件时需要沿着layer stack一层一层地去定位。因此其性能相较于overlayFS差，尤其是镜像层数比较深时。 Overlay driver每层都构筑完整的镜像目录结构，通过硬链接的形式复用底层镜像层的文件。在镜像层数较深时，定位文件及写容器层的性能要略好于overlay2。但是由于每层都是完整的镜像目录结构，各级子目录会占用大量的inode，尤其当层数很深时，inode易被耗尽。 Overlay2 driver每层仅包含当前层的增量内容，通过overlay multiple lower layers形式构筑lowerdir，解决了overlay driver中消耗大量inode的问题，也是docker官方推荐的storage driver。 ","date":"2021-10-16","objectID":"/posts/storage-driver/:4:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"附录 使用dive对ubuntu:16.04进行镜像分析： $ dive ubuntu:16.04 ubuntu:16.04的镜像分析 ","date":"2021-10-16","objectID":"/posts/storage-driver/:5:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"参考文献 About storage drivers Use the AUFS storage driver Use the OverlayFS storage driver DOCKER基础技术：AUFS ","date":"2021-10-16","objectID":"/posts/storage-driver/:6:0","tags":["container"],"title":"浅谈Docker Storage Driver","uri":"/posts/storage-driver/"},{"categories":["探索与实战"],"content":"Telepresence用于在本地轻松开发和调试服务，同时将服务代理到远程 Kubernetes 集群。 使用 telepresence 可以为本地服务使用自定义工具（如调试器和 IDE）， 并提供对 Configmap、Secret 和远程集群上运行的服务的完全访问。 ","date":"2021-10-02","objectID":"/posts/telepresence/:0:0","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"背景 应用容器化带来了高效率、强扩展性等优势的的同时也带来一些复杂性。尤其对于开发人员，需要将修改部署到容器中（构建容器-\u003e发布到registry-\u003e部署到集群中）。如果使用k8s这种容器平台，还需要维护容器编排及配置从而延长开发迭代周期。总之，在容器化应用中，研发要对软件的整个生命周期负责。 为了提升开发效率，需要引入将远程k8s集群与本地开发桥接起来的方法，以此减少反馈时间，提升debug效率。加速反馈的最佳实现方式是单个本地服务，其他依赖服务都是远程的，telepresence便是基于此思想的一个加速反馈， 便于调试及协作的开发者工具。 Telepresence被设计为让k8s开发者的笔记本如同加入到了k8s集群中一样，能够将服务在本地运行，并且被proxy到远端集群中。 ","date":"2021-10-02","objectID":"/posts/telepresence/:1:0","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"架构 Telepresence架构图 Telepresence CLI：用于编排出所有其他的组件，如telepresence daemon，traffic manager，授权Ambassador Cloud等。既起到bootstrap作用，又起到发送控制命令作用。 # 首次执行telepresence list cli $ telepresence list Launching Telepresence Daemon v2.3.2 (api v3) Connecting to traffic manager... Connected to context minikube (https://10.122.101.148:38443) frontend : ready to intercept (traffic-agent not yet installed) redis-follower: ready to intercept (traffic-agent not yet installed) redis-leader : ready to intercept (traffic-agent not yet installed) 上例可以看到，在初次执行telepresence cli时，其先启动了telepresence daemon，紧接着安装并连接traffic manager，最后才返回对应cmd的结果。 Telepresence Daemon：是开发者本地的代理点。 Traffic Manager：集群流量入口点，也是流量控制的中枢代理。同时它会与Ambassador Cloud交互以支持Preview URL的特性。 Traffic Agent：用于流量代理的sidecar，主要根据拦截规则判断到达的请求，要么将请求直接交付给pod对应的端口，要么路由到traffic manager以转发到本地服务中。 拦截方式 目前主要的拦截方式有两种：全量拦截和preview URL，前者traffic agent会全量地路由给traffic manager；后者会对请求进行判断，只有preview URL请求会路由到traffic manager，其他请求交付给pod内的服务。 Ambassador Cloud：产生临时域名，将preview URL从授权的用户路由到traffic manager。 ","date":"2021-10-02","objectID":"/posts/telepresence/:2:0","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"使用 ","date":"2021-10-02","objectID":"/posts/telepresence/:3:0","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"准备工作 安装telepresence k8s集群中安装guestbook demo $ kubectl get pod NAME READY STATUS RESTARTS AGE frontend-7bcb4574cb-j2wwz 1/1 Running 0 39s frontend-7bcb4574cb-rbxkj 1/1 Running 0 41s frontend-7bcb4574cb-xxm56 1/1 Running 0 44s redis-follower-dd4df4648-pvd2c 1/1 Running 5 50d redis-follower-dd4df4648-wzf54 1/1 Running 5 50d redis-leader-6d7765b8f6-mbzm8 1/1 Running 5 50d $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE frontend ClusterIP 10.111.60.121 \u003cnone\u003e 80/TCP 50d kubernetes ClusterIP 10.96.0.1 \u003cnone\u003e 443/TCP 50d redis-follower ClusterIP 10.96.4.108 \u003cnone\u003e 6379/TCP 50d redis-leader ClusterIP 10.101.93.34 \u003cnone\u003e 6379/TCP 50d 由于k8s环境是通过minikube虚拟机搭建的，为了方便在宿主机访问集群内部服务，我们使用port-forward进行端口转发。 # 集群服务器148开端口转发，将148宿主机上12180端口的流量转发到集群内frontend service中 $ kubectl port-forward svc/frontend 12180:80 --address=0.0.0.0 验证一下frontend service提供的是guestbook服务。 $ curl 10.122.101.148:12180 \u003chtml ng-app=\"redis\"\u003e \u003chead\u003e \u003ctitle\u003eGuestbook\u003c/title\u003e \u003clink rel=\"stylesheet\" href=\"//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css\"\u003e \u003cscript src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js\"\u003e\u003c/script\u003e \u003cscript src=\"controllers.js\"\u003e\u003c/script\u003e \u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/angular-ui-bootstrap/0.13.0/ui-bootstrap-tpls.js\"\u003e\u003c/script\u003e \u003c/head\u003e \u003cbody ng-controller=\"RedisCtrl\"\u003e \u003cdiv style=\"width: 50%; margin-left: 20px\"\u003e \u003ch2\u003eGuestbook\u003c/h2\u003e \u003cform\u003e \u003cfieldset\u003e \u003cinput ng-model=\"msg\" placeholder=\"Messages\" class=\"form-control\" type=\"text\" name=\"input\"\u003e\u003cbr\u003e \u003cbutton type=\"button\" class=\"btn btn-primary\" ng-click=\"controller.onRedis()\"\u003eSubmit\u003c/button\u003e \u003c/fieldset\u003e \u003c/form\u003e \u003cdiv\u003e \u003cdiv ng-repeat=\"msg in messages track by $index\"\u003e {{msg}} \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2021-10-02","objectID":"/posts/telepresence/:3:1","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"连接到集群 $ telepresence connect Launching Telepresence Daemon v2.3.2 (api v3) Connecting to traffic manager... Connected to context minikube (https://10.122.101.148:38443) 需要确保kubeconfig的配置，示例中的context连接到的apiserver为10.122.101.148:38443 ","date":"2021-10-02","objectID":"/posts/telepresence/:3:2","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"服务全量拦截 查看可拦截的服务列表： $ telepresence list frontend : ready to intercept (traffic-agent not yet installed) redis-follower: ready to intercept (traffic-agent not yet installed) redis-leader : ready to intercept (traffic-agent not yet installed) 结果中可以看出有三个service可以拦截，并且都没有安装traffic agent。不过不用担心，我们在进行服务拦截时会自动向pod内注入traffic agent。 拦截frontend留言板服务，拦截前我们先本地18888端口起一个nginx服务。 $ docker run --rm --name nginx-test -p 18888:80 -d nginx:1.21.1 9d514b006d2c386b7d3676bb40d1dafcbf9211ba9737e213e0dac180496c8d3a 验证一下nginx可以正常访问。 $ curl localhost:18888 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 下面我们来看看如何使用本地的nginx service来拦截集群中的frontend service。其实不过在本地执行一句简单的命令：\u0003 # telepresence intercept \u003cservice-name\u003e --port \u003clocal-port\u003e[:\u003cremote-port\u003e] --env-file \u003cpath-to-env-file\u003e $ telepresence intercept frontend --port 18888:80 --env-file frontend-svc.env Using Deployment frontend intercepted Intercept name : frontend State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:18888 Service Port Identifier: 80 Volume Mount Error : sshfs is not installed on your local machine Intercepting : all TCP connections 这样，我们便将对frontend:80的访问就会被转发到localhost:18888，即访问到的是nginx服务。验证一下： curl 10.122.101.148:12180 | head -5 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e 使用telepresence cli可以查看到服务的拦截状态： $ telepresence list frontend : intercepted Intercept name : frontend State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:18888 Service Port Identifier: 80 Intercepting : all TCP connections redis-follower: ready to intercept (traffic-agent not yet installed) redis-leader : ready to intercept (traffic-agent not yet installed) 此外，frontend service的endpoints也从80端口（frontend container）切换到了9900端口（traffic agent）。这意味着pod内访问9900端口（traffic agent）会经由traffic manager转发到本地nginx服务中。 $ kubectl describe svc frontend Name: frontend Namespace: default Labels: app=guestbook tier=frontend Annotations: telepresence.getambassador.io/actions: {\"version\":\"2.3.2\",\"make_port_symbolic\":{\"PortName\":\"\",\"TargetPort\":80,\"SymbolicName\":\"tx-80\"}} Selector: app=guestbook,tier=frontend Type: ClusterIP IP Family Policy: SingleStack IP Families: IPv4 IP: 10.111.60.121 IPs: 10.111.60.121 Port: \u003cunset\u003e 80/TCP TargetPort: tx-80/TCP Endpoints: 172.18.0.2:9900,172.18.0.7:9900,172.18.0.8:9900 # 端口从80切换到了9900 Session Affinity: None Events: \u003cnone\u003e 信息 上面提及到在执行telepresence intercept时会自动注入traffic agent这个sidecar，可以观察一下frontend svc对应的endpoint pod的信息。 $ kubectl get pod NAME READY STATUS RESTARTS AGE frontend-698684655-bfd2l 2/2 Running 0 7m50s frontend-698684655-ddh4c 2/2 Running 0 7m54s frontend-698684655-zvnmg 2/2 Running 0 7m47s redis-follower-dd4df4648-pvd2c 1/1 Running 5 50d redis-follower-dd4df4648-wzf54 1/1 Running 5 50d redis-leader-6d7765b8f6-mbzm8 1/1 Running 5 50d $ kubectl describe pod frontend-698684655-bfd2l Name: frontend-698684655-bfd2l Namespace: default ...... Containers: php-redis: Image: gcr.io/google_samples/gb-frontend:v5 Port: 80/TCP .... traffic-agent: Image: docker.io/datawire/tel2:2.3.2 Port: 9900/TCP ... ...... ","date":"2021-10-02","objectID":"/posts/telepresence/:3:3","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"拦截Preview URL 一般在开发环境中，会有多个开发人员协同工作。上述的拦截方案的最大问题是traffic agent无脑将请求全部转发给traffic manger，从而将流量全部转发到本地。这势必会造成对其他开发人员的影响，因此我们需要能够在不影响其他人的情况下进行拦截调试。实现上利用了context 传递来实现可控的拦截，借助Ambassador Cloud的preview URL给我们提供了可控拦截的能力。 Preview URL需要与ingress配合使用，所以第一步我们先创建一个ingress： $ cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: demo-ingress annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 spec: rules: - host: frontend.raygecao.com http: paths: - path: / pathType: Prefix backend: service: name: frontend port: number: 80 $ kubectl apply -f ingress.yaml $ kubectl describe ing demo-ingress Name: demo-ingress Namespace: default Address: 172.17.0.40 Default backend: default-http-backend:80 (\u003cerror: endpoints \"default-http-backend\" not found\u003e) Rules: Host Path Backends ---- ---- -------- frontend.raygecao.com / frontend:80 (172.18.0.2:9900,172.18.0.7:9900,172.18.0.8:9900) Annotations: nginx.ingress.kubernetes.io/rewrite-target: /$1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal UPDATE 3m54s (x2 over 20d) nginx-ingress-controller Ingress default/demo-ingress 然后我们清理掉之前的拦截。 $ telepresence leave frontend $ telepresence list frontend : ready to intercept (traffic-agent already installed) redis-follower: ready to intercept (traffic-agent not yet installed) redis-leader : ready to intercept (traffic-agent not yet installed) 注册并登陆Ambassador Cloud，然后重新进行拦截： $ telepresence intercept frontend --port 18888:80 --env-file frontend-svc.env To create a preview URL, telepresence needs to know how cluster ingress works for this service. Please Confirm the ingress to use. 1/4: What's your ingress' layer 3 (IP) address? You may use an IP address or a DNS name (this is usually a \"service.namespace\" DNS name). [default: 10.122.101.148]: 2/4: What's your ingress' layer 4 address (TCP port number)? [default: 9999]: 3/4: Does that TCP port on your ingress use TLS (as opposed to cleartext)? [default: n]: 4/4: If required by your ingress, specify a different layer 5 hostname (TLS-SNI, HTTP \"Host\" header) to access this service. [default: frontend.raygecao.com]: Using Deployment frontend intercepted Intercept name : frontend State : ACTIVE Workload kind : Deployment Destination : 127.0.0.1:18888 Service Port Identifier: 80 Volume Mount Error : sshfs is not installed on your local machine Intercepting : HTTP requests that match all headers: 'x-telepresence-intercept-id: 2da7518d-ce3f-4732-9c02-f144de3443a8:frontend' Preview URL : https://musing-kirch-7616.preview.edgestack.me Layer 5 Hostname : frontend.raygecao.com 注意 由于之前拦截时配置过一次，因此这一次一路默认就好，第一次配置需要结合自身的ingress来配置。 为了使hostname可以访问到集群内的服务，需要手动添加一条DNS记录将hostname resolve到ingress节点上（即在hosts文件中添加一条172.17.0.40 frontend.raygecao.com）。或者利用ingress路由请求的原理，直接将hostname指定到Hostheader进行路由。 可以看到，Ambassador Cloud为我们生成了一个preview URL https://musing-kirch-7616.preview.edgestack.me，并且添加了转发规则：所有包含'x-telepresence-intercept-id: 2da7518d-ce3f-4732-9c02-f144de3443a8:frontend'header并且host是frontend.raygecao.com的request会被本地nginx服务拦截。 Ambassador Cloud进行服务拦截管理 从效果上看，添加了这种特殊的header，就会将流量导到preview URL上，这正是ambassador cloud给予我们的便利。我们验证一下具体的拦截情况。 # 正常的请求不会被本地的nginx服务拦截 $ curl 10.122.101.148:9999 -H 'Host: frontend.raygecao.com' | head -5 \u003chtml ng-app=\"redis\"\u003e \u003chead\u003e \u003ctitle\u003eGuestbook\u003c/title\u003e \u003clink rel=\"stylesheet\" href=\"//netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css\"\u003e \u003cscript src=\"https://ajax.googleapis.com/ajax/libs/angularjs/1.2.12/angular.min.js\"\u003e\u003c/script\u003e # 添加header会被转发到preview URL中 $ curl 10.122.101.148:9999 -H 'Host: frontend.raygecao.com' -H 'x-telepresence-intercept-id: 2da7518d-ce3f-4732-9c02-f144de3443a8:frontend' | head -5 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e ","date":"2021-10-02","objectID":"/posts/telepresence/:3:4","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"参考文献 Telepresence官方文档 ","date":"2021-10-02","objectID":"/posts/telepresence/:4:0","tags":["k8s"],"title":"Telepresence——本地调试k8s服务利器","uri":"/posts/telepresence/"},{"categories":["探索与实战"],"content":"通过隔离namespace与cgroup构建出一个小型容器，项目来源：containers-from-scratch ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:0:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"概述 本项目以几十行代码搭建起了一个最简单的container，包含如下特点： Mount namespace隔离，通过chroot将container的文件系统隔离到宿主机的单个目录层次结构中。 Pid namespace隔离，保证container与host的pid相互独立。 使用独立的cgroup限制容器内的资源使用。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:1:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"代码分析 package main import ( \"fmt\" \"io/ioutil\" \"os\" \"os/exec\" \"path/filepath\" \"strconv\" \"syscall\" ) // go run main.go run \u003ccmd\u003e \u003cargs\u003e func main() { switch os.Args[1] { case \"run\": run() case \"child\": child() default: panic(\"help\") } } func run() { fmt.Printf(\"Running %v \\n\", os.Args[2:]) cmd := exec.Command(\"/proc/self/exe\", append([]string{\"child\"}, os.Args[2:]...)...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr cmd.SysProcAttr = \u0026syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID | syscall.CLONE_NEWNS, Unshareflags: syscall.CLONE_NEWNS, } must(cmd.Run()) } func child() { fmt.Printf(\"Running %v \\n\", os.Args[2:]) cg() cmd := exec.Command(os.Args[2], os.Args[3:]...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr must(syscall.Sethostname([]byte(\"container\"))) must(syscall.Chroot(\"/home/liz/ubuntufs\")) must(os.Chdir(\"/\")) must(syscall.Mount(\"proc\", \"proc\", \"proc\", 0, \"\")) must(syscall.Mount(\"thing\", \"mytemp\", \"tmpfs\", 0, \"\")) must(cmd.Run()) must(syscall.Unmount(\"proc\", 0)) must(syscall.Unmount(\"thing\", 0)) } func cg() { cgroups := \"/sys/fs/cgroup/\" pids := filepath.Join(cgroups, \"pids\") os.Mkdir(filepath.Join(pids, \"liz\"), 0755) must(ioutil.WriteFile(filepath.Join(pids, \"liz/pids.max\"), []byte(\"20\"), 0700)) // Removes the new cgroup in place after the container exits must(ioutil.WriteFile(filepath.Join(pids, \"liz/notify_on_release\"), []byte(\"1\"), 0700)) must(ioutil.WriteFile(filepath.Join(pids, \"liz/cgroup.procs\"), []byte(strconv.Itoa(os.Getpid())), 0700)) } func must(err error) { if err != nil { panic(err) } } run函数中主要有两个工作： Fork出子进程并调用child函数：/proc/self/exe 表明当前的程序，即fork出一份子进程执行当前程序的child命令。 设置Clone隔离属性：Cloneflags通过设置syscall.CLONE_NEWUTS,syscall.CLONE_NEWPID,syscall.CLONE_NEWNS分别隔离了uts, pid及mount namespace。Unshareflags设置了syscall.CLONE_NEWNS用以禁用挂载传播。 child函数中主要做了四件事： 为子进程设置cgroup，设置当前cgroup总的进程数上限为20。 更新子进程的hostname，用以验证uts namespace隔离。 更新子进程的root目录，将container文件系统隔离到宿主机中的单个目录中。 挂载proc及tmpfs，用以验证pid隔离以及mount隔离。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:2:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"视频讲解 视频基本上将代码的核心模块全部手敲了一遍，核心内容解释的较为清晰，但存在如下问题： 视频中run方法中并未设置cmd.SysProcAttr.Unshareflags=syscall.CLONE_NEWNS，在实践中发现，未指定此flag会导致容器内的挂载会泄露到宿主机上，这与挂载传播相关，笔者系统上默认使用shared传播类型，猜测Liz的系统默认使用private传播类型。 视频中在Liz退出容器时会触发panic，报错内容为No such file or directory，探索后发现是在第59行卸载thing时出错，看一下syscall关系Mount和Unmount的api： func Mount(source string, target string, fstype string, flags uintptr, data string) (err error) func Unmount(target string, flags int) (err error) 尽管umount命令支持使用设备文件或者挂载点，但从报错信息及syscall api上看到，比较稳妥的方式是通过挂载点卸载，因此Line:59建议改成must(syscall.Unmount(\"mytemp\", 0))。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:3:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"实践探究 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:4:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"准备ubuntufs 实现所需的ubuntufs是container的rootfs，为其提供必要的指令。笔者采用从ubuntu container中将整个文件系统拷贝出来。 docker run -it --rm ubuntu:21.04 docker cp ${UBUNTU_IMAGE_ID}:/ ~/ubuntufs 准备完需要更新Line:51chroot的path为ubuntufs所在的路径。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:4:1","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"运行容器并验证namespace隔离情况 # 容器内 root@container:/# ls -l /proc/self/ns/ total 0 lrwxrwxrwx 1 root root 0 Sep 24 05:57 cgroup -\u003e 'cgroup:[4026531835]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 ipc -\u003e 'ipc:[4026531839]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 mnt -\u003e 'mnt:[4026536295]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 net -\u003e 'net:[4026531969]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 pid -\u003e 'pid:[4026536294]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 user -\u003e 'user:[4026531837]' lrwxrwxrwx 1 root root 0 Sep 24 05:57 uts -\u003e 'uts:[4026536293]' # host $ ll /proc/self/ns total 0 lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 cgroup -\u003e cgroup:[4026531835] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 ipc -\u003e ipc:[4026531839] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 mnt -\u003e mnt:[4026531840] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 net -\u003e net:[4026531969] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 pid -\u003e pid:[4026531836] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 user -\u003e user:[4026531837] lrwxrwxrwx 1 ubuntu ubuntu 0 Sep 24 13:57 uts -\u003e uts:[4026531838] 由此可知，container内的mnt, pid, uts namespace与host的均不同。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:4:2","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"验证Cgourp设置 Cgroups的设置也生效，在后台起若干个sleep进程后，新起的进程被cgroup限制了。 Cgroup限制最大进程数 Liz在测试时使用了fork bomb :(){ :|:\u0026 };:，这种形式本质上是shell实现的一个自身指数递归调用，其简化形式为： bomb() { bomb | bomb \u0026 }; bomb :()定义了一个名字叫:的函数。 {}声明了函数体。 :|:表示函数递归调用并pipe到自身，从而实现进程数指数增长。 \u0026表示进程在后台运行。 ;表示函数调用结束。 :运行此函数，触发fork bomb。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:4:3","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"功能扩展 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:5:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"支持bind mount Bind mount将宿主目录映射到容器内，实现上比较简单，即在chroot jail前进行bind即可。示例代码如下： // bind mount testBindPath := filepath.Join(rootPath, \"test\") os.Mkdir(testBindPath, 0755) must(syscall.Mount(fmt.Sprintf(\"%s/test\", homePath), testBindPath, \"\", syscall.MS_BIND, \"\")) 上例将家目录下的test目录bind mount到rootfs的test目录，从而在容器内部可见。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:5:1","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"Cgroup资源扩展（Memory/CPU） 在memory和cpu下创建两个cgroup，设置好限制内容，并将container pid加入到这两个cgroup中： // Add cpu limitation for 0.3 core cpu := filepath.Join(cgroups, \"cpu\") os.Mkdir(filepath.Join(cpu, \"liz\"), 0755) must(ioutil.WriteFile(filepath.Join(cpu, \"liz/cpu.cfs_period_us\"), []byte(\"100000\"), 0700)) must(ioutil.WriteFile(filepath.Join(cpu, \"liz/cpu.cfs_quota_us\"), []byte(\"30000\"), 0700)) must(ioutil.WriteFile(filepath.Join(cpu, \"liz/notify_on_release\"), []byte(\"1\"), 0700)) must(ioutil.WriteFile(filepath.Join(cpu, \"liz/cgroup.procs\"), []byte(strconv.Itoa(os.Getpid())), 0700)) // Add memory limitation for 100M mem := filepath.Join(cgroups, \"memory\") os.Mkdir(filepath.Join(mem, \"liz\"), 0755) must(ioutil.WriteFile(filepath.Join(mem, \"liz/memory.limit_in_bytes\"), []byte(\"100M\"), 0700)) must(ioutil.WriteFile(filepath.Join(mem, \"liz/memory.swappiness\"), []byte(\"0\"), 0700)) must(ioutil.WriteFile(filepath.Join(mem, \"liz/notify_on_release\"), []byte(\"1\"), 0700)) must(ioutil.WriteFile(filepath.Join(mem, \"liz/cgroup.procs\"), []byte(strconv.Itoa(os.Getpid())), 0700)) 上例分别对cpu与memory进行了限制： CPU: 限制container最大使用核数为0.3 Memory: 限制物理内存上限为100M，且禁用swap，即内容使用超过了100M的话立刻触发OOM。 验证cgroup隔离效果： root@container:/# cat /proc/self/cgroup 12:perf_event:/ 11:cpuset:/ 10:devices:/user.slice 9:net_cls,net_prio:/ 8:pids:/liz 7:blkio:/user.slice 6:memory:/liz 5:rdma:/ 4:hugetlb:/ 3:freezer:/ 2:cpu,cpuacct:/liz 1:name=systemd:/user.slice/user-1000.slice/session-12.scope 0::/user.slice/user-1000.slice/session-12.scope 使用while : ; do : ; done 压测cpu limitation： CPU使用被限制在0.3Core 验证memory limitation： 分配400M内存导致OOM ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:5:2","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"使用pivot_root系统调用替换chroot pivot_root与chroot作用类似，都是将rootfs jail到一个目录上。区别在于前者更改此mount namespace下的所有进程的rootfs，后者仅更改当前进程的rootfs。 pivot_root核心思想是将root mount更改为new_root，并且原root mount会移到put_old中。其定义了一系列限制，列举如下： - new_root and put_old must be directories. - new_root and put_old must not be on the same mount as the current root. - put_old must be at or underneath new_root; that is, adding some nonnegative number of \"/..\" prefixes to the pathname pointed to by put_old must yield the same directory as new_root. - new_root must be a path to a mount point, but can't be \"/\". A path that is not already a mount point can be con‐ verted into one by bind mounting the path onto itself. - The propagation type of the parent mount of new_root and the parent mount of the current root directory must not be MS_SHARED; similarly, if put_old is an existing mount point, its propagation type must not be MS_SHARED. These re‐ strictions ensure that pivot_root() never propagates any changes to another mount namespace. - The current root directory must be a mount point. 参考 runc pivotRoot func 对代码进行修改： must(syscall.Mount(rootPath, rootPath, \"bind\", syscall.MS_BIND, \"\")) // jail rootfs with pivot_root syscall // ref: https://github.com/opencontainers/runc/blob/v1.0.2/libcontainer/rootfs_linux.go#L817 putOldPath := filepath.Join(rootPath, \"put_old\") os.Mkdir(putOldPath, 0755) must(syscall.PivotRoot(rootPath, putOldPath)) // lazy unmount must(syscall.Unmount(\"/put_old\", syscall.MNT_DETACH)) if err := os.Remove(\"/put_old\"); err != nil{ panic(err) } //must(syscall.Chroot(\"fmt.Sprintf(\"%s/ubuntufs\", homePath)) must(os.Chdir(\"/\")) 为满足第二条限制，new_root以bind mount形式脱离current root filesystem。 由于存在process使用原root mount下的文件，因此无法直接unmount掉put_old。这里使用lazy unmount（通过syscall.MNT_DETACH flag）的方式卸载掉。Lazy unmount使新的进程看不到此挂载点（隐藏掉），并且当接入此mount的进程全部退出后将其真正卸载掉。 注意 上述扩展的完整代码参考这里。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:5:3","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"知识延伸 为探究Line:34 Unshareflags对挂载的影响，我们了解一下挂载传播。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:6:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"挂载传播 Mount namespace有时会因提供太强的隔离性导致便捷性降低的问题，比如将一个新磁盘加载到一个光驱驱动器中，当有多个mount namespace时，需要将该磁盘挂载到每个namespace中。为了只使用一次挂载命令就可以将磁盘挂载到所有的mount namespace，Linux 从2.6.15起引入了共享子树特性（Shared Subtrees），即允许在namespace之间自动、可控地传播挂载和卸载事件。 此特性下，每个挂载点都有一个传播类型，此类型决定在此挂载点下创建/删除的挂载点能否传播到其他挂载点下，传播类型有如下四种： 传播类型 作用 MS_SHARED 该挂载点下同一对等组中的挂载点双向共享挂载和卸载事件 MS_SLAVE 该挂载点下同一对等组中的主挂载点可以将挂载或卸载事件单向传播到从属挂载点 MS_PRIVATE 挂载点不会将事件传播给任何对等方，同时也不会接收事件 MS_UNBINDABLE 在MS_PRIVATE基础上不能作为绑定挂载操作的源 判断挂载点的默认类型基本方法如下： 如果挂载点非根挂载点，且其父节点传播类型是MS_SHARED，则新挂载点的传播类型也是MS_SHARED。 否则挂载点的传播类型是MS_PRIVATE。 有了这个概念，我们验证一下这个Unshareflag加与不加的区别： 添加Unshareflags： # 容器中 root@container:/# cat /proc/self/mountinfo 4153 4751 0:610 / /proc rw,relatime - proc proc rw 4223 4751 0:642 / /mytemp rw,relatime - tmpfs thing rw 未添加Unshareflags： # 容器中 root@container:/# cat /proc/self/mountinfo 4750 4223 0:610 / /proc rw,relatime shared:304 - proc proc rw 4753 4223 0:642 / /mytemp rw,relatime shared:312 - tmpfs thing rw # 容器外 $ cat /proc/self/mountinfo | grep ubuntufs 4751 25 0:610 / ~/ubuntufs/proc rw,relatime shared:304 - proc proc rw 4754 25 0:642 / ~/ubuntufs/mytemp rw,relatime shared:312 - tmpfs thing rw 由于实验系统根挂载点是MS_SHARED类型，新建子挂载点的默认类型为MS_SHARED，因此container内的挂载会共享到host。 解决办法有两种: 如代码展示那样，加入Unshareflags=syscall.CLONE_NEWNS 参考 issue-38471。 指定根挂载为MS_PRIVATE类型，如must(syscall.Mount(\"\", \"/\", \"\", syscall.MS_PRIVATE|syscall.MS_REC, \"\"))。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:6:1","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"Unshare Linux unshare 指令可以新建namespace，并在namespace中运行程序，支持的命名空间类型有： Mount namespace，默认使用private传播类型。 UTS namespace IPC namespace Network namespace Pid namespace User namespace 欲达到上述的隔离效果，可以通过如下命令来进行： $ sudo unshare --fork --pid --mount-proc --uts /bin/bash --pid声明了pid namespace隔离。 --mount-proc挂载了proc，并声明了mount namespace隔离。 --uts声明了uts namespace隔离。 ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:6:2","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["探索与实战"],"content":"参考文献 containers-from-scratch 挂载命名空间和共享子树 unshare(1) ","date":"2021-09-27","objectID":"/posts/container-from-scratch/:7:0","tags":["container"],"title":"Containers from scratch探究","uri":"/posts/container-from-scratch/"},{"categories":["工作项目"],"content":"私有化交付的三要求：可复现性，易操作性和易维护性。 ","date":"2021-08-26","objectID":"/posts/oras/:0:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"私有化交付 公有云交付和私有化交付是ToB服务交付两种重要手段。公有云交付成本较低，数据安全性及性能的要求不需要太高，通常允许连接外网；而私有化交付一定程度上避免的资源的复用与共享，成本较高，但数据安全性高，适合数据敏感度高的企业，这些企业中大部分不允许连接外网。综上，私有化交付在数据安全很高的事业单位、银行及政府部门中占据主导地位。 私有化交付的一个重要的应用场景是如何将一组服务部署到与外部隔绝的内网环境，这也是我们今天研究的主题。本文不深入探究平台等基建是如何bootstrap出来，毕竟这个topic要溯源的话甚至要问一问服务器是哪来的。我们更专注于应用的打包方案。 显然，在网络受限的环境下，应用相关的所有资源都只能通过人肉搬运。下面的模式是私有化交付场景下常见的模式。 传统私有化交付方案 ","date":"2021-08-26","objectID":"/posts/oras/:1:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"一个应用部署需要啥？ 镜像：容器化是私有化交付的关键，而容器化中最重要的组成就是image，image在服务部署中是不可或缺的。 编排文件与配置信息（应用包）：私有化交付的特点是允许较高程度上的定制：不同私有化集群受节点数目、资源条件、特殊需求等因素，对服务的配置有着不同的需求。为了尽可能地自动化，将编排及默认配置打包在应用中是不错的选择，其中比较经典的例子是helm chart。编排及配置相当于应用管理工具包，有了它可以便于部署的自动化，但并不像image那么刚需。 数据包：应用中的部分服务可能会依赖于某些数据包，如算法模型、调试工具包等。服务相关的附属物件的都可以认为是一种数据包。是否需要数据包是具体服务确定的。 打包应用，无外乎将应用所需的上述的所有组件pack起来带到现场进行部署。 ","date":"2021-08-26","objectID":"/posts/oras/:2:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"当前交付方案 镜像/编排/数据包各自有着不同的发布流程，并且各自的发布中心独立，比如： 镜像的发布中心可以是docker registry。 编排包的发布中心可以是chart museum。 数据包的发布中心可以是某个s3。 为保证系统的可复现性，意味着现场环境也需要mirror这些infra，结构大致如下： 当前mirror方案 这个架构存在两个痛点： 发布中心分散，无法进行统一管理，增加维护成本。 公网中将各个组件打成一个tar包，隔离性太强，很多版本的image/数据包没有变化，存在同一组件打包多份的情况。这增加了打包时长，浪费公网存储资源。 ","date":"2021-08-26","objectID":"/posts/oras/:3:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"优化方向 ","date":"2021-08-26","objectID":"/posts/oras/:4:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"统一制品仓库 为解决第一个问题，我们考虑是否可以将这些发布中心归拢在一起。我们把上面提及的镜像，应用包及数据包统一描述为制品（artifact），我们需要一个统一制品仓库来实现这些制品的发布。而OCI registry是个不错的选择，理由是： OCI registry即满足OCI Distribution Spec的registry，很多开源产品实现了此规范，如docker registry, harbor, nexus等。灵活性较强，可以根据发布规模、运维需求等因素灵活选型。 OCI registry天然支持docker image的发布。上述制品中，只有image的发布最为复杂，OCI Image Format Spec里定义的镜像格式是基于content-addressed的，而一般的存储系统都是location-addressed，使用registry统一制品会相对简单一些。 普通制品使用OCI registry存储已存在开源的解决方案oras (OCI registry as storage)，可以方便地实现content-addressable 特性。 ","date":"2021-08-26","objectID":"/posts/oras/:4:1","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"公网发布中心改造 公网发布中心面临的最大问题就是存储空间的浪费。由于不同应用间以及同一应用各版本均为彼此隔离地打包上传，这导致了公网发布中心无法对内容做任何复用，经常一份镜像要重复打包成百上千次（比如ubuntu 这种base image），极大的浪费了存储空间。 一个显而易见的优化方向是将公网发布中心向着OCI registry方向改造，这样很多content可以被复用，这会带来一系列的好处： 节省了大量存储空间。 利用layer cache可以节省打包上传的时间。 可以增加layer粒度的diff机制，用户在公网下在新版本的应用时，只需要下载对应的patch即可，提升交付效率。 技巧 如果公网也是OCI registry，那么包的上传就有点类似于docker push，当上传的layer在registry已存在时，对应的blob将不会重复上传。只有新增的layer blob会被上传，形式上类似于增加了一层layer cache。 ","date":"2021-08-26","objectID":"/posts/oras/:4:2","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"增量发布 如果app bundle尺寸很大的话，会对交付效率带来较大的影响。 下载应用包耗时较长。 应用包占用的存储空间较大，并且应用包的跨网离线拷贝耗时较长。 现场环境加载应用包耗时较长。 交付效率 交付效率并没有一个官方的定义，我们这里姑且用将一个应用从公司内网环境mirror到现场环境并成功部署所耗费的时间来表述。增量发布只会影响mirror的效率，对部署效率不会有影响。 初次交付时，这一问题无法避免，因为这时现场是个空的registry，需要mirror应用所需的所有artifacts。但是如果现场因新需求或bugfix等原因需要升级时，就不需要mirror另一个版本的全部content了。我们可以对这两个版本的bundle进行diff，生成一个补丁包来承载新版本存在，但老版本不存在的blob集合。如下图所示，app从v1.0.0升级到v1.0.1仅需要将M2与L4打到补丁包里即可，即其复用了就版本的L1与L2两个blob。 app v1.0.1基于v1.0.0版本的patch 增量发布对于小版本bugfix效率很高，改动越小，可以复用的layer越多，补丁包就越小，交付效率也就越高。 ","date":"2021-08-26","objectID":"/posts/oras/:4:3","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"优化方案 ","date":"2021-08-26","objectID":"/posts/oras/:5:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"现有开源项目参考 ORAS：使用OCI registry存储artifacts。 可以将若干文件以layer的方式存到某个repository下。 push操作只能实现一层关联，即manifest =\u003e layers+config。 依赖特定annotation，无法与docker image兼容，即无法pull出docker image。 crane: 与registry交互的go lib。 基本封装了OCI Distribution Spec。 与image format绑定较深，对OCI Artifacts支持不完善，尤其是对index的支持。 cnab-to-oci：使用OCI registry来发布应用包（application bundle），其引入了打包的概念，将若干Image打包成一个bundle，并在registry中流转。 突出了app bundle的概念，便于应用整体的管理。 与Docker Image Format Spec强绑定，底层调用moby sdk，无法支持通用的artifacts。 helm registry：chart 支持基于OCI registry的发布。 local cache以OCI image-layout结构组织，content addressable \u0026 location addressable。 与registry的交互必须依赖于local cache。 同样仅支持两层镜像结构 containered 一个容器运行时的标准，其中imagespkg包含对OCI image format的处理，remotespkg包含与registry交互的底层sdk。 兼容全部的OCI Spec。 封装度低，对image的处理十分通用，不局限于docker image。 ","date":"2021-08-26","objectID":"/posts/oras/:5:1","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"组织结构 打包操作无非是将若干的artifacts包在一起，并给其一个用于定位的reference（包名），而其对偶的拆包操作就是根据包名能获取到所有artifacts。 cnab给我们提供一个很好的组织方式的参考，bundle reference作为检索的入口，可以关联到所有的artifacts，形式上就类似于一个image index与image manifest之间的关系。 cnab’s bundle format in OCI registry 简化一下组织结构，本质上就是一棵多叉树。 bundle组织结构 ","date":"2021-08-26","objectID":"/posts/oras/:5:2","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"实践探索 我们以一个具体的例子来简化一下我们要解决的问题：我们如何使用helm在私有化k8s集群中部署起来一个nginx服务。我们要做的是将helm chart与image发布并打包，在现场环境中导入这些artifacts。 结合oras, crane与containerd sdk开发一个小工具cb，提供如下核心功能 cmd 作用 实现要点 push 上传普通制品（非docker image） 对oras.Push做简单封装 pull 下载普通制品，是push的对偶操作 对oras.Pull做简单封装 bundle 打包生成bundle，将一组制品关联起来 生成index索引artifact list，并将index及artifacts push到registry中特定的repository里 pack 将registry中的bundle保存到本地 以OCI image-layout的形式存储上述结构，相关的reference通过特殊的annotation记录在index.json中 load 从OCI image-layout中恢复bundle 加载OCI image-layout中的index.json并将关联的content及reference push到registry diff 比较并产生target reference相对source reference的patch包 形式仍以OCI image-layout存在，只是blobs与index.json中存在的是两个ref之间的diff，而非全量的引用关系。一般来讲，diff产生的patch包，脱离了source reference是无法加载的 patch 加载patch包 Provider需要结合source reference与patch包构建完整target reference。另外patch包里记录source与target是一个不错的选择 此外cb还wrap了crane的manifest，catalog及list 功能，另外结合了go-graphviz实现了OCI layer可视化功能。 ","date":"2021-08-26","objectID":"/posts/oras/:6:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"准备阶段 在本地起一个docker registry（确保registry是干净的），并添加dns myregistry。 在docker hub上下载ubuntu:21.04和nginx:1.21.1，以及使用helm create创建默认的nginx chart。 ","date":"2021-08-26","objectID":"/posts/oras/:6:1","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"上传制品 # push chart $ cb push myregistry:5000/nginx-chart:1.0.0 --files mychart # push nginx image $ docker tag nginx:1.21.1 myregistry:5000/nginx:1.21.1 $ docker push myregistry:5000/nginx:1.21.1 $ cb catalog myregistry:5000 # 列出registry中的repositories NO. NAME 0 nginx 1 nginx-chart Push仅仅包装了oras cli，nginx chart的manifest如下 { \"schemaVersion\": 2, \"config\": { \"mediaType\": \"application/vnd.unknown.config.v1+json\", \"digest\": \"sha256:44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\", \"size\": 2 }, \"layers\": [ { \"mediaType\": \"application/vnd.oci.image.layer.v1.tar+gzip\", \"digest\": \"sha256:64f02409c5583265a67390256055c95902696345bdfb46a566b14ea9eac7c306\", \"size\": 3943, \"annotations\": { \"io.deis.oras.content.digest\": \"sha256:f2c5bf294ce3a1fcd249c00df5f916135059d107cc3f35c92c86189b7113d74e\", \"io.deis.oras.content.unpack\": \"true\", \"org.opencontainers.image.title\": \"mychart\" } } ] } ","date":"2021-08-26","objectID":"/posts/oras/:6:2","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"打包bundle 定义nginx-app bundle，tag为v1.0.0，包含nginx镜像与nginx chart。bundle.yaml简化如下 name: myregistry:5000/nginx-app tag: v1.0.0 artifacts: - name: \"myregistry:5000/nginx:1.21.1\" - name: \"myregistry:5000/nginx-chart:1.0.0\" $ cb bundle nginx-bundle.yaml # 打包nginx-app nginx-app:v1.0.0的index如下，reference记录在org.opencontainers.image.ref.name annotation里，这个reference（tag）需要被记录并在现场加载时恢复。 { \"schemaVersion\": 2, \"manifests\": [ { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:053598290cc6fad47d9af8f98baa939d6ebb92f672f7a1871e29cb55a85f2964\", \"size\": 257 }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"digest\": \"sha256:5e95e5eb8be4322e3b3652d737371705e56809ed8b307ad68ec59ddebaaf60e4\", \"size\": 1570, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx:1.21.1\" } }, { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:7ca59b0f5387479b242896fa0c507d8c29dfa5292100a5576f8663a546e41611\", \"size\": 602, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx-chart:1.0.0\" } } ] } ","date":"2021-08-26","objectID":"/posts/oras/:6:3","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"下载bundle # 将nginx-app bundle下载到nginx-app目录下 $ cb pack myregistry:5000/nginx-app:v1.0.0 -o nginx-app # 验证nginx-app中的结构符合OCI Image Layer Spec $ tree nginx-app nginx-app ├── blobs │ └── sha256 │ ├── 053598290cc6fad47d9af8f98baa939d6ebb92f672f7a1871e29cb55a85f2964 │ ├── 12455f71a9b5e0c207a601fb32bcf7f10a933d7193574d968409bbc5c2d89fe0 │ ├── 2a53fa598ee20ad436f2f9da7c0a21cce583bd236f47828895d771fb2e8795e1 │ ├── 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a │ ├── 572061c855037851b6384e6bba08cb7d48f71e74631865641518644ba1469e32 │ ├── 5e95e5eb8be4322e3b3652d737371705e56809ed8b307ad68ec59ddebaaf60e4 │ ├── 64f02409c5583265a67390256055c95902696345bdfb46a566b14ea9eac7c306 │ ├── 7ca59b0f5387479b242896fa0c507d8c29dfa5292100a5576f8663a546e41611 │ ├── 9e324aa228dbd3c1b80ea7c20b6d63b897605fa92f168ccce976a2df42375e77 │ ├── b86f2ba62d17b165964516228297d3ba669d60b6a283b5fd7779b27d7ec33871 │ ├── dd34e67e3371dc2d1328790c3157ee42dfcae74afffd86b297459ed87a98c0fb │ ├── e1acddbe380c63f0de4b77d3f287b7c81cd9d89563a230692378126b46ea6546 │ ├── e21006f71c6fb784a76159590b6ba8ab3fb22e5026f67abcf5feb8e4231837d6 │ └── f3341cc17e586daa9660abf087f13b2eba247bcf6646ee972e85d4cbaf18dbae ├── index.json ├── ingest └── oci-layout # 查看index内容，相当于把nginx-app在全局index中拍平 $ cat nginx-app/index.json| jq . { \"schemaVersion\": 2, \"manifests\": [ { \"mediaType\": \"application/vnd.oci.image.index.v1+json\", \"digest\": \"sha256:572061c855037851b6384e6bba08cb7d48f71e74631865641518644ba1469e32\", \"size\": 674, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx-app:v1.0.0\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"digest\": \"sha256:5e95e5eb8be4322e3b3652d737371705e56809ed8b307ad68ec59ddebaaf60e4\", \"size\": 1570, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx:1.21.1\" } }, { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:7ca59b0f5387479b242896fa0c507d8c29dfa5292100a5576f8663a546e41611\", \"size\": 602, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx-chart:1.0.0\" } } ] } ","date":"2021-08-26","objectID":"/posts/oras/:6:4","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"加载bundle 删除掉registry中的内容并重启，将nginx-app加载到registry中，可以恢复完整的nginx-app bundle。 $ cb load nginx-app ","date":"2021-08-26","objectID":"/posts/oras/:6:5","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"多层结构 当前打包策略实现了OCI Artifacts的高扩展性，结构树可以灵活地向上延展，比如bundle里嵌套bundle。 name: myregistry:5000/ubuntu-nginx-app tag: v1.0.0 artifacts: - name: \"myregistry:5000/nginx-app:v1.0.0\" # 这是一个bundle - name: \"myregistry:5000/ubuntu:21.04\" # 这是一个image 打包后，ubuntu-nginx-app:v1.0.0的index如下： { \"schemaVersion\": 2, \"manifests\": [ { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:52e0a43cc3025f0814510022bc2bc7f64135a9f4c93944ea0b82df344cc798c0\", \"size\": 257 }, { \"mediaType\": \"application/vnd.oci.image.index.v1+json\", \"digest\": \"sha256:572061c855037851b6384e6bba08cb7d48f71e74631865641518644ba1469e32\", \"size\": 674, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/nginx-app:v1.0.0\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"digest\": \"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\", \"size\": 529, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/ubuntu:21.04\" } } ] } 对应的层序结构为： ubuntu-nginx-app:v1.0.0镜像的层序结构 下载与加载均可以正常工作。 ","date":"2021-08-26","objectID":"/posts/oras/:6:6","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"补丁机制 通过ubuntu-nginx-app:v1.0.0与nginx-app:v1.0.0两个bundle验证一下补丁机制的可用性。从上述层序结构图中可以看到后者是前者的一个子集，所以预想中patch包应包含根节点及其左右两个子分支全部节点对应的blob，及ubuntu-nginx-app:v1.0.0和ubuntu:21.04两个reference。 $ cb diff myregistry:5000/ubuntu-nginx-app:v1.0.0 myregistry:5000/nginx-app:v1.0.0 -o patch INFO[0000] get 6 diffs $ tree patch patch ├── blobs │ └── sha256 │ ├── 4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1 │ ├── 52e0a43cc3025f0814510022bc2bc7f64135a9f4c93944ea0b82df344cc798c0 │ ├── a2723fc64a92418869862e0a14d8e913641ba6e4bca78cf43d0db4be4c3c14fa │ ├── bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481 │ ├── ee951c00fa8985cc5f5b49f0d7fe5e456697198f24fed85142b4869083b7085b │ └── ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0 ├── index.json ├── ingest └── oci-layout 3 directories, 8 files $ cat patch/index.json | jq . { \"schemaVersion\": 2, \"manifests\": [ { \"mediaType\": \"application/vnd.oci.image.index.v1+json\", \"digest\": \"sha256:a2723fc64a92418869862e0a14d8e913641ba6e4bca78cf43d0db4be4c3c14fa\", \"size\": 669, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/ubuntu-nginx-app:v1.0.0\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"digest\": \"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\", \"size\": 529, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/ubuntu:21.04\" } } ] } 上述结果表明patch包中blobs及index.json中的引用均符合我们预期。 清空registry并将nginx-app:v1.0.0load到registry中以验证应用patch的正确性。 $ cb load nginx-app INFO[0000] successfully push ref myregistry:5000/nginx-app:v1.0.0 INFO[0001] successfully push ref myregistry:5000/nginx-chart:1.0.0 $ cb patch patch INFO[0000] successfully push ref myregistry:5000/ubuntu:21.04 INFO[0001] successfully push ref myregistry:5000/ubuntu-nginx-app:v1.0.0 ","date":"2021-08-26","objectID":"/posts/oras/:6:7","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["工作项目"],"content":"总结 本文主要介绍了私有化交付下的应用打包方案，简单介绍了私有化交付的模式，提出了现有的打包方案及基于layer cache的一些优化方向。 本文要解决的问题是如何将一组内容从一个registry离线搬运到另一个registry中。问题的核心是如何将一组artifacts关联起来，这些内容应以什么组织形式在公司与现场之间流转。 通过bundle的概念对应用进行打包，便于对应用及版本进行管理。而OCI image layout是将bundle本地化的一个不错的解决思路，此架构可以与helm OCI完全兼容。 ","date":"2021-08-26","objectID":"/posts/oras/:7:0","tags":["container","platform"],"title":"私有化交付平台——打包篇","uri":"/posts/oras/"},{"categories":["探索与实战"],"content":"OCI Image Format定义了content addressable与location addressable结合的分层树状结构。 ","date":"2021-08-26","objectID":"/posts/oci-image/:0:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"基本组成 Image manifest：用于对镜像的内容寻址。 Image index：指向多个manifest的更高级别manifest，一般用于区分多平台。 Filesystem layers：用于描述容器文件系统内容变化。 Configuration：用于记录镜像配置及运行时信息等元信息。 引用go-containerregistry项目中结构图来宏观描述一下上述组成的关系： 镜像层级结构 OCI Image Spec中有更细化的描述： ","date":"2021-08-26","objectID":"/posts/oci-image/:1:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Content Descriptor Content descriptor用于描述对象内容的位置，组件内的descriptor可以描述当前组件对其他组件的引用关系，其应包含如下核心元素： 内容的类型 内容唯一标识 内容的大小 ","date":"2021-08-26","objectID":"/posts/oci-image/:2:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"属性描述 属性 类型 作用 mediaType string 对象内容的类型 digest string 对象内容的唯一标识，常使用sha256算法加密 size int64 对象内容的字节数 urls []string 对象可以被下载的url列表（optional） annotations map[string]string 携带额外信息的键值对集合 mediaType的作用 mediaType用于唯一地标识当前blob的类型，通过此类型可以标准化对blob的处理。以containerd的image.ChildrenHandler获取当前descriptor所有直接子引用为例。可以看出对不同blob的处理依据就是mediaType。 // Children returns the immediate children of content described by the descriptor. func Children(ctx context.Context, provider content.Provider, desc ocispec.Descriptor) ([]ocispec.Descriptor, error) { var descs []ocispec.Descriptor switch desc.MediaType { case MediaTypeDockerSchema2Manifest, ocispec.MediaTypeImageManifest: p, err := content.ReadBlob(ctx, provider, desc) if err != nil { return nil, err } // TODO(stevvooe): We just assume oci manifest, for now. There may be // subtle differences from the docker version. var manifest ocispec.Manifest if err := json.Unmarshal(p, \u0026manifest); err != nil { return nil, err } descs = append(descs, manifest.Config) descs = append(descs, manifest.Layers...) case MediaTypeDockerSchema2ManifestList, ocispec.MediaTypeImageIndex: p, err := content.ReadBlob(ctx, provider, desc) if err != nil { return nil, err } var index ocispec.Index if err := json.Unmarshal(p, \u0026index); err != nil { return nil, err } descs = append(descs, index.Manifests...) default: if IsLayerType(desc.MediaType) || IsKnownConfig(desc.MediaType) { // childless data types. return nil, nil } log.G(ctx).Debugf(\"encountered unknown type %v; children may not be fetched\", desc.MediaType) } return descs, nil } ","date":"2021-08-26","objectID":"/posts/oci-image/:2:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Image Manifest Manifest用于定位镜像内容，可以认为是一个镜像的实际入口，包含一个特定platform下image所需的全部信息： 若干个layers 一个configuration ","date":"2021-08-26","objectID":"/posts/oci-image/:3:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"MediaType application/vnd.oci.image.manifest.v1+json OCI Image Format Spec application/vnd.docker.distribution.manifest.v2+json 兼容Docker Image Format Spec ","date":"2021-08-26","objectID":"/posts/oci-image/:3:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"属性描述 属性 类型 作用 schemaVersion int 指定manifest schema，为确保与旧版本docker兼容，此Spec下固定值为2 mediaType string 内容的类型 config descriptor 与容器运行时相关的配置信息 layers []descriptor 用于构建镜像内文件系统布局，其中layers[0]描述base layer annotations map[string]string 携带额外信息的键值对集合 ","date":"2021-08-26","objectID":"/posts/oci-image/:3:2","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"实践探索 以linux/amd64下的ubuntu:21.04为例，我们看一下其manifest: { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"config\": { \"mediaType\": \"application/vnd.docker.container.image.v1+json\", \"size\": 1462, \"digest\": \"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" }, \"layers\": [ { \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 31837572, \"digest\": \"sha256:4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1\" } ] } 可以看到，其mediaType为application/vnd.docker.distribution.manifest.v2+json，包含一个config blob与一个layer blob。 ","date":"2021-08-26","objectID":"/posts/oci-image/:3:3","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Image Index Index又被称为fat manifest，manifest可以视为layer的索引，而index是在manifest上又加了一层的索引。有了index，这种两层树状结构变成了多层，提供了多描述符入口点。 在docker image中，index的主要作用是区分多平台（OS/ORCH）。 ","date":"2021-08-26","objectID":"/posts/oci-image/:4:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"MediaType application/vnd.oci.image.index.v1+json OCI Image Format Spec application/vnd.docker.distribution.manifest.list.v2+json 兼容Docker Image Format Spec ","date":"2021-08-26","objectID":"/posts/oci-image/:4:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"属性描述 属性 类型 作用 schemaVersion int 指定manifest schema，为确保与旧版本docker兼容，此spec下固定值为2 mediaType string 内容的类型 manifests []object 描述运行时要求的最小集，主要是操作系统/架构等平台相关，列表中有多个manifest，提供平台相关的属性用以进行filter。 annotations map[string]string 携带额外信息的键值对集合 ","date":"2021-08-26","objectID":"/posts/oci-image/:4:2","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"实践探索 我们看一下dockerhub上ubuntu:21.04的index。 { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.list.v2+json\", \"manifests\": [ { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\", \"platform\": { \"architecture\": \"amd64\", \"os\": \"linux\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:b7de3b708ddbdb5ca7d0a6a81f6d9df450276fc4794174a7b7a3441b00281a61\", \"platform\": { \"architecture\": \"arm\", \"os\": \"linux\", \"variant\": \"v7\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:ca763e1a382a5b23f91abaf1c36a84be33da2d657f45746112f28ae010571041\", \"platform\": { \"architecture\": \"arm64\", \"os\": \"linux\", \"variant\": \"v8\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:54b3fc49fc1949bcedbafbf1f18393920545ba934331cf72176cb14087962879\", \"platform\": { \"architecture\": \"ppc64le\", \"os\": \"linux\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:9c389f10c2b192dd01e87188c7cf1591dc830370046085190dd3ecfdaa1f2cfb\", \"platform\": { \"architecture\": \"riscv64\", \"os\": \"linux\" } }, { \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"size\": 529, \"digest\": \"sha256:13532df2f7a272c2c973268db5264059be5ba9882962d30db3d86ca38db3a737\", \"platform\": { \"architecture\": \"s390x\", \"os\": \"linux\" } } ] } 从中不难看出，该镜像提供了六种CPU架构下编译的ubuntu镜像，当某个client发出docker pull命令时，registry会index到对应的架构平台，找到合适的manifest。 ","date":"2021-08-26","objectID":"/posts/oci-image/:4:3","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Filesystem layer Layer是镜像内文件系统的组成成分，每一层都在描述一系列文件系统变化。 ","date":"2021-08-26","objectID":"/posts/oci-image/:5:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"MediaType application/vnd.oci.image.layer.v1.tar+gzip OCI Image Format Spec application/vnd.docker.image.rootfs.diff.tar.gzip 兼容Docker Image Format Spec ","date":"2021-08-26","objectID":"/posts/oci-image/:5:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"实践探索 我们copy出ubuntu:21.04的layer并解压，看一下base image的样式，tar内文件太多，仅列出前10行。 $ cp blobs/sha256/44/4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1/data ~/ubuntu.tar.gz \u0026\u0026 gzip -d ~/ubuntu.tar.gz -c | tar tv | head -10 # output lrwxrwxrwx 0/0 0 2021-07-24 01:47 bin -\u003e usr/bin drwxr-xr-x 0/0 0 2021-04-19 15:26 boot/ drwxr-xr-x 0/0 0 2021-07-24 01:50 dev/ drwxr-xr-x 0/0 0 2021-07-24 01:50 etc/ -rw------- 0/0 0 2021-07-24 01:47 etc/.pwd.lock -rw-r--r-- 0/0 3028 2021-07-24 01:47 etc/adduser.conf drwxr-xr-x 0/0 0 2021-07-24 01:50 etc/alternatives/ -rw-r--r-- 0/0 100 2021-04-14 18:32 etc/alternatives/README lrwxrwxrwx 0/0 0 2021-07-24 01:50 etc/alternatives/awk -\u003e /usr/bin/mawk lrwxrwxrwx 0/0 0 2021-07-24 01:50 etc/alternatives/nawk -\u003e /usr/bin/mawk 技巧 可以通过... | awk '{print $6}' | awk -F/ '{print $1}'| sort | uniq 对上述输出结果进行聚合获取第一层目录，结果可以看到就是标准的ubuntu root filesystem。 再探索一下filesystem changeset的内容，新创建一个镜像，修改镜像内的文件系统 FROM ubuntu:21.04 RUN echo \"hello world\" \u003e /tmp/hello.txt COPY ccc . 分别将第二层与第三层的内容拷贝出来并解压 # 略去拷贝过程 ######Layer2###### $ tar zxvf layer2.tar.gz tmp/ tmp/hello.txt ######Layer3###### $ tar zxvf layer3.tar.gz ccc 上述结果验证了之前踩过的一个坑：写Dockerfile构建镜像时，使用COPY将宿主机上的文件复制到镜像里时，如果源文件变化了，docker缓存会失效。之前误以为一个dockerfile中的一条语句对应一个layer，只要语句不变，layer就不变，就可以使用cache。此例清晰地描述出layer会与文件系统的changeset密切相关。 ","date":"2021-08-26","objectID":"/posts/oci-image/:5:2","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Configuration 用于描述镜像的一些元信息及容器运行时所需的信息。 ","date":"2021-08-26","objectID":"/posts/oci-image/:6:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"MediaType application/vnd.oci.image.config.v1+json OCI Image Format Spec application/vnd.docker.container.image.v1+json 兼容Docker Image Format Spec ","date":"2021-08-26","objectID":"/posts/oci-image/:6:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"属性描述 属性 类型 作用 created string 描述镜像创建日期 author string 描述镜像创建的作者 architecture string 描述编译镜像中二进制包的节点CPU架构 os string 描述构建镜像的节点的操作系统 config object 容器运行时所需要的执行参数（docker run中所能指定的参数），如Volumes，Env，ExposedPort等 rootfs object 描述image各层DiffID history object 描述每一层的历史信息 上述大部分属性可以通过docker inspect [IMAGE]获取到。 DiffID DiffID是layer未压缩时的tar包hash后的digest，可用于解压后内容验证。 由于DiffID仅能描述某个layer的信息，无法描述整个layer布局的信息，因此又引入ChainID来校验image的布局，主要思想是引入与之前layer的相关性来生成对应layer的ID。从定义上看，第一层base layer的DiffID与ChainID一致。 ","date":"2021-08-26","objectID":"/posts/oci-image/:6:2","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"实践探索 展示一下ubuntu:21.04的configuration，部分与container相关的字段超出此spec范围。 { \"architecture\": \"amd64\", \"config\": { \"Hostname\": \"\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"bash\" ], \"Image\": \"sha256:2a1126c0612fcbe61f0acaa6b1f2caf3a156b31684219de8bbb763ee3e99940c\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": null }, \"container\": \"acac01451c096428e536623ecd3887aa7c79f8377ac8a94885b6ceae8971dfcf\", \"container_config\": { \"Hostname\": \"acac01451c09\", \"Domainname\": \"\", \"User\": \"\", \"AttachStdin\": false, \"AttachStdout\": false, \"AttachStderr\": false, \"Tty\": false, \"OpenStdin\": false, \"StdinOnce\": false, \"Env\": [ \"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ], \"Cmd\": [ \"/bin/sh\", \"-c\", \"#(nop) \", \"CMD [\\\"bash\\\"]\" ], \"Image\": \"sha256:2a1126c0612fcbe61f0acaa6b1f2caf3a156b31684219de8bbb763ee3e99940c\", \"Volumes\": null, \"WorkingDir\": \"\", \"Entrypoint\": null, \"OnBuild\": null, \"Labels\": {} }, \"created\": \"2021-07-26T21:21:54.791192114Z\", \"docker_version\": \"20.10.7\", \"history\": [ { \"created\": \"2021-07-26T21:21:54.424131139Z\", \"created_by\": \"/bin/sh -c #(nop) ADD file:6ae44786caae9af1c6b70dc9cc244e7d4e06fffc0696f68877527d69aa3fc735 in / \" }, { \"created\": \"2021-07-26T21:21:54.791192114Z\", \"created_by\": \"/bin/sh -c #(nop) CMD [\\\"bash\\\"]\", \"empty_layer\": true } ], \"os\": \"linux\", \"rootfs\": { \"type\": \"layers\", \"diff_ids\": [ \"sha256:ce91b7d7ac5b2c288515e8eee3a83720d6855e7f1cf8dfa6e9b524453956175f\" ] } } ","date":"2021-08-26","objectID":"/posts/oci-image/:6:3","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"Image Layout 用于描述OCI内容寻址（content-addressable）blob与位置寻址（location-addressable）reference的目录结构。 content-addressable vs location-addressable 在location-addressed存储中，每个数据元素存储在特定的物理媒介中，并且它的（物理媒介）location会被记录下来以供后续访问。当想要访问到对应数据内容时，只需要在request中使用这个location即可。位置寻址不关心存储的具体内容是什么，只关心内容存储在什么位置，内容的大小多少（与盘空间占用相关），location所标识的内容可以被灵活地修改/覆盖/删除。 与之相比，content-addressed存储通过与内容相关的唯一ID来定位，通过存储系统找到对应的内容。内容一旦发生变化，这个标识ID也会发生变化，即寻址地址也会发生变化。由于这个特点，一般的content-addressed系统不允许修改原有的内容，并且删除操作也会通过严格的策略进行控制。 参考wikipedia: Content-addressed vs. location-addressed ","date":"2021-08-26","objectID":"/posts/oci-image/:7:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"结构组成 blobs目录：包含内容寻址的blob。 子目录是hash算法名称 内容索引形式为blobs/\u003calg\u003e/\u003cencoded\u003e oci-layout 文件：声明OCI image-layout的版本。 Index.json文件：image-layout中reference的入口点。 一般使用org.opencontainers.image.ref.name annotation声明引用。 组织形式与image index形式十分相似。 ","date":"2021-08-26","objectID":"/posts/oci-image/:7:1","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"示例 比较完善的示例是helm对OCI支持，chart的发布支持OCI规范，实现上依赖于本地的OCI Image Layout与OCI registry交互完成chart的发布。 helm支持OCI的交互方式 使用save cmd保存两个版本的chart。 $ helm chart save mychart-0.1.0 myregistry:5000/mychart:v0.1.0 ref: myregistry:5000/mychart:v0.1.0 digest: dfec110f2b7aecb1d8604d64f7f32026b0af51aa1286627c6520ff2cf1576337 size: 3.7 KiB name: mychart version: 0.1.0 v0.1.0: saved $ helm chart save mychart-0.2.0 myregistry:5000/mychart:v0.2.0 ref: myregistry:5000/mychart:v0.2.0 digest: fe45ba098c1f8bc61e19245c6123f47d7c51f78cec016834e2c0c26c28901e24 size: 3.7 KiB name: mychart version: 0.2.0 v0.2.0: saved local OCI image layout的结构为： $ tree ~/.cache/helm/registry/cache ├── blobs │ └── sha256 │ ├── 573f8b72a735d3f6e5919acb325d365aeddf69edee1e4840c59a5d741179da97 │ ├── 65a07b841ece031e6d0ec5eb948eacb17aa6d7294cdeb01d5348e86242951487 │ ├── 98ddb183b4658761a6e431fbbde4c6c15863b0c3597b74b519c67776830de282 │ ├── dfec110f2b7aecb1d8604d64f7f32026b0af51aa1286627c6520ff2cf1576337 │ ├── e76837ca35eb2e8f22ce8a78f14a1275511eafed58b76955b2ac7ddd0211c965 │ └── fe45ba098c1f8bc61e19245c6123f47d7c51f78cec016834e2c0c26c28901e24 ├── index.json ├── ingest └── oci-layout 查看一下index.json中的内容cat ~/.cache/helm/registry/cache/index.json | jq . { \"schemaVersion\": 2, \"manifests\": [ { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:dfec110f2b7aecb1d8604d64f7f32026b0af51aa1286627c6520ff2cf1576337\", \"size\": 322, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/mychart:v0.1.0\" } }, { \"mediaType\": \"application/vnd.oci.image.manifest.v1+json\", \"digest\": \"sha256:fe45ba098c1f8bc61e19245c6123f47d7c51f78cec016834e2c0c26c28901e24\", \"size\": 322, \"annotations\": { \"org.opencontainers.image.ref.name\": \"myregistry:5000/mychart:v0.2.0\" } } ] } 从index.json中很容易看出来通过descriptor中的digest与org.opencontainers.image.ref.name annotion将location（chart reference）与content（OCI manifest）关联了起来。 ","date":"2021-08-26","objectID":"/posts/oci-image/:7:2","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"总结与延伸 本文介绍了OCI Image Format Spec的组成，其对mediaType做了兼容，可以说是Docker Image Format Spec的一个超集。但OCI Image Format Spec要更通用些，体现在layer content可以更加多样，并且index不局限于一层。在此通用规范下可以做一些更cool的事情，比如OCI Artifacts。 此外，OCI Image Format Spec与OCI Runtime Spec及OCI Distribution Spec密切相关，比如image config如何转换成runtime bundle；image如何存储到registry。可见OCI Image Format Spec是OCI 规范中关键纽带。 后续会写一篇文章来描述一个私有化交付下的应用打包方案，从中可以看到OCI Artifacts，OCI Image Format Spec与OCI Distribution结合起来释放的强大力量。 ","date":"2021-08-26","objectID":"/posts/oci-image/:8:0","tags":["container"],"title":"OCI Image Format Spec探索与实践","uri":"/posts/oci-image/"},{"categories":["探索与实战"],"content":"OCI Distribution Spec 定义了内容分发的一组标准API。 ","date":"2021-08-26","objectID":"/posts/oci-distribution/:0:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"术语介绍 Registry：存储/发布artifact的中心，提供OCI Distribution Spec规范的api。 Blob：存储在registry中二进制形式的内容，通过digest寻址到。 Manifest：定义一个artifact的json文档，用于定位artifact的具体内容。 Digest: 对blob内容加密后的唯一标识符。 Artifact：以blob形式存储的概念化的内容，一般是由一个manifest定址若干blobs构成。 Tag：用于便于人类阅读、定位manifest的引用。 ","date":"2021-08-26","objectID":"/posts/oci-distribution/:1:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"Registry基本要求 Pull，从registry中拉取conent。 Push，向registry中发布content。 Content Discovery，从registry中获取content列表项。 Content Managerment，控制registry中content的完整生命周期。 ","date":"2021-08-26","objectID":"/posts/oci-distribution/:2:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"标准API汇总 Method API Path Status code 用途 GET /v2/ 200/400/401 用于判断registry是否实现OCI Distribution Spec HEAD /v2/\u003cname\u003e/blobs/\u003cdigest\u003e 200/404 用于判断指定的blob是否存在 GET /v2/\u003cname\u003e/blobs/\u003cdigest\u003e 200/404 用于获取指定的blob HEAD /v2/\u003cname\u003e/manifests/\u003creference\u003e 200/404 用于判断指定的manifest是否存在 GET /v2/\u003cname\u003e/manifests/\u003creference\u003e 200/404 用于获取指定的blob POST /v2/\u003cname\u003e/blobs/uploads/ 202/404 获取上传blob的sessionID，为后续PUT/PATCH操作提供locator POST /v2/\u003cname\u003e/blobs/uploads/?digest=\u003cdigest\u003e 201/202/404/400 直接通过POST上传blob，optional PATCH /v2/\u003cname\u003e/blobs/uploads/\u003creference\u003e 202/404/416 分片上传blob chunks PUT /v2/\u003cname\u003e/blobs/uploads/\u003creference\u003e?digest=\u003cdigest\u003e 201/404/400 上传blob。reference为之前POST请求获取的id PUT /v2/manifests/ 201/404 上传一个manifest GET /v2/\u003cname\u003e/tags/list?n=\u003cinteger\u003e\u0026last=\u003cinteger\u003e 200/404 获取某个repository下的所有tag，可以通过list，last query进行分页 DELETE /v2/\u003cname\u003e/manifests/\u003creference\u003e 202/404/400/405 删除某个manifest DELETE /v2/\u003cname\u003e/blobs/\u003cdigest\u003e 202/404/405 删除某个blob POST /v2/\u003cname\u003e/blobs/uploads/?mount=\u003cdigest\u003e\u0026from=\u003cother_name\u003e 201/202/404 如果某个blob在其他repository上存在，此API可以将blob挂载到同一registry下的不同repository reference与digest区别 digest指的是content的唯一标识，使用digest可以标识anything，因此digest可以用作reference。但使用digest的问题是不容易记忆及索引，从而引入tag来便于记忆，因此tag是通常意义上的reference。需要注意的是一般只有manifest可以使用reference寻址，而blob仅支持digest寻址。原因是对于经典分层结构，blob对外不会单独使用，需要通过manifest定位到。即面向用户的manifest才需要用户可理解的reference(location)去寻址，底层存储blob通过manifest记录的digest(content)定位即可。 有关于content discovery的扩展 OCI Distribution Spec中关于content discovery仅定义了获取某个repository的tags api，但仍有获取到某个registry的repositories的需求，目前大部分registry都实现了此api及其分页形式：GET /v2/_catalog?n=\u003cinteger\u003e\u0026last=\u003cstring\u003e ","date":"2021-08-26","objectID":"/posts/oci-distribution/:3:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"探索与实践 下面以ubuntu:21.04发布为例探索一下上述api的使用。 先看一下ubuntu:21.04的manifest，其中config与layers中的每个descriptor均为一个blob。 { \"schemaVersion\": 2, \"mediaType\": \"application/vnd.docker.distribution.manifest.v2+json\", \"config\": { \"mediaType\": \"application/vnd.docker.container.image.v1+json\", \"size\": 1462, \"digest\": \"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" }, \"layers\": [ { \"mediaType\": \"application/vnd.docker.image.rootfs.diff.tar.gzip\", \"size\": 31837572, \"digest\": \"sha256:4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1\" } ] } ubuntu manifest自身的digest为：ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0 ","date":"2021-08-26","objectID":"/posts/oci-distribution/:4:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"Push流程 参考oras封装了一下containerd的remotes.PushContent方法将myregistry:5000/ubuntu:21.04 push到registry。将log-level设置成debug模式，我们可以看到containerd中相关api调用的track如下（仅以layer blob为例，省略掉config blob的日志信息）。 Step1：Client端发起一个HEAD blob请求，server端返回404表明当前blob不存在。 DEBU[0000] do request digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json request.header.accept=\"application/vnd.docker.container.image.v1+json, */*\" request.header.user-agent=containerd/1.5.2+unknown request.method=HEAD size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" DEBU[0000] fetch response received digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json response.header.content-length=157 response.header.content-type=application/json response.header.date=\"Sat, 21 Aug 2021 04:48:26 GMT\" response.header.docker-distribution-api-version=registry/2.0 response.header.x-content-type-options=nosniff response.status=\"404 Not Found\" size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" Step2：Client端调用POST blob获取upload session id，server端返回一个locator。 DEBU[0000] do request digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json request.header.user-agent=containerd/1.5.2+unknown request.method=POST size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/uploads/\" DEBU[0000] fetch response received digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json response.header.content-length=0 response.header.date=\"Sat, 21 Aug 2021 04:48:26 GMT\" response.header.docker-distribution-api-version=registry/2.0 response.header.docker-upload-uuid=10729918-5b28-4a0b-b793-319d048a3b66 response.header.location=\"http://myregistry:5000/v2/ubuntu/blobs/uploads/10729918-5b28-4a0b-b793-319d048a3b66?_state=vQM2ysmno9_Yvwoz6NK0qgEKVG1aCm3GqKFHYoQEX6F7Ik5hbWUiOiJ1YnVudHUiLCJVVUlEIjoiMTA3Mjk5MTgtNWIyOC00YTBiLWI3OTMtMzE5ZDA0OGEzYjY2IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTA4LTIxVDA0OjQ4OjI2Ljc3MTY5MloifQ%3D%3D\" response.header.range=0-0 response.header.x-content-type-options=nosniff response.status=\"202 Accepted\" size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/uploads/\" Step3：Client端调用PUT blob上传blob，server端返回上传成功。 DEBU[0000] do request digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json request.header.content-type=application/octet-stream request.header.user-agent=containerd/1.5.2+unknown request.method=PUT size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/uploads/10729918-5b28-4a0b-b793-319d048a3b66?_state=vQM2ysmno9_Yvwoz6NK0qgEKVG1aCm3GqKFHYoQEX6F7Ik5hbWUiOiJ1YnVudHUiLCJVVUlEIjoiMTA3Mjk5MTgtNWIyOC00YTBiLWI3OTMtMzE5ZDA0OGEzYjY2IiwiT2Zmc2V0IjowLCJTdGFydGVkQXQiOiIyMDIxLTA4LTIxVDA0OjQ4OjI2Ljc3MTY5MloifQ%3D%3D\u0026digest=sha256%3Abf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" DEBU[0000] fetch response received digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json response.header.content-length=0 response.header.date=\"Sat, 21 Aug 2021 04:48:26 GMT\" response.header.docker-content-digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" response.header.docker-distribution-api-version=registry/2.0 response.header.location=\"http://myregistry:5000/v2/ubuntu/blobs/sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" response.header.x-content-type-options=nosniff response.status=\"201 Created\" size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/uploads/10729918-5b28-4a0b-b793-319d048a3b66?_state=vQM2ysmno9_Yvwoz6NK0qgEKVG1aCm3GqKFHY","date":"2021-08-26","objectID":"/posts/oci-distribution/:4:1","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"Pull流程 封装了containerd remotes.FetchHandler与images.ChildrenHandler，以将myregistry:5000/ubuntu:21.04 pull到本地为例展示一下api调用track。 Step1：Client端HEAD一下ubuntu:21.04 manifest，server端返回相关的digest。 DEBU[0000] do request host=\"myregistry:5000\" request.header.accept=\"application/vnd.docker.distribution.manifest.v2+json, application/vnd.docker.distribution.manifest.list.v2+json, application/vnd.oci.image.manifest.v1+json, application/vnd.oci.image.index.v1+json, */*\" request.header.user-agent=containerd/1.5.2+unknown request.method=HEAD url=\"http://myregistry:5000/v2/ubuntu/manifests/21.04\" DEBU[0000] fetch response received host=\"myregistry:5000\" response.header.content-length=529 response.header.content-type=application/vnd.docker.distribution.manifest.v2+json response.header.date=\"Sat, 21 Aug 2021 06:57:20 GMT\" response.header.docker-content-digest=\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" response.header.docker-distribution-api-version=registry/2.0 response.header.etag=\"\\\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\\\"\" response.header.x-content-type-options=nosniff response.status=\"200 OK\" url=\"http://myregistry:5000/v2/ubuntu/manifests/21.04\" Step2：Client端获取到ubuntu:21.04的manifest，server端返回manifest的内容、 DEBU[0000] do request digest=\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" mediatype=application/vnd.docker.distribution.manifest.v2+json request.header.accept=\"application/vnd.docker.distribution.manifest.v2+json, */*\" request.header.user-agent=containerd/1.5.2+unknown request.method=GET size=529 url=\"http://myregistry:5000/v2/ubuntu/manifests/sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" DEBU[0000] fetch response received digest=\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" mediatype=application/vnd.docker.distribution.manifest.v2+json response.header.content-length=529 response.header.content-type=application/vnd.docker.distribution.manifest.v2+json response.header.date=\"Sat, 21 Aug 2021 06:57:20 GMT\" response.header.docker-content-digest=\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" response.header.docker-distribution-api-version=registry/2.0 response.header.etag=\"\\\"sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\\\"\" response.header.x-content-type-options=nosniff response.status=\"200 OK\" size=529 url=\"http://myregistry:5000/v2/ubuntu/manifests/sha256:ef8ee90cfa9cfc7c218586dea9daa6a8d1d191b3c73be143f4120fe140dae3d0\" Step3：Client端从manifest解析出blobs，并发送GET blob请求，server端将blob的内容返回。 ------Requests for 2 blobs------ DEBU[0000] do request digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json request.header.accept=\"application/vnd.docker.container.image.v1+json, */*\" request.header.user-agent=containerd/1.5.2+unknown request.method=GET size=1462 url=\"http://myregistry:5000/v2/ubuntu/blobs/sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" DEBU[0000] do request digest=\"sha256:4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1\" mediatype=application/vnd.docker.image.rootfs.diff.tar.gzip request.header.accept=\"application/vnd.docker.image.rootfs.diff.tar.gzip, */*\" request.header.user-agent=containerd/1.5.2+unknown request.method=GET size=31837572 url=\"http://myregistry:5000/v2/ubuntu/blobs/sha256:4451f5c7eb7af74432585f5ebfbeb01bbfc87ec4a74dc93703bdd89330559cd1\" ------Responses for 2 blobs------ DEBU[0000] fetch response received digest=\"sha256:bf70ebd2c444440ae068c5ccea80e2087906a825ff1019a9f6d6cbb229e33481\" mediatype=application/vnd.docker.container.image.v1+json response.header.accept-ranges=bytes response.header.cache-control=\"max-age=31536000\" response.header.content-length=1462 response.header.content-type=application/octet-stream response.header.date=\"Sat, 21 Aug 2021 06:57:20 GMT\" response.header.docker-content-digest=\"sha256:bf70ebd2c","date":"2021-08-26","objectID":"/posts/oci-distribution/:4:2","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["探索与实战"],"content":"参考文献 Oras Containerd remotes Containerd images OCI Distribution Spec ","date":"2021-08-26","objectID":"/posts/oci-distribution/:5:0","tags":["container"],"title":"OCI Distribution Spec探索与实践","uri":"/posts/oci-distribution/"},{"categories":["整理与总结"],"content":" The Open Container Initiative is an open governance structure for the express purpose of creating open industry standards around container formats and runtimes. ","date":"2021-08-26","objectID":"/posts/oci/:0:0","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"内容概览 OCI定义了如下几个标准： ","date":"2021-08-26","objectID":"/posts/oci/:1:0","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"OCI Runtime-Spec 定义容器配置，运行环境及生命周期规范。 ","date":"2021-08-26","objectID":"/posts/oci/:1:1","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"OCI Image-Spec 定义image格式规范。 manifest filesystem layers image index ","date":"2021-08-26","objectID":"/posts/oci/:1:2","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"OCI Distribution-Spec 定义内容分发的一组标准API。 ","date":"2021-08-26","objectID":"/posts/oci/:1:3","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"OCI Artifacts OCI Image-Spec的通用扩展，将普通artifact按照image形式发布到distribution上。 ","date":"2021-08-26","objectID":"/posts/oci/:1:4","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"对标准的认识 标准是什么？字面上的意思是指衡量、区别事物的准则。遵循某一标准的事物可以达到某种程度上的统一。 最初对标准的认识就是两个字：抽象。似乎人们天生对于抽象的东西的初印象都是十分抗拒的，而具体的东西更有助于人们去理解新事物。随着具有相同功能的产品越来越多，人们对这些产品的选择也越来越多，但是当我们从一个产品的使用切换到另一个产品的使用上却犯了难：完成同样的事，操作或行为却大相径庭。而标准正是为了统一这类产品而生，即我们只需要了解相应的标准即可完成想要的功能。完成此功能的产品可以很多，但他们需要满足这个标准，而一旦他们满足了这个标准，我们就可以方便地在这些产品中进行切换。 上面的描述还是过于抽象，举个生活中最简单的例子：数据线接口。如果没有标准，每个设备的接口的样式可以五花八门，试想一下某款鼠标为了适配几千种笔记本的接口形状，也需要提供成千上万种的数据线，这简直就是灾难。而通过制定一组标准接口(VGA, HDMI, USB, TypeC)，所有厂商制造笔记本时受限于这些标准接口，这样外设便可以做到很通用，即便存在为数不多的几种不同标准，一个绿联就可以轻松搞定。 总之标准就是为了提升通用性，就好比多态一样，实现可以多种多样，但都实现统一的接口，而这接口便是完成某个功能的最短路径。 在工作中受益于标准化的案例就是OpenTracing，这是一套分布式追中的标准规范，定义了一组接口规范。在开发过程中，无需陷于产品的选型，直接使用这组规范接口，而只要是实现了此规范的产品（jaeger, skywalking, zipkin等）都可以在项目中轻松切换。 有了OCI Spec，我们可以使用contained来替换docker，也可以将镜像仓库从docker registry迁移到harbor。OCI让容器化变得更通用。 ","date":"2021-08-26","objectID":"/posts/oci/:2:0","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"系列文章 OCI Distribution Spec探索与实践 OCI Image Format Spec探索与实践 私有化交付下的应用打包方案 ","date":"2021-08-26","objectID":"/posts/oci/:3:0","tags":["container"],"title":"OCI 标准探究","uri":"/posts/oci/"},{"categories":["整理与总结"],"content":"加密，保障信息内容安全；认证，确保信息的来源可靠。 互联网的出现拉近了整个世界的距离，它的魔力就在于可以将芸芸众生都收罗在其控制范围内。正所谓有人的地方就一定有恶，网络面临着各种各样的威胁。而网络由于其巨大的影响力，一旦遭受破坏波及范围难以估量，因此网络安全便显得尤为重要。 ","date":"2021-07-22","objectID":"/posts/certification/:0:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"网络面临的威胁 计算机网络是不可靠的，一来网络的稳定性受制于网络提供商及对应的硬件设备；二来就是来自人为的恶意干预。前者往往是难以预测和防范的小概率事件，而后者通过精妙的网络安全技术几乎可以完全规避掉。计算机网络所面临的恶意威胁主要分为被动攻击与主动攻击两大类： ","date":"2021-07-22","objectID":"/posts/certification/:1:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"被动攻击 攻击者观察和分析数据单元而不干扰正常的网络通信。 截获：攻击者从网上窃听他人的通信内容。 ","date":"2021-07-22","objectID":"/posts/certification/:1:1","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"主动攻击 攻击者对某个连接中的数据协议单元进行处理，采取行动扰乱原本的网络通信。 中断：攻击者有意中断他人在网络上的通信，比如DDos。 篡改：攻击者故意篡改网络上传送的报文。 伪造：攻击者伪造信息在网络上传送。 这些攻击大多数并不像网络断线或者波动那样能让我们有所感知，而是在无形中破坏/泄露我们的通信。面临这些威胁，我们必须采取措施保证网络安全。 ","date":"2021-07-22","objectID":"/posts/certification/:1:2","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"加密 从上面的威胁类型来看，当务之急是如何保证我们的消息不被他人获知？百分百的不泄露信息要么是不产生任何信息，要么是与世隔绝，所以根本无法做出这个保证。但当我们换个思路：如何保证我们有用的消息不被他人获知？转机便出现了：加密技术保护我们有用的信息不被他人获取。 ","date":"2021-07-22","objectID":"/posts/certification/:2:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"无加密时代 在那个计算机网络还没有出现，人类还比较纯粹的时代，语言和文字成为了人们通信的主要手段，消息的传递也都是基于明文。这时，我们对于加密的需求不强，因为本身消息传播途径比较少，如果真的需要传递一些秘密，只要通过人来实现就可以。随着时代的进步，消息安全的重要性额外地凸显出来。最显著的场景是在战争年代的战略消息的传递，上面提到的各种类型的威胁在这个时代都有所体现。 截获：投信的使臣投宿客栈时，信报被细作偷看，但没有改变信报上的内容。 中断：投信的使臣被细作射杀了。 篡改：投信的使臣叛变了，改了信报的内容。 伪造：有人假扮投信的使臣传递信报。 通过在信报上做手脚，往往可以不折一兵一卒损敌方千军万马，因此消息的保密在人类文明伊始就意义重大。 ","date":"2021-07-22","objectID":"/posts/certification/:2:1","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"对称加密 最早的加密技术应当算是凯撒加密了，其核心思想就是简单的替换：将明文中所有字母在字母表上向前或向后偏移固定数目后形成了密文。对于外人而言，其看到的密文是一堆不相关的字符串，无法破解其要表达的信息。而对于消息接收者而言，他清楚加密的策略，也清楚对应解密方法。因此可以通过对应的逆偏移解码出明文，获取有用的消息。 这就是对称加密的雏形，即加密解密双方有一个公共的规则（相同的密钥），发送发方通过此规则（密钥）进行编码（加密），接收方通过此规则（密钥）进行解码（解密），信息在传输过程中是以明文经加密后产生的密文传递。 消息在网络中进行传递就要求加密算法是需要公开的，因为我们无法在通信的最开始私下里统一加密方案，也不会绞尽脑汁去创造新的加密算法。因为我们的通信对象太多了，也都远在天边，即便我们可以创造各式各样的加密算法，也无法安全地与对方分享我们的算法。所以现代的加密算法是公开的、标准化的，我们通信的时候只需要声明我们使用的算法即可，对加密的安全性保证仅在于我们为加密算法提供的特殊标识（密钥）。比如凯撒算法是公开的，但偏移量作为特殊标识是通信双方约定好的。也正是因为此标识的多样性，才能保证我们的消息不易被破解。 对称加密的好处就是简单，计算速度快，开销小。而问题也显而易见，我们的密钥需要通信双方共享，但是密钥没有办法安全地传递。一旦密钥泄露出去，密文的传递跟明文的传递别无二致。 破解难易度 对于凯撒算法而言，密钥很容易通过穷举法破解出来，毕竟字母偏移量只有25种可能，因此极易被破解。但成熟的加密算法要求密钥足够长，当然这并不意味着密钥不会被破解，只是说破解难度十分巨大，可以认为是在计算上（而不是在理论上）不可破解，或者更直白的说破解密钥的成本要比消息本身的价值高很多。这侧面说明了在对称加密中，与密钥泄露相比，密钥被破解的可能性可以完全忽略不计。 目前常用的对称加密算法是DES，3DES，AES等。 下图是一个简单的DES示例 对称加密算法示意 ","date":"2021-07-22","objectID":"/posts/certification/:2:2","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"非对称加密 对称加密致命缺陷就是无法保证密钥安全传递，因此基于公钥密码体制的非对称加密走上了舞台。顾名思义，非对称加密指的是加密密钥与解密密钥是不同的。其中加密密钥又称公钥，是向公众公开的；而解密密钥又称私钥，是对外保密的。由于公钥本身是公开的，因此无需保证其安全传递。即便被第三方截获，其也无法破解加密后的密文，因为只有对应的私钥才能将密文解密。在公钥密码体制中，最著名的是基于大数分解问题的RSA体制。 加密与解密 经常有个误区会认为公钥是用来加密的，私钥是用来解密的，但实际上二者并无相关性。公钥与私钥是通过某种算法算出的一对密码串，使用一方进行编码，便可以通过另一方进行解码。唯一的原则是公钥是公开的，私钥是保密的，我们会为不同用途选择不同的角色进行编解码。比如信息加密我们会采用公钥编码，私钥解码；数字签名会采用私钥编码，公钥解码。 那么有了安全性如此之高的非对称加密，对称加密是不是就可以退出历史舞台了呢？并不是的，因为非对称加密有个致命的确定：开销大，性能差，而且开销是与明文的长度正相关的。在海量数据通信的时代，这显然会降低信息传递效率，因此TLS的最佳实践是通过非对称加密传递对称加密密钥，具体的信息加密仍然采用对称加密。 非对称加密的另一个应用场景是数字签名，数字签名是通过私钥加密，并通过公钥解密。其作用类似于写在纸上的签名，申明其所传递的内容无误。签名只能有发信者完成，而所有人均可通过其公钥对其进行验签。有了数字签名后，其他人无法篡改或伪造信息的内容，因为他们没有也无法破解发信者的私钥，所以无法对篡改后/伪造的内容进行签名。 原始信息的大小直接影响到签名的性能，通常我们需要对原始信息进行hash获取摘要，并对摘要进行签名。这要求hash算法是不可逆转的，即无法根据摘要推算出私钥。同时，好的hash算法能保证不同的信息输出的结果也都不相同，即在计算上同一结果只能对应一种原始信息。目前比较流行的hash算法是md5，其将原始信息散列成固定128位的结果。 数字签名示意 有了非对称加密，信息的传递就绝对安全了吗？也不是，上述过程中最大的问题仍旧是…公钥的传递。不对呀，之前不是说了公钥是公开的吗，为什么公钥的传递还会引发安全性呢？问题的本质不在于传递公钥内容的安全性，而在于公钥的所有者是谁。换句话说，小明要和小丽通信，小明如何知道传递过来的公钥是小丽的呢？如果小丽传递过来的公钥被小三拦截，并且将小三的公钥传递给了小明，那小明原本要发给小丽的情话不就被小三拦截并破译了？小三便可以窃听或者篡改小明与小丽的通信。这就是所谓的中间人攻击。 中间人攻击示例 那么如何避免中间人攻击呢？这就需要引出第三方认证了，即通过权威机构证明这个公钥是小丽的。 ","date":"2021-07-22","objectID":"/posts/certification/:2:3","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"认证 认证通俗来讲，就是通过一个第三方来验明某个人的身份，从而确认公钥就是这个人的，以避免中间人攻击。在流程上，一般是一方带着自己的身份信息向第三方机构去申请一个证书，第三方机构颁发一个带有自己签名的证书，该方将证书发送给对端进行验证。我们着重介绍一下数字证书和颁发机构这两个概念。 ","date":"2021-07-22","objectID":"/posts/certification/:3:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"数字证书 数字证书指在互联网通讯中标识通讯各方身份信息的一个数字认证，人们可以在网上用它来识别对方的身份。一般而言，证书需要包含如下内容： 持有者身份信息 证书颁发者信息 持有者的公钥 认证机构的数字签名 认证机构验证持有者的身份（CSR）无误后，便向其发布一个带其签名的证书，以此来标识此证书中的公钥是可信的。 在TLS中，通常使用x509证书，下例演示了minikube为kube-apiserver生成的服务端证书，我们根据这个证书看一下证书的机构。 Certificate: Data: Version: 3 (0x2) # x509证书版本号 Serial Number: 2 (0x2) # 序列号，每个证书唯一，用以与其他证书区分开 Signature Algorithm: sha256WithRSAEncryption # 签名使用的算法 Issuer: CN=minikubeCA # 颁发机构名称 Validity # 证书有效期（不早于...不晚于...） Not Before: Jul 9 02:57:51 2021 GMT Not After : Jul 10 02:57:51 2022 GMT Subject: O=system:masters, CN=minikube # 持有者的名字 Subject Public Key Info: # 持有者的公钥信息 Public Key Algorithm: rsaEncryption Public-Key: (2048 bit) Modulus: 00:b6:e3:24:20:ce:84:11:21:e5:a4:ee:fd:11:75: 51:94:1e:bb:c6:18:3c:fa:e3:1f:16:d2:18:4d:98: e2:44:30:b4:aa:c4:7a:de:8a:f4:ec:c5:e9:a6:0b: ac:12:b2:03:e9:ca:16:f4:69:f7:08:bb:1c:0d:af: 58:9a:51:39:30:cf:bc:0f:60:97:a9:a4:0a:f8:37: 65:dd:b9:21:1f:da:0b:79:19:1d:6b:96:ff:98:57: 36:3b:a3:e3:34:98:0d:70:92:5f:6f:0f:e8:a2:3b: 88:d5:4c:3d:fb:e0:75:48:af:5e:e5:16:a8:50:d6: 92:ef:be:d5:23:91:e1:c9:d5:9c:e6:ff:49:e7:ca: fb:ab:c7:72:0d:90:23:dc:0a:59:cf:8f:e4:f6:6a: fe:73:c0:8d:bb:23:8c:a8:0f:e7:90:61:e1:66:b1: ac:4d:96:23:c8:0b:43:25:a2:8a:4f:d5:ff:2d:57: 8c:fb:16:d2:69:a0:80:d1:f9:f2:e1:a3:0a:40:7a: 56:1b:3c:69:36:73:dd:b5:f6:9e:75:d4:91:5c:58: b4:e8:f7:a2:2f:1e:0c:4b:2d:d6:1e:6d:92:6f:d3: 07:85:a6:9f:7c:20:0c:7c:e8:3c:0f:6b:c8:74:a4: 8b:c6:d3:10:af:a7:dd:dc:90:ae:bc:f6:b7:b0:e1: 1b:bb Exponent: 65537 (0x10001) X509v3 extensions: # 一堆x509扩展，后面会挑一些重要的说明 X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS:kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:10.122.101.148, IP Address:172.17.0.40, IP Address:10.96.0.1, IP Address:127.0.0.1, IP Address:10.0.0.1 Signature Algorithm: sha256WithRSAEncryption # 签名信息 97:e6:66:a9:a4:fb:42:93:59:c2:2e:ea:77:9a:bf:46:99:11: 72:f6:e1:f7:ce:22:9b:35:7d:37:03:d6:ea:8d:c0:9a:9e:0b: 5c:70:93:e5:5c:e2:c8:43:df:e6:ee:ff:ba:90:8d:de:1f:e5: 4e:ee:72:a7:bc:9e:7a:a9:7f:58:2e:7e:6f:aa:91:60:02:c5: 51:71:1a:e0:80:8b:3d:08:34:a9:65:47:7a:89:7d:31:6d:1a: 1f:0f:0b:17:88:48:eb:e5:20:94:56:52:9a:e0:30:88:fb:cc: 62:12:ea:86:45:e2:a8:59:cd:d0:b1:d6:7b:a9:37:fa:24:b6: 34:d4:4d:aa:cd:28:2c:e2:57:90:6a:11:4f:c0:01:68:49:ef: bd:54:68:7f:f8:2e:99:ba:d0:b1:01:c2:eb:7e:5a:ff:f9:dd: 6c:1b:43:b4:3b:10:fc:46:ef:5b:63:05:27:12:74:6d:2c:ec: af:d6:66:2a:eb:45:ee:d9:d9:bd:73:2d:38:bd:bc:b5:38:7c: 90:9b:f0:11:ee:0a:93:1d:63:25:89:e6:c3:59:3c:7b:4a:4d: 53:d3:a1:a6:3e:82:64:f1:04:02:2c:ce:b9:68:5b:fc:1d:40: c8:8f:9a:ce:04:9d:5a:c9:6c:94:8e:43:f5:e4:78:00:82:bb: f4:5c:5b:d7 x509证书定义了一系列扩展，用于对证书进行约束及限制，常用的扩展列举如下： 扩展名 用途 是否为critical 示例 X509v3 Basic Constraints 描述该证书是否属于CA，具有签署证书的能力 Y CA=false：此证书不是CA证书，不可签署其他证书 X509v3 Key Usage 限定了证书的用途 Y Digital Signature, Key Encipherment表示可以用于数字签名及密钥加密 X509v3 Extended Key Usage 限定了证书的用途 N TLS Web Server Authentication, TLS Web Client Authentication表示可以用于服务端及客户端认证 X509v3 CRL Distribution Points 提供了CRL地址列表 F URI:http://crl.starfieldtech.com/sfs3-20.crl Authority Information Access 提供OSCP地址；CA签发者的连接(用于获取中间证书) F OCSP - URI:http://ocsp.int-x3.letsencrypt.org CA Issuers - URI:http://cert.int-x3.letsencrypt.org/ X509v3 Subject Key Identifier 持有者的唯一标识 F Authority Key Identifier 颁发者的标识 F 与颁发者的X509v3 Subject Key Identifier相等 X509v3 Subject Alternative Name（SAN） 列出证书所有的合法域名，如果其不存在就回退到Common Name寻找 F DNS:localhost, IP Address:10.122.101.148 表示证书来源的dns为localhost或者Ip为10.122.100.148的是合法的，其他均不可信 结合以上扩展可以列举出证书验证的失败的几个原因： 证书过期了 证书由不可信CA颁布 证书被吊销了 证书来源不在其SAN中 critical作用 有些扩展带有critical标识，其意义为所有的证书验证方必须理解它的含义，否则认定为认证失败。简单理解就是这些扩展要求必须能够被识别。 ","date":"2021-07-22","objectID":"/posts/certification/:3:1","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"颁发机构（CA） 为避免中间人攻击，证书必须由第三方权威机构认证才可信，这里的权威机构就是CA。上面描述的数字证书就是申请者发送证书签署请求（CSR）给CA，由CA认证后用其私钥进行签名形成证书。可见CA的信誉直接影响着证书的可信度。 那么CA又是怎么保证其是可信的呢？答案是通过更权威的CA进行认证，那么这个更权威的CA怎么保证是可信的呢？那就要找更更权威的CA认证……现实情况下，这种情况对应着证书链认证，即认证是一级一级向上传递的。当然为了避免证书无休止的验证下去，是需要有一个最终信任源的，这个信任源又称为根证书。 按下葫芦浮起瓢，CA的认证大致明确了，但是它又引入了两个问题： 根证书是如何终止链式验证过程的？如何成为信任源的？如果私钥泄露怎么办？如果过期怎么办？ 为什么需要证书链，直接使用根证书进行用户证书的认证会有什么问题？ 上述两个问题并非是割裂的，而是息息相关的。 根证书作为可信任的源头，其使用自己的私钥来签署自己的证书（这类证书又叫做自签名证书，其特点是Issuer与Subject信息一致），验证方只要持有该根证书的公钥便可对此根证书进行验证。而这些具有权威性的CA根证书会内置在浏览器及操作系统中，这样便可以对此类根证书进行验证。如果我们想信任某个第三方证书，那么只需要将其加入到浏览器/操作系统的证书信任列表中即可。 根证书的有效期一般会比较长，如果根证书过期了，那么只能由该机构重新签发。而在临期时，便会借着操作系统/浏览器的更新而将新证书植入进去。另一方面，现在很多证书都采用了交叉认证，即使某个根证书过期了，也能确保其他链条上的认证成功。如果根证书的私钥泄露了，那么它就可以被用于发布非法证书，作为互联网的信任根源之一，带来的影响会是灾难性的。因此为了避免根证书私钥的安全，需要做到以下保证： 私钥在计算上须是不可破解的，因为根证书有效期较长，如果私钥长度过短的话很有可能在有效期内被破解，因此需要是在计算上不可破解的。当前的x509证书可以保证这一点。 用户证书不直接通过根证书签名颁发，而是通过中间证书颁发，建立起中间证书链进行认证。 我们着重来说一下第二点，试想一下如果每个证书都需要根证书颁发的话，会不会增大私钥泄露的风险呢？从统计学角度看，如果签署的证书过多，其规律性信息量也会暴露越多，无论是泄露还是被破解的概率都会增大。因此通过追加一层中间证书来解决此问题，即中间证书颁发用户证书，根证书颁发中间证书。这样便构成了完整的证书链体系，从而减小根证书密钥泄露的风险。通常来讲，为了严格控制根证书私钥的安全性，其所在的机器通常是离线的，只有在需要签署/吊销中间证书时才会联网，将暴露的风险降到最低。 中间证书的特点 中间证书不应是自签名的，即Issuer与Subject是不同的，其携带上一级证书的DN (Distinguished Name)，可以通过DN来找到上一级证书。此外中间证书是可以签发下级证书的，即X509v3 Basic Constraints扩展字段需设置为CA=true。最后中间证书的有效期一般比根证书的短很多，如果中间证书因私钥泄露等问题需要被吊销（可以通过CRL或者OCSP来查询证书是否被吊销）时，影响范围会小很多。 证书链示意图 总结一下链式证书发布及验证过程： 发布方 权威CA生成自签名根证书，并将根证书置于操作系统/浏览器中 权威CA使用根证书私钥签名中间证书 CA使用中间证书采用其私钥签名用户证书 用户使用其私钥签名数据摘要 验证方 采用中间证书验证用户证书 如果有多层中间证书，则使用上一级中间证书验证当前中间证书 通过内置的根证书验证中间证书 通过用户证书中的公钥对数据摘要进行验签 最后一个问题是在对用户证书进行验证时，如何获取中间证书呢？在SSL握手的过程中，是允许在将客户端/服务器证书发送后紧着着将中间证书发送出去的。但这并不是必须的，还有一些其他的方式获取中间证书，比如上述提到过的Authority Information Access扩展中可能会有颁发者的获取地址等。 ","date":"2021-07-22","objectID":"/posts/certification/:3:2","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"总结 本文介绍了网络安全面临哪些威胁，使用数据加密技术可以做到有效规避部分威胁。接着详细介绍了对称加密及非对称加密的特点。单方面的加密技术无法抵御中间人攻击，因此需要第三方认证以验明通信双方的身份。最后介绍了认证中的一些重要概念，如数字签名、数字证书、颁发机构等，并将这些概念在认证的流程中发挥的作用串联起来。此外，文章穿插着对TLS的认证及加密流程的介绍。 网络安全远不止于此，加密与认证仅仅是保证了最基本的通信安全，而网络安全还涵盖很多的方面，诸如人为因素，准入机制，角色权限控制等。对加密/认证的概念有了初步的认识，后续会逐步深入地研究kubernetes中的认证、鉴权、准入的安全管理机制。 ","date":"2021-07-22","objectID":"/posts/certification/:4:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["整理与总结"],"content":"参考文献 数字证书原理 openssl x509证书 Wikipedia openssl cookbook ","date":"2021-07-22","objectID":"/posts/certification/:5:0","tags":["tls","网络"],"title":"网络安全基础","uri":"/posts/certification/"},{"categories":["探索与实战"],"content":"诸多巧合凑在一起就是完美的偏差 ","date":"2021-07-11","objectID":"/posts/minikube-tls/:0:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"缘起 对TLS的认知最初应该在学习计算机网络中的https协议，该协议通过TLS层对传输数据进行加密解密。为了防止中间人攻击，需要第三方证书机构进行认证，认证的方式是通过数字签名。基本上这几点就涵盖了所有的考纲内容。然而当我在学习或者在工作中真的遇到TLS引起的认证问题时，我发现我所理解的十分笼统，无法给我提供任何有价值的排查思路，因此准备稍微深挖一下TLS的认证机制。 可是载体为什么是minikube？最近在跟随大佬熟悉一些operator相关的机制，在开始前搭了一套minikube环境，想着能有更好的开发体验，打算在本地去连服务器的minikube集群，后面有机会再去研究一下telepresence以便在本地调试。连接服务器的k8s集群相关的文档也有很多，核心的解决方案就是将minikube集群中的kubeconfig及对应的证书文件拷贝到本机。顺着这个思路一顿操作猛如虎，结果在使用kubectl获取资源时，出现了x509证书认证错误。 kubectl get pods Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of “crypto/rsa: verification error” while trying to verify candidate authority certificate “minikubeCA”) 在google没有找到合适的诱因及解决方案，因此打算自己探秘一番。有关TLS认证，在这篇文章有所提及，本文主要记录k8s的TLS双向认证的过程。 ","date":"2021-07-11","objectID":"/posts/minikube-tls/:1:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"初识 k8s在权限管理上做的格外细致，尤其是在k8s api的访问上，更是细分为认证、鉴权和准入等阶段。我们这里只对tls双向认证做粗略的描述。 什么是双向认证呢？简单来说就是服务器需要验明客户端的身份，同时客户端也需要验明服务器的身份。k8s的认证载体为x509证书。在minikube集群中有一个统一的CA（默认路径为.minikube/ca.crt）来对证书进行验签。 双向认证发生在任何两个进行通信的组件中，下面列举一些例子： etcd集群内peer间通信是双向的，etcd peer既充当客户端，又充当服务端。因此既需要持有客户端证书，又需要持有服务端证书进行认证。 kube-apiserver与etcd之间的通信是单向的，kube-apiserver充当客户端，etcd充当服务端。因此kube-apiserver需持有客户端证书，etcd需持有服务端证书。 kubectl与apiserver的通信是单向的，kubectl充当客户端，etcd充当服务端，kubectl需要持有客户端证书，apiserver需要持有服务端证书。 …… 每个控制面组件在启动时会指定所使用到的证书，下例列举出apiserver的启动参数 # 获取kube-apiserver 相关配置 $ kubectl get pods/kube-apiserver-minikube -o yaml --namespace kube-system spec: containers: - command: - kube-apiserver - --advertise-address=172.17.0.40 - ...... # 用于验证访问apiserver客户端证书的CA根证书 - --client-ca-file=/var/lib/minikube/certs/ca.crt - .... # 与客户端通信的服务端证书即私钥 - --tls-cert-file=/var/lib/minikube/certs/apiserver.crt - --tls-private-key-file=/var/lib/minikube/certs/apiserver.key # 用于验证服务器证书的CA根证书 - --etcd-cafile=/var/lib/minikube/certs/etcd/ca.crt # 用于访问etcd的客户端证书 - --etcd-certfile=/var/lib/minikube/certs/apiserver-etcd-client.crt # 与etcd通信使用的私钥 - --etcd-keyfile=/var/lib/minikube/certs/apiserver-etcd-client.key # kubelet相关证书及私钥 - --kubelet-client-certificate=/var/lib/minikube/certs/apiserver-kubelet-client.crt - --kubelet-client-key=/var/lib/minikube/certs/apiserver-kubelet-client.key # kube-proxy相关证书及私钥 - --proxy-client-cert-file=/var/lib/minikube/certs/front-proxy-client.crt - --proxy-client-key-file=/var/lib/minikube/certs/front-proxy-client.key - --requestheader-client-ca-file=/var/lib/minikube/certs/front-proxy-ca.crt - .... 从上面不难看出，对于一条通信链路的服务端和客户端，都需要提供一个证书，一个私钥，和一个验证对端证书的CA根证书。 再来看看我们的kubeconfig，里面有kubectl的客户端证书及秘钥，以及验证apiserver服务端证书的CA根证书。 $ kubectl config view apiVersion: v1 clusters: - cluster: # 用于验证apiserver服务端证书的根证书 certificate-authority: $HOME/.minikube/ca.crt server: https://172.17.0.40:8443 name: minikube contexts: - context: cluster: minikube user: minikube name: minikube current-context: minikube kind: Config preferences: {} users: - name: minikube user: # kubectl持有的客户端证书及私钥 client-certificate: $HOME/.minikube/profiles/minikube/client.crt client-key: $HOME/.minikube/profiles/minikube/client.key ","date":"2021-07-11","objectID":"/posts/minikube-tls/:2:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"寻幽 下面我们来验证一下kubectl与apiserver的证书及双向认证。 从上一节可以推断出kubectl中CA $HOME/.minikube/ca.crt可以验证apiserver的服务端证书 /var/lib/minikube/certs/apiserver.crt，apiserver的CA /var/lib/minikube/certs/ca.crt 可以验证kubectl的客户端证书$HOME/.minikube/profiles/minikube/client.crt，验证结果如下： # 服务端证书验证 $ openssl verify -CAfile ~/.minikube/ca.crt /var/lib/minikube/certs/apiserver.crt /var/lib/minikube/certs/apiserver.crt: O = system:masters, CN = minikube error 7 at 0 depth lookup:certificate signature failure 139668477843096:error:0407006A:rsa routines:RSA_padding_check_PKCS1_type_1:block type is not 01:rsa_pk1.c:103: 139668477843096:error:04067072:rsa routines:RSA_EAY_PUBLIC_DECRYPT:padding check failed:rsa_eay.c:705: 139668477843096:error:0D0C5006:asn1 encoding routines:ASN1_item_verify:EVP lib:a_verify.c:218: # 客户端证书验证 $ openssl verify -CAfile /var/lib/minikube/certs/ca.cr ~/.minikube/profiles/minikube/client.crt $HOME/minikube/profiles/minikube/client.crt: O = system:masters, CN = minikube-user error 7 at 0 depth lookup:certificate signature failure 139864367138456:error:0407006A:rsa routines:RSA_padding_check_PKCS1_type_1:block type is not 01:rsa_pk1.c:103: 139864367138456:error:04067072:rsa routines:RSA_EAY_PUBLIC_DECRYPT:padding check failed:rsa_eay.c:705: 139864367138456:error:0D0C5006:asn1 encoding routines:ASN1_item_verify:EVP lib:a_verify.c:218: 双方验证均未通过，难道是根证书有问题？ # 查看apiserver客户端证书 $ openssl x509 -text -in /var/lib/minikube/certs/apiserver.crt -noout Certificate: Data: Version: 3 (0x2) Serial Number: 2 (0x2) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=minikubeCA Validity Not Before: Aug 8 09:07:42 2020 GMT Not After : Aug 9 09:07:42 2021 GMT Subject: O=system:masters, CN=minikube Subject Public Key Info: ...... X509v3 extensions: X509v3 Key Usage: critical Digital Signature, Key Encipherment X509v3 Extended Key Usage: TLS Web Server Authentication, TLS Web Client Authentication X509v3 Basic Constraints: critical CA:FALSE X509v3 Subject Alternative Name: ...... ...... # 查看kubectl ca根证书 $ openssl x509 -text -in /var/lib/minikube/certs/ca.crt -noout Certificate: Data: Version: 3 (0x2) Serial Number: 1 (0x1) Signature Algorithm: sha256WithRSAEncryption Issuer: CN=minikubeCA Validity Not Before: Aug 8 09:07:41 2020 GMT Not After : Aug 7 09:07:41 2030 GMT Subject: CN=minikubeCA ...... apiserver服务端证书和kubectl的CA证书输出也符合预期，前者的Issuer与后者的Subject相同，说明前者的证书是后者颁发的；后者Issuer与Subject相同，说明其是一个合法的自签名证书。 我是谁，我在哪… 明明服务器上kubectl可以访问到apiserver，证书的颁发关系也没问题，而且TLS双向认证原理也是无懈可击，为什么openssl证书验证就是有问题的呢？ ","date":"2021-07-11","objectID":"/posts/minikube-tls/:3:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"溯源 上述结论存在一个致命的错误 Bug 证书A的Issuer与证书B的Subject相同，说明A的证书是B颁发的 这个错误犯得自己都想笑，x509证书中Issuer代表颁发机构，而Subject代表证书所有者，A的Issuer与B的Subject相同，只能说明A的证书颁发机构与B的所有者重名，并不能说明A一定是B颁发的。这就好比小明的妈妈叫小丽，但是叫小丽的不一定是小明的妈妈。证书认证本质上是通过数字签名进行的，签名的私钥是唯一的，但机构名称并不唯一，因此验签失败并不与实际矛盾。 但是事出反常必有妖，叫小丽的可能有很多，但是一个房子里就两个人，并且他们都叫小丽就很不寻常。这就让我开始怀疑环境中是不是有另一套minikubeCA根证书，即存在不止一个minikube？ 在验证kubectl客户端证书的时候留意到~/.minikube的配置目录中也有个apiserver.crt $ tree ~/.minikube minikube ├── addons ├── ca.crt ├── ca.key ├── ca.pem ├── cert.pem ├── certs │ ├── ca-key.pem │ ├── ca.pem │ ├── cert.pem │ └── key.pem ├── config │ └── config.json ├── files ├── key.pem ├── last_update_check ├── logs ├── machines │ ├── minikube │ │ ├── config.json │ │ ├── id_rsa │ │ └── id_rsa.pub │ ├── server-key.pem │ └── server.pem ├── profiles │ └── minikube │ ├── apiserver.crt │ ├── apiserver.crt.49553cf0 │ ├── apiserver.key │ ├── apiserver.key.49553cf0 │ ├── client.crt │ ├── client.key │ ├── config.json │ ├── events.json │ ├── proxy-client.crt │ └── proxy-client.key ├── proxy-client-ca.crt └── proxy-client-ca.key 使用openssl使用Kubectl的服务端验证根证书验证一下这个证书，验证通过。 $ openssl verify -CAfile ~/.minikube/ca.crt ~/.minikube/profiles/minikube/apiserver.crt $HOME/.minikube/profiles/minikube/apiserver.crt: OK 这时候另一个疑惑就涌上来了：为什么kubectl客户端证书和apiserver服务端证书会放在完全不相关的两个目录下？又挖一挖config文件，发现有auth path相关的路径配置。 $ cat ~/.minikube/machines/minikube/config.json { .... { \"AuthOptions\": { \"CertDir\": \"$HOME/.minikube\", \"CaCertPath\": \"$HOME/.minikube/certs/ca.pem\", \"CaPrivateKeyPath\": \"$HOME/.minikube/certs/ca-key.pem\", \"CaCertRemotePath\": \"\", \"ServerCertPath\": \"$HOME/.minikube/machines/server.pem\", \"ServerKeyPath\": \"$HOME/.minikube/machines/server-key.pem\", \"ClientKeyPath\": \"$HOME/.minikube/certs/key.pem\", \"ServerCertRemotePath\": \"\", \"ServerKeyRemotePath\": \"\", \"ClientCertPath\": \"$HOME/.minikube/certs/cert.pem\", \"ServerCertSANs\": null, \"StorePath\": \"$HOME/.minikube\" } } 尽管没看过minikube的源码，也没在官方文档上找到很明确的说法，但是基本上可以确定minikube启动的所有控制面组件使用的证书是在这里配置的，也就是完全存放在~/.minikubepath中。而pod中挂载的/var/lib/minikube/certs应该并非挂是宿主机对应的path上，毕竟minikube是运行在VM中的，挂载路径的灵活性便可想而知。 minikube的定位 Minikube is a tool that makes it easy to run Kubernetes locally. Minikube runs a single-node Kubernetes cluster inside a Virtual Machine (VM) on your laptop for users looking to try out Kubernetes or develop with it day-to-day. ","date":"2021-07-11","objectID":"/posts/minikube-tls/:4:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"破疑 接下来的就是之前使我们步入泥淖的两个问题： 在本机执行kubectl为什么会出现x509证书认证失败？ 宿主机上为什么会有/var/lib/minikube/certs一系列证书？ 这两个问题有着很强的相关性，一个大胆的猜想就是本机连的apiserver是在/var/lib/minikube/certs中认证的。 首先来看一下apiserver的配置 # 查看apiserver的配置 $ kubectl get pods/kube-apiserver-minikube -o yaml --namespace kube-system spec: containers: - command: - kube-apiserver - --advertise-address=172.17.0.40 - --secure-port=8443 - --allow-privileged=true - --authorization-mode=Node,RBAC - --client-ca-file=/var/lib/minikube/certs/ca.crt ... podIP: 172.17.0.40 podIPs: - ip: 172.17.0.40 从apiserver的配置中我们可以看到对应的podIP并不是宿主机的ip，而是个虚拟机的ip。这个再结合minikube在虚拟机中运行k8s很容易理解。那么也就是说minikube并没有暴露在宿主机的8443端口。可是我们执行kubectl并不是返回端口connection refused，而是返回的x509认证失败，这也就是宿主机的8443端口被监听。使用ps查看apiserver的进程，结果居然存在2个进程。 这样之前的灵异现象都能串起来了：宿主机还有一个额外的minikube集群，并且对外暴露了宿主机8443端口，使用/var/lib/minikube/certs进行认证。 经过一番查证与折腾，发现在/etc/kubernetes下有一些kubeconfig file以及控制面组件的部署文件。 $ tree /etc/kubernetes /etc/kubernetes ├── addons │ ├── dashboard-clusterrolebinding.yaml │ ├── dashboard-clusterrole.yaml │ ├── dashboard-configmap.yaml │ ├── dashboard-dp.yaml │ ├── dashboard-ns.yaml │ ├── dashboard-rolebinding.yaml │ ├── dashboard-role.yaml │ ├── dashboard-sa.yaml │ ├── dashboard-secret.yaml │ ├── dashboard-svc.yaml │ ├── ingress-configmap.yaml │ ├── ingress-dp.yaml │ ├── ingress-rbac.yaml │ ├── storageclass.yaml │ └── storage-provisioner.yaml ├── admin.conf ├── controller-manager.conf ├── kubelet.conf ├── manifests │ ├── etcd.yaml │ ├── kube-apiserver.yaml │ ├── kube-controller-manager.yaml │ └── kube-scheduler.yaml └── scheduler.conf 然后在服务器上指定kubeconfig为admin.conf执行kubectl，果然结果来自另外的一个集群。这个集群应该是我在最早接触k8s时使用的版本比较低的minikube搭建的，至于为啥部署形态会成这样我也不清楚至今仍然未解。 观察这个Minikube部署的apiserver，的确对外暴露的宿主机的8443端口。 $ sudo KUBECONFIG=/etc/kubernetes/admin.conf kubectl get pods/kube-apiserver-XX --namespace kube-system -o yaml ...... spec: containers: - command: - kube-apiserver - --advertise-address=10.122.101.148 - --secure-port=8443 - --client-ca-file=/var/lib/minikube/certs/ca.crt - --tls-cert-file=/var/lib/minikube/certs/apiserver.crt - --tls-private-key-file=/var/lib/minikube/certs/apiserver.key - ...... ...... podIP: 10.122.101.148 podIPs: - ip: 10.122.101.148 qosClass: Burstable 将admin.conf中的证书与/var/lib/minikube/certs下的证书进行互相验签，均可通过。真相由此浮出水面。 ","date":"2021-07-11","objectID":"/posts/minikube-tls/:5:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"前行 知道了问题的症结，需要向最初的目的前进了。先忽视掉上面揪出来的野minikube，尝试连接新搭建的minikube，另一个后续会通过一个额外的context加入到本地多集群管理中。 由于8443端口已经被先前的端口占用了，那么我们就选择暴露一个额外的端口38443吧。 # 首先启动minikube（我在启动前删除了之前部署的minikube数据），指定apiserver的端口为38443，指定apiserver的ip为10.122.101.148 # apiserver 端口其实不必指定，后续会介绍须引入本地端口转发来将服务暴露出去 $ minikube start --apiserver-ips=10.122.101.148 --apiserver-port=38443 # 查看apiserver的配置 $ kubectl get pods/kube-apiserver-minikube -o yaml --namespace kube-system spec: containers: - command: - kube-apiserver - --advertise-address=172.17.0.40 - --allow-privileged=true - --secure-port=38443 - --authorization-mode=Node,RBAC - --client-ca-file=/var/lib/minikube/certs/ca.crt ... podIP: 172.17.0.40 podIPs: - ip: 172.17.0.40 What? 不是设置了apiserver的ip为宿主机的ip了吗？为什么podIP还是虚拟机的ip？文档里还特意的解释了flag apiserver-ips的作用，就是要 make the apiserver available from outside the machine。 $ minikube start --help Starts a local Kubernetes cluster Options: --addons=[]: Enable addons. see `minikube addons list` for a list of valid addon names. --apiserver-ips=[]: A set of apiserver IP Addresses which are used in the generated certificate for kubernetes. This can be used if you want to make the apiserver available from outside the machine --apiserver-name='minikubeCA': The authoritative apiserver hostname for apiserver certificates and connectivity. This can be used if you want to make the apiserver available from outside the machine --apiserver-names=[]: A set of apiserver names which are used in the generated certificate for kubernetes. This can be used if you want to make the apiserver available from outside the machine --apiserver-port=8443: The apiserver listening port ...... apiserver-ips option is not working 这个issue专门描述了此问题，明确指出此参数仅会将对应的ip添加到证书SAN，不会修改apiserver的podIP。当然，作者困惑是否存在这种使用方式的场景。虽然不是很明确这样使用会带来什么样的问题（maybe端口冲突？安全？可扩展性？），但是作为初入坑k8s的同学来讲，这种简单的访问方式可能会降低一些我们的学习门槛。 apiserver-ips option is not working 既然这样，怎样才能将apiserver可以被本地访问到呢？这个也简单，可以通过ssh本地端口转发进行。 # 将服务器10.122.101.148的38443端口转发到172.17.0.40的38443端口 $ ssh -L 10.122.101.148:38443:172.17.0.40:38443 -N -f 10.122.101.148 最后，只需要将我们的kubeconfig的server字段改成10.122.101.148:38443就可以开心的访问远程的minikube了。 x509证书扩展 将对应ip添加到证书SAN，这个解释我并不是很理解，直到后面做了个实验才有些感觉。在没指定apiserver-ips去curl https://10.122.101.148，会出现诸如Hostname 10.122.101.148 doesn’t match certificate’s altnames: “Host: XXX. is not in the cert’s altnames:XXX的错误，而指定了apiserver-ips=10.122.101.148后，apiserver的服务端证书便可以通过验证。这是因为该参数将ip加入到了服务端证书的SAN扩展字段中，使得10.122.101.148成为证书可信的域。 $ openssl x509 -text -in .minikube/profiles/minikube/apiserver.crt -noout X509v3 extensions: X509v3 Subject Alternative Name: DNS:minikubeCA, DNS:control-plane.minikube.internal, DNS:kubernetes.default.svc.cluster.local, DNS:kubernetes.default.svc, DNS: kubernetes.default, DNS:kubernetes, DNS:localhost, IP Address:10.122.101.148, IP Address:172.17.0.40, IP Address:10.96.0.1, IP Address:127.0.0. 1, IP Address:10.0.0.1 ","date":"2021-07-11","objectID":"/posts/minikube-tls/:6:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"插曲 在验证双向认证时，为了避免两端都要使用openssl验证，便想通过curl去验证。使用curl验证的结果让我再次怀疑人生… # 执行双向验证，客户端验证服务端证书失败 $ curl https://10.122.101.148:38443 --cert ~/.minikube/profiles/minikube/client.crt --key ~/.minikube/profiles/minikube/client.key --cacert ~/.minikube/ca.crt curl: (60) server certificate verification failed. CAfile: ~/.minikube/ca.crt CRLfile: none More details here: http://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. # 关闭客户端的服务端证书验证，正常返回 $ curl https://10.122.101.148:38443 --cert ~/.minikube/profiles/minikube/client.crt --key ~/.minikube/profiles/minikube/client.key -k { \"paths\": [ \"/api\", \"/api/v1\", \"/apis\", \"/apis/\", ... ] } # 确认客户端CA对服务端的证书验证结果，认证通过 openssl verify -CAfile ~/.minikube/ca.crt ~/.minikube/profiles/minikube/apiserver.crt ~/.minikube/profiles/minikube/apiserver.crt: OK 上面的测试结果看上去互相矛盾，折腾了很久，大概查了下curl的文档，感觉自己做的并没有问题，大胆的怀疑一下是curl的版本问题，于是准备在一个curl版本更高的镜像里去执行这个curl命令。 # 在golang:1.16.2镜像中通过curl验证双向认证，成功 $ docker run --rm -v $HOME/.minikube:/minikube golang:1.16.2 curl https://10.122.101.148:38443 --cert /minikube/profiles/minikube/client.crt --key /minikube/profiles/minikube/client.key --cacert /minikube/ca.crt { \"paths\": [ \"/api\", \"/api/v1\", \"/apis\", \"/apis/\", ... ] } 果然是旧版本curl的问题。由于目前为止在这上面浪费了比较久的时间，并且感觉越往后排查越偏离初始方向，因此便未继续往下追溯。二者版本列举如下，希望自己未来有时间、有兴趣去往底层排查一下。 # 服务器10.122.101.148上curl版本 $ curl --version curl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3 Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets # go:1.16.2 image中curl版本 $ docker run --rm -v $HOME/.minikube:/minikube golang:1.16.2 curl --version curl 7.64.0 (x86_64-pc-linux-gnu) libcurl/7.64.0 OpenSSL/1.1.1d zlib/1.2.11 libidn2/2.0.5 libpsl/0.20.2 (+libidn2/2.0.5) libssh2/1.8.0 nghttp2/1.36.0 librtmp/2.3 Release-Date: 2019-02-06 Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp scp sftp smb smbs smtp smtps telnet tftp Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP HTTP2 UnixSockets HTTPS-proxy PSL ","date":"2021-07-11","objectID":"/posts/minikube-tls/:7:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"悟道 你永远不知道你不知道的事，只有学习才会让我们发现原本我们不知道的事，哪怕过程使我们谦(zi)卑。 最初只是想体验一下telepresence，仅仅是一个准备工作就牵出如此多的知识点，遇到问题是痛苦的，但寻找解决问题的方法是快乐的。 与安全相关的问题搞得多复杂都不为过，仅仅是k8s认证阶段中的证书认证这一步就如此的丰富，后面的鉴权、准入等过程会更加刺激。 工具的使用很皮毛，比如curl，openssl，不过比较复杂的工具往往在实际问题排查时仅需要最基本的使用方法。 仍然存在诸多基础知识盲区，比如 ssh端口转发， 数字签名，x509证书相关概念等，后面会有专门的文章来记录这些基础。 好记性不如烂笔头，文章落笔于解决问题的第二天，但是能记住的甚微，写文章时基本又根据history重放了一遍才将脉络重新梳理清楚。 ","date":"2021-07-11","objectID":"/posts/minikube-tls/:8:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["探索与实战"],"content":"启明 远程访问minikube 一文带你彻底厘清 Kubernetes 中的证书工作机制 Installing Kubernetes with Minikube Certificate Signing Requests apiserver-ips option is not working ","date":"2021-07-11","objectID":"/posts/minikube-tls/:9:0","tags":["k8s","tls"],"title":"Minikube中的TLS认证探秘","uri":"/posts/minikube-tls/"},{"categories":["整理与总结"],"content":"参考谢希仁 《计算机网络（第5版）》 ","date":"2021-06-27","objectID":"/posts/network/:0:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"概述 ","date":"2021-06-27","objectID":"/posts/network/:1:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"电路交换与分组交换 电路交换 需要建立物理连接，需要通过电线两两相连。 通话过程中，两个用户始终占用端到端的通信资源，线路传输效率比较低。 分组交换 存储转发技术，将报文划分成若干数据包（分组），并将地址等信息加入消息头封包。 使用特定协议，通过路由器转发分组。 对于高速网络，提高的仅仅是数据发送速率，而不是比特在链路上的传播速率，后者仅与传输介质有关。 ","date":"2021-06-27","objectID":"/posts/network/:1:1","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"OSI七层模型 应用层：直接为用户的应用进程提供服务。 传输层：为两个主机中进程间通信提供服务，由于每个主机可以同时运行多个进程，因此传输层有复用和分用的功能，一台主机上的多进程一般通过不同端口来区分。 网络层：为网络上不同主机提供通信服务，以IP数据报方式进行封装并传输。 数据链路层：IP数据报封装成帧，加入控制信息，并在相邻节点的链路上透明传输。 物理层：传输比特流。 ","date":"2021-06-27","objectID":"/posts/network/:1:2","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"网络分层特点 协议是控制对等层通信的规则集合，即协议是水平的。 服务由下层向上层通过层间接口提供，即服务是垂直的。本层服务无法看见下面的协议，即下面的协议对上面的实体是透明的。 底层的实体在封装上一层的数据包时，通常会加一个包头用以描述本层的信息。在解包时，对应层将对应的包头剥离并解析。 网络分层 ","date":"2021-06-27","objectID":"/posts/network/:1:3","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"数据链路层 ","date":"2021-06-27","objectID":"/posts/network/:2:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"基本问题 封装成帧 将IP数据报前后添加首部和尾部，构成一个帧。 首部和尾部的一个作用是起到帧定界，此外还包含很多控制信息。 MTU表示最大传输单元，即帧的数据部分的长度上限。 透明传输 传输的文本可能与定界控制字符冲突，这样需要通过添加转义字符来将数据文本与控制字符区分出来，以便输入的任何字符都可以放在这样的帧中传输过去，这样的传输称为透明传输。 差错检验 现实通信链路不会是理想的，bit在传输过程中可能产生零一倒置，引发比特差错。 数据链路层常常通过采用CRC（循环冗余检验）来检错，并不能纠错。 数据链路层的差错检验只能检出比特差错，而无法检测出帧丢失，帧重复，帧失序等问题，因此数据链路层无法提供可靠传输。 注意 为什么当前网络协议中数据链路层不通过帧编号，确认和重传机制做到可靠传输呢？由于当前通信线路的质量大大提高，数据链路层的出错概率大大降低。因此广泛使用的网络协议中都不要求数据链路层提供可靠传输。一旦传输时出现了差错，那么改正差错由上层的TCP协议完成，从而提升通信效率。 ","date":"2021-06-27","objectID":"/posts/network/:2:1","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"局域网 拓扑 星型（使用最广泛） 令牌环形 总线型 树形 共享信道可以使多个用户共享通信媒体资源，主要有以下两种方法： 静态划分信道，即时分/频分/波分/码分等复用方式使得各用户分配到的信道资源正交，不会彼此发生冲突。但其利用率较低，代价较高，不适用于局域网。 动态媒体接入控制，又称多点接入，分为随机接入和受控接入两种。前者用户可以随机发送信息，如果发生碰撞就需要通过解决冲突的网络协议来处理冲突；后者需要服从某种控制来接入，常用于令牌环拓扑中的轮询控制，使用的较少。 ","date":"2021-06-27","objectID":"/posts/network/:2:2","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"以太网 采用随机接入的方式。 将数据链路层演进为MAC层（媒体接入层）。 采用曼彻斯特编码，解决连0/连1问题导致的无法从比特流中提取位同步问题。 由于多站共享一个信道，单个站无法做到同时发和收，因此以太网是半双工通信。 冲突解决协议采用CSMA/CD协议（载波监听多点接入/碰撞检测）。 多点接入表明其为总线型网络，多个站点共享信道。 载波监听表明发送前先监听，，尽量避免冲突的产生。 碰撞检测表示边发送边监听，在发送过程中通过传输信号的异动判断出其他人也在发送，从而检测出碰撞。此时应立即停止发送，避免持续浪费网络资源，并采用指数退避的方式重试。 集线器 集线器使得以太网从总线型拓扑演进到星型拓扑，采用的传输介质是双绞线。 使用集线器后的以太网仍是一个总线网，同一时刻只有一个站发送数据，扩展以太网是网桥的事。 集线器工作在物理层，仅做简单的比特转发，本质就是一个多接口转发器，不进行碰撞检测。 MAC层 MAC地址即物理地址，48位，固化在适配器中，具有唯一性。 MAC帧组成 目的地址 源地址 类型（描述上一层的协议类型） 数据（长度在46-1500字节之间，不足46字节加入填充字段） 帧检测序列（FCS） 扩展以太网 物理层通过使用光纤和光纤调制解调器进行电/光信号转换，可以将多个集线器连接起来，避免减少信号的衰减导致的CSMA/CA协议无法工作，本质上扩大了碰撞域。 扩大碰撞域 网桥：将多个网段连接起来，扩展以太网范围，其依赖转发表来在各个网段中转发帧。 优势 各网段碰撞域相互隔离，不同网段的通信互不干扰，增大吞吐量。 扩大了物理范围，增加了接入的站数。 提升可靠性，网段之间互不干扰。 缺点 查找转发表增加了时延。 没有流量控制，当网络负载很重时，会出现丢帧情况。 接入站点数过多时会导致广播风暴 多个相互隔离的碰撞域 透明网桥 以太网上的站点不知道帧会经过哪几个网桥，这类网桥一旦接入局域网，无需经过人工配置转发表就可以工作。 透明网桥的自学习 初始时转发表为空。 网桥每收到一个帧时，就会记录其源地址和来源端口，构建转发表（盲转发）。 收到一个帧时，会先查转发表，如果目的地址存在，则向对应的端口转发，否则向除来源外的所有端口转发。 转发表项中会记录更新时间，在站地址发生变更时能够使得相关表项过期掉。 透明网桥使用生成树算法，避免帧在网络中兜圈子。 自学习示意图 网桥与集线器的区别 网桥位于数据链路层，是基于存储转发，将完整的帧接收下来并校验无误后转发出去，网桥将两个局域网连接成一个大的网络，不同局域网碰撞域相互隔离；而集线器工作在物理层，是逐比特转发，只是扩大了碰撞域。 以太网交换机（L2交换机） 本质是多接口网桥，通过隔离碰撞域连接多个网络，提升吞吐率。 VLAN可以通过交换机实现，在MAC帧首部增加4个字节，通过其中的VID来标识此帧所属的VLAN。 ","date":"2021-06-27","objectID":"/posts/network/:2:3","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"网络层 ","date":"2021-06-27","objectID":"/posts/network/:3:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"特点 向上提供best-effect数据报服务，不提供质量保证。 实际链路传输由下层提供，且对网络层透明。 网络层使用IP地址标识每个站点，其中包括网络号用以标识网络，主机号用以在网络中唯一标识主机。 使用路由器作为中间设备，将不同的网络（可以是异构的）连通起来，构成一张逻辑网络。而网桥连接的多张网络在网络层而言只是一个网络，具有同一个网络号。 ","date":"2021-06-27","objectID":"/posts/network/:3:1","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"IP地址与硬件地址 IP地址是网络层上使用的地址，存放在IP数据报的首部；硬件地址是物理层和链路层使用的地址，存放在MAC帧的首部，MAC帧会将IP数据报作为自己的数据负载。 硬件地址对网络层不可见，换句话说，数据链路层及以下的通信对于网络层是透明的。 IP地址与MAC地址在数据流动中起到的作用 ARP协议（地址解析协议）：根据IP地址解析物理地址。 主机的ARP高速缓存中存放IP地址到物理地址的映射表，并动态更新。 具体流程 如果ARP高速缓存中有，则直接获取。 否则在本网络广播一个ARP请求。 站点发现IP是本站的IP则将物理地址加入到一个ARP响应中并回复，其他站点忽略此请求。 将该响应的物理地址与请求的IP地址构建的映射关系加入ARP高速缓存。 同网桥的自学习类似，ARP缓存也存在生存时间（ttl）避免站点地址发生变化。 RARP协议（逆地址解析协议）：根据物理地址解析IP地址，逐渐被DHCP所取代。 为何要在物理地址的基础上构建IP地址，而不是直接使用物理地址来通信呢？ 异构网络的物理地址十分复杂，彼此之间的通信所带来的复杂转换过程基本上无法在用户主机上完成。使用统一的IP地址屏蔽掉了底层网络的复杂性。 ","date":"2021-06-27","objectID":"/posts/network/:3:2","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"IP数据报 格式：首部（可变部分+不可变部分）+ 数据部分。 首部固定部分（20字节） 版本：表示IP协议（v4, v6）。 首部长度：表示首部总长度，为4*x字节。 区分服务，一般不用此字段。 总长度：首部和数据的长度之和。 标识：用于串联分片。 标志：DF=1表示不允许分片，MF=1表示后续还有分片。 片偏移：用于描述分片的顺序，从而在收端组装分片。 生存时间：IP报文在路由器间转发的最大跳数，避免在网络中兜圈子。 首部校验和：CRC校验首部。 源地址 目的地址 可变部分 一般不使用，在ipv6中无可变部分。 需填充至4字节的整数倍。 ","date":"2021-06-27","objectID":"/posts/network/:3:3","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"路由 路由表 每条路由记录着 （目的网络地址，下一跳地址）。 路由表中记录着网络地址而非主机地址的原因是避免路由表过大，导致查表时引入较大的开销。 会有一些特例，在路由表中的地址是特定主机的地址，称为特定主机路由，一般用于排查问题。 在路由表中没有找到网络地址时，会走默认路由。 路由转发时，不会改变数据报的源地址与目的地址，而是通过ARP协议将路由表中的网络地址转为MAC地址进行转发，IP地址屏蔽了底层帧传输细节。 分组转发算法 从数据报首部获取目的IP地址D，得到网络地址N。 若网络N与该路由器直接相连，则直接交付到特点的主机。 若路由表中有地址D的特定路由，则直接把数据报转发给下一跳所对应的路由器。 若路由表中有到达网络N的路由，则把数据报转发给下一跳所对应的路由器。 若路由表中有默认路由，则将数据报转发给默认路由器。 以上均不满足则报错。 ","date":"2021-06-27","objectID":"/posts/network/:3:4","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"子网与超网 子网出现的原因 IP地址空间利用率低。 每个物理网络一个网络号会使路由表太大导致性能变坏。 两级IP地址不够灵活 子网特点 原物理网络基础上，划分出若干个子网，由两级变三级。 对外仍表现为一个网络，减少外网路由表的条目数，通过多分一级变线性为乘法。 划分子网后，路由表多了一列子网掩码，在路由查找时会多一步将IP地址与子网掩码进行与运算获取网络地址，再进行匹配查找。 超网（CIDR，无分类编址）特点 消除了A,B,C类地址及划分子网的概念，通过引入网络前缀的概念使得网络的划分更灵活。 从三级编址\u003c网络号，子网掩码，主机号\u003e回到了两级编址\u003c网络前缀，主机号\u003e。 采用最长前缀匹配从而进行能匹配得更具体。 通过二叉查找树的数据结构提升路由查找效率。 超网出现的原因 为解决传统IP分类及子网划分引起的IP地址分配不灵活，从而降低IP地址利用率低的问题。 ","date":"2021-06-27","objectID":"/posts/network/:3:5","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"路由协议 自治系统AS 单一技术管理下的一组路由器。 典型的一个ISP就是一个AS。 本质上是一个网络单元，内部表现同质，对外部表现为一个整体。 内部网关协议（IGP）：一个AS内部使用的路由选择协议。 RIP OSPF 原理描述 基于距离（跳数）向量的路由选择协议 基于代价的最短路径优先算法，使用链路状态协议 使用算法 距离向量算法（基于ford算法） 洪泛法+Dijkstra最短路径算法 交换对象 相邻下一跳路由器 相邻的所有路由器 交换内容 本路由器所知道的全部信息（到所有网络的距离） 本路由器与相邻所有路由器的链路状态（为构建网络拓扑图） 交换间隔 固定时间间隔 发生变化时 限制 跳数不超过16 - 传输方式 UDP IP数据报 适用场景 网络规模小的场景（少于最大跳数限制） 网络和规模大的场景 优势 实现简单，开销较小 更新过程收敛的快；代价相同的路径可以通过负载均衡策略选择 劣势 网络故障时，将消息传递给所有的路由器时间很慢；只会选择一条跳数最小的路径 协议负载，需维护链路状态数据库，开销较大 外部网管协议（BGP） 需考虑的问题 网络规模太大，使得AS之间的路由选择很困难。 AS之间是异质的，路由选择时需考虑相关策略。 核心：力求找到一条比较好的路由，而非选择最佳路由。 ","date":"2021-06-27","objectID":"/posts/network/:3:6","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"虚拟专用网VPN 机构内部使用专用地址，节省公网地址资源。 因特网中所有路由器对目的地址是专用地址的数据报一律不行进转发。 专用IP地址集合： 10.*.*.* 172.[16-31].*.* 192.168.*.* VPN：用于外网/专网/远程接入专用互联网。 实现方式：IP隧道技术 隧道技术示意图 ","date":"2021-06-27","objectID":"/posts/network/:3:7","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"网络地址转换NAT 用于专用网内的一台主机与外网进行通信。 NAT路由器（一般是网关）需要有公网IP，专网内的主机通过公网IP与外界通信。 NAT本质上是通过端口映射的方式实现多机同时与外界通信的，即一个主机的请求映射到公网IP：特定端口上，与外部的通信地址就是该形式，在NAT路由器侧进行转换。 根据通信方向将NAT分为sNAT与dNAT。 sNAT：专网内主机访问外网。 dNAT：外网访问专网内主机。 ","date":"2021-06-27","objectID":"/posts/network/:3:8","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"其他 PING使用了ICMP（网际控制协议）协议进行报文信息交换。 IGMP（网际组管理协议）用于通知多播路由器本局域网中是否有主机加入或退出了某多播组。 ICMP，IGMP，OSPF报文均在IP层传输。 ","date":"2021-06-27","objectID":"/posts/network/:3:9","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"传输层 传输层与网络层的区别 网络层为主机之间提供逻辑通信（主机之间），传输层为应用进程之间提供端到端的逻辑通信（进程之间）。 两个主机上的多进程通信是在IP协议上加上一层端口的概念区分开来的。 IP数据报只检验首部，而TCP/UDP的检验是首部+数据。 ","date":"2021-06-27","objectID":"/posts/network/:4:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"UDP协议 仅在IP数据报服务上加了复用/分用及差错检测的功能。 特点 无连接。 尽最大努力交付，不可靠（校验有误后就丢弃掉）。 面向报文，一次交付一个完整报文。 无拥塞控制。 支持单播，多播，广播通信。 首部开销小（8字节，源端口、目的端口、长度、校验和），传输速度快。 ","date":"2021-06-27","objectID":"/posts/network/:4:1","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"TCP协议 特点 面向连接，虚连接。 只能点对点，无法实现一对多，TCP的端点是Socket，形如IP:PORT。 提供可靠交付，保证数据无差错、不丢失、不重复、不失序。 全双工通信。 面向字节流，即不像UDP一次交付一个完整报文，而是根据窗口值和网络拥塞决定报文段的大小。只是最终接收方应用程序需要和发送方应用程序发出的字节流完全一样。 TCP报文的首部 源端口和目的端口。 报文段序号，按字节顺序编号，起始序号需要在TCP建立连接时确认。 确认号，期望收到对方下一个报文段的第一个数据字节的序号。 数据偏移，指出TCP首部长度，因为TCP首部是变长的。 保留位 控制位（6位） 紧急URG：表明报文为紧急数据，不受接收窗口值限制。 确认ACK：表明此信息为确认信息。 推送PSH：表明报文需要尽快将报文交付给应用程序。 复位RST：表明因为某种原因需要释放连接。 同步SYN：表明该报文用于请求建立TCP连接。 终止FIN：表明该报文用于请求释放TCP连接。 窗口，接收窗口值。 检验和 紧急指针，URG=1时生效，表明紧急数据字节数。 选项，长度可变。 可靠传输 停止等待协议 又称为自动重传请求（ARQ） 每发送完一个分组就停止发送，等待对方确认，收到确认后再发送下一个分组。 一段时间（略大于RTT）后没有收到收端的分组确认，便会重传。因此要求发送完分组必须暂时保留已发送的副本，此外分组和确认分组需要进行编号来确认是否发送成功。 ARQ协议比较简单，但信道利用率很低，可以通过利用流水线技术予以改造（连续ARQ协议）。 连续ARQ协议 在发送窗口内的多个分组可以连续发送出去，不需要等待对方的确认。 接收方采用累积确认，即对按序到达的最后一个分组发送确认。 滑动窗口协议 本质是连续ARQ协议，滑动窗口以字节为单位，而非以报文为单位。 TCP流量控制的实现，流量控制的作用是让发送方的发送速率不要太快，要让接收方来得及接收。 窗口内可能包含两类数据，可以使用三个指针来标识，其中P1、P3为窗口前后沿，P2为两类数据的分界。 已发送但未收到确认（P2-P1），此类数据需要被缓存住，以备重传。 允许发送但尚未发送（P3-P2）。 发端和收端均有一个窗口值，发送窗口会受接收窗口的调节（发送窗口\u003c=接收窗口），因此接收窗口是根据接收端处理能力动态变化的。不过发送窗口和接收窗口并不总是一样大，因为通过网络进行窗口的同步存在延时。 为何发送窗口可能小于接收窗口? 发送窗口除了受流量控制的调节外，还受拥塞控制的调节。 在拥塞控制中，引入了拥塞窗口的概念，因此为避免网络拥塞，发送窗口需要小于拥塞窗口。即 发送窗口=min(接收窗口，拥塞窗口) 接收方需要有累积确认的能力，对于未按序到达的数据，建议接收方可以将其缓存起来（而非直接丢弃）以提升传信率。 由于累积确认只会传递按序到达的最后一个分组的确认消息，在网络有较多的丢包情况下，发端会重传很多数据，加速网络的恶化。因此引入选择确认(SACK)进行优化，其可以在ACK报文首部中描述已到达消息的边界。 如果每次传输的报文数据相对于TCP首部很小，那么传输效率会很低，滑动窗口存在以下问题。 当用户数据逐字发送时，会引起单个TCP报文数据内容过短。可以通过发送方攒batch方式将数据缓存起来发送。 糊涂窗口问题，即收端逐字节腾出接收窗口空间，导致发送窗口一直很小。此问题可以通过让接收方攒batch方式批量增大接收窗口。 接收窗口为0时是否可能引发死锁？ 如果接收窗口从0发生变化时，其会向发送方同步接收窗口。但如果此同步信息丢失了，那么发送窗口是否会一直为0，发端无法发送数据，收端接收不到数据，且没有变更窗口值的打算，是不是会陷入死锁的状态？ 不会，首先需要明确窗口的同步报文不存在确认机制，同样也不存在超时重传机制。为了避免上述死锁的问题，TCP连接维护一个计时器，当接收窗口为0时，计时器开始计时，若计时到期则发送一个零窗口探测报文，对方会在确认这个报文时给出当前的窗口值。 拥塞控制 网络拥塞主要是由于对资源的需求大于可用资源产生，引起网络拥塞的可能原因列举如下。 结点缓存的容量太小，导致到达该结点的分组被丢弃。 处理机处理时间太慢，导致队列逐渐被灌满，后到达的请求会被丢弃。此外处理速度慢，可能导致驻留在队列时间过长，超过了超时重传间隔，引起重传。 拥塞控制的核心思想在于通过控制发送方发送速率，防止过多数据注入到网络，避免网络中的路由器或链路出现过载。 拥塞控制的方法 慢开始：发送方维护一个拥塞窗口，发送起始窗口值为1，之后每轮窗口值翻倍，直到超过了慢开始门限 ssthresh。当发生拥塞时，ssthresh减半（乘法减小），从头进行慢开始。 拥塞避免：当拥塞窗口达到了ssthresh，窗口值开始逐1递增（加法增大），以降低发送增速。 快重传：当收端收到一个失序的报文就立即发送一个重复确认，当发端连续收到3个重复确认就立即重传报文，而无需等待此报文的重传计时器到期。快重传用于尽早重传未被确认的报文段，快速发现拥塞。 快恢复：当发生乘法减小时，并不从头执行慢开始，而是将拥塞窗口设为ssthresh的一半，之后开始进行加法增大。 全局同步 当网络拥塞发生时，缓存队列已满，新来的分组全部被丢弃（尾部丢弃策略），此时所有TCP连接都会触发重传，从而触发乘法减小，这就导致了TCP中的全局同步问题。 全局同步导致全网通信量可能瞬时减小很多，而一段时间后又会增大很多。 可以通过随机早期检测RED策略来改善全局同步问题，此策略核心是设置软硬门限；在软门限内，不丢弃任何报文，而在软门限和硬门限间，以一个自适应概率丢弃报文；当超过了硬门限，则采用尾部丢弃策略。RED可以将批量的乘法减小预打散，以缓解全局同步问题。 TCP的连接管理 连接建立 三次握手用以同步双方序号。 建立连接为何需要3次握手，而不是2次或者4次？ 由于建立连接需要双方同步一些信息（主要是序号），而在不可靠信道中完成可靠的同步，至少需要3次通信。 如果是两次的话，当一个历史握手请求由于滞留在网络中而在连接关闭后到达收端，收端给出的ACK会被发端忽略掉，此时收端已经进入ESTABLISHED状态，而发端又不会向收端发信，导致资源会出现浪费。 理论上双方互相确认是需要4次通信的，即像挥手一样的两个来回。只是建立连接时并不像释放连接那样在两个来回间还会继续发送未发完的数据。因此第一次握手的ACK和同步请求可以放在一个报文里（从图示可以看到），所以可以减少一次握手。 连接释放 四次挥手 前两次挥手由主动方发起，被动方确认，关闭主动方的连接。 被动方等待数据发送完。 后两次回收由被动方发起，主动方确认，关闭被动方的连接。 TIME-WAIT的作用 主动端发送最后一个ACK时进入TIME-WAIT状态，需要等待2MSL才变更到CLOSED状态，有以下两个原因。 最主要的原因是确保被动端收到了确认消息并正常关闭。如果此ACK丢失了，被动端可以重传FIN报文，主动端可以重新发送ACK。如果主动端跳过TIME-WAIT状态直接进入CLOSED，其无法重传ACK，被动端也无法正常的进入CLOSED。 为了确保在网络中兜圈子的报文进入到下一次连接中，可以在此次TIME-WAIT阶段消化掉。 ","date":"2021-06-27","objectID":"/posts/network/:4:2","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"应用层 ","date":"2021-06-27","objectID":"/posts/network/:5:0","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"域名系统DNS 域名系统 将人们使用的机器的名字转换为IP地址。 因特网上的域名系统是由分布式域名服务器实现的。 使用UDP传输。 域名服务器 分类 根域名服务器，知道所有顶级域名服务器的域名和IP地址，是最高层次的域名服务器。 顶级域名服务器，负责管理在该顶级域名服务器注册的所有二级域名。 权限域名服务器，负责一个区的域名服务器，一个域下可能会有多个区。 本地域名服务器，也称为默认域名服务器，离用户较近，可以解析同一个ISP下的所有域名，是向外查询的入口。 解析方式 递归查询 如果DNS服务器不知道被查询域名的IP地址，那么其就以DNS client身份向根域名服务器发出查询请求报文，而不是告知主机下一个查询DNS服务器的地址让主机自己查询。 一般用于主机向本地域名服务器的查询。 迭代查询 当域名服务器收到迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器下一步应向那个域名服务器进行查询，并不充当DNS client帮其查询。 一般用于本地域名服务器向根域名服务器的查询。 高速缓存 为提高DNS查询效率，减少解析所传的报文数量。 存放最近查询过的域名以及从何处获得域名映射信息的记录。 可以是域名对应的IP。 也可以是域名对应顶级域名服务器的IP地址。 为防止无法感知到域名发生变动，DNS缓存一般会有一个过期时间。一般而言，DNS发生变动的频率较低，缓存与数据库的一致性影响不大。（在云原生的某些场景下DNS变化较为频繁，DNS缓存会成为瓶颈。） ","date":"2021-06-27","objectID":"/posts/network/:5:1","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"文件传输协议FTP/TFTP FTP基于TCP，TFTP基于UDP FTP通过复制整个文件以实现文件共享，如果需要修改文件，需要先获取文件的一个本地副本，在副本上进行修改，然后再将修改后的文件传回。 FTP基本原理 一个主进程负责接收新的请求。 若干从属进程，负责处理单个请求。 FTP的客户端与服务器间要建立两个并行的TCP连接，控制连接和数据连接，控制连接在会话期间一直打开，客户端传输请求通过控制连接发送给服务端的控制进程，控制连接不用于传输文件；数据连接用于传输文件，服务端控制进程收到客户端的请求后便会创建数据传输进程和数据连接，用来传输文件。FTP的数据面与控制面相互分离。 熟知端口是21（监听）和20（传输）。 FTP缺陷：复制整个文件实现共享不够灵活。即便只需要在原有的文件作出一点修改时，也得传输整个文件。而NFS可以允许应用进程打开一个本地文件，并在该文件的某个特定位置开始读写数据，因此在网络上传输的只是少量的修改数据。 TFTP工作原理 类似停止等待协议，发送数据需要确认。 熟知端口是69。 ","date":"2021-06-27","objectID":"/posts/network/:5:2","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"万维网WWW 统一资源定位符URL 用于标志分布在整个因特网上的万维网文档。 组成：\u003c协议\u003e://\u003c主机\u003e:\u003c端口\u003e/\u003c路径\u003e。 最常使用的协议是http，其次是ftp。 超文本传输协议 HTTP 无状态，使用TCP传输。 从浏览器点击一个链接发生的事件： 浏览器分析URL并通过DNS解析出IP。 浏览器向此IP服务器建立TCP链接。 浏览器发送读取文件的命令。 服务器将对应的文件以HTTP response的方式返回给浏览器。 释放TCP连接。 浏览器将收到的数据进行渲染，将结果返回给用户。 HTTP 1.0 vs HTTP 1.1 vs HTTP 2.0 在HTTP 1.0中每个http请求都会创建一个TCP连接，因此一个GET命令需要2RTT；此外，短连接的频繁创建与销毁会影响数据传输效率，因为每次创建后数据的发送都会历经一个慢开始的过程。 HTTP 1.1支持了长连接，减少了频繁建立TCP连接的开销。此外还支持pipeline技术，以缩短TCP连接空闲时间，提升服务器资源利用率。 HTTP 2.0在HTTP 1.1 pipeline基础上又提升了复用程度，此外消息的传输从明文变为二进制字节流，提升传输效率。 代理服务器可用作万维网高速缓存，用户的请求到达proxy后，如果发现有缓存则直接返回。否则proxy充当http client向服务器发起请求，并将响应缓存起来。 Cookie：由服务器返回给浏览器，记录一些用户信息，在下次发http请求时连同此信息发送给服务器。存在的核心原因是http是无状态的。 检索系统 搜索引擎分类 全文搜索引擎：通过蜘蛛在网络链接间爬行，建立一个大型的在线数据库，用户输入关键词在此数据库中搜索结果。 分类目录搜索引擎：数据库是按目录分类组织的，用户需要按设计好的目录逐级查询。 垂直搜索引擎：相比全文搜索引擎，更加针对某一特定领域、特定人群或特定需求提供搜索服务，即在某个行业知识上下文中进行关键词搜索。 元搜索引擎：将用户请求发到多个独立的搜索引擎上搜索，将结果统一处理，称为搜索引擎之上的搜索引擎。 ","date":"2021-06-27","objectID":"/posts/network/:5:3","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"电子邮件 主要构成组件 用户代理（UA） 邮件服务器 邮件发送协议（如SMTP）及邮件读取协议(如POP3) SMTP协议和POP3协议 二者均是基于TCP传输。 从UA发送到发送方邮件服务器的过程使用了SMTP协议。 从发送方邮件服务器向接收方邮件服务器传输使用了SMTP协议。 UA从接收方邮件服务器读取邮件的过程使用了POP3协议。 为什么不将邮件服务器程序植入到UA中？ 并非所有计算机都具有运行邮件服务器的条件。 邮件服务器程序需要24小时运行，而个人PC通常不具有long running的特性。 ","date":"2021-06-27","objectID":"/posts/network/:5:4","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"动态主机配置协议 DHCP 作用：自动进行网络配置。 在计算机第一次连入到因特网时，需要相应的网络配置。 IP地址 子网掩码 默认路由器的IP地址 域名服务器的IP地址 计算机在网络中的位置发生变化时，需要更新网络配置，因此需要采用自动协议配置方式。 DHCP是基于UDP的（需要广播），知名端口是68（client）与67（server）。 工作原理 待配置的主机广播一个发现报文，用以寻找DHCP server。 DHCP中继路由器发现此报文，会以单播形式转发给DHCP server。 DHCP server将网络配置信息及租用期返回给DHCP中继路由器。 DHCP中继路由器将其返回给主机。 主机获取到对应的网络配置并应用，同时计算租期。 租期临近时向DHCP server发起续约请求。若同意则更新租期，否则发送发现报文重新获取网络配置。 ","date":"2021-06-27","objectID":"/posts/network/:5:5","tags":["网络"],"title":"《计算机网络》纲要整理","uri":"/posts/network/"},{"categories":["整理与总结"],"content":"参考王爽《汇编语言》 CPU读写外部设备需要如下交互信息（分别对应三大逻辑总线） 地址信息：存储单元的地址。 控制信息：器件的选择，读或写的命令。 数据信息：读或写的数据。 寄存器的作用 CPU可直接读写的部件，可以通过改变寄存器的内容实现对CPU的控制。 寄存器cheat sheet 名称 类型 作用 AX 通用寄存器 用于一般的数据传递 BX 通用寄存器 用于表示偏移地址，如mov ax [bx]将ds:[bx]中的内容送入ax CX 通用寄存器 用于存储循环变量，cx中的值会影响loop指定的执行结果，cx为零时会跳出循环 SI/DI 配合BX指明内存单元的偏址，如[bx+si+idata] SP 存放栈顶的偏移地址 IP 指令指针寄存器，存放代码段的偏址 BP 栈内偏址，主要用于指向/寻找栈内的某个地址的元素 DS 段寄存器 存放数据段的寄存器，配合[addr]做段内寻址 SS 段寄存器 存放栈的段地址，配合SP/BP做段内寻址 CS 段寄存器 存放代码段的寄存器，配合IP做段内寻址 Flag 标志寄存器 用来存储相关指令的某些执行结果，从而为CPU相关指令提供行为依据 标志寄存器标志位 作用 ZF 零标志位，记录指令执行结果是否为0 PF 奇偶标志位，记录相关指令执行后，所有bit位中1的个数是否为偶数 SF 符号标志位，记录相关指令执行后，结果是否为负 CF 进位标志位，在无符号数运算时，记录了运算结果的最高有效位的进位值 OF 溢出标志位，用于标识指令执行结果是否溢出 DF 方向标识位，在串处理指令中，控制每次操作后si, di的增减 TF 调试标志位，TF=1时，表示进入单步执行 IF 中断允许标志位，表示能否接收外部中断请求，为1时能响应外部中断，反之屏蔽 8086CPU是16位机，但却有20位地址总线（1MB的寻址能力），无法做到简单的一一映射，其合成物理地址的方式如下 通过两个地址16位地址合成一个20位地址：物理地址=基址+偏址。 在8086CPU中物理地址合成公式为：物理地址=段地址*16+偏移地址。 内存并没有分段，段的划分来自CPU。 将若干的地址连续的内存单元看做一个段，用段地址*16定位段的基址，用偏移地址定位段中的内存单元。 CPU如何区分出内存中存放的是数据还是指令呢？ 数据和指定在内存中的存储形式是一致的，都是二进制信息 CPU将CS:IP指向的内存单元中的内容看做指令，将DS:[addr]指向的内存单元中的内容看做数据 SS:SP用于指向栈顶元素，需要注意栈顶越界的问题。 内存单元的描述 完整的描述一个内存单元需要两种信息：内存单元的地址和内存单元的长度。 mov ax, [0]：将ds:[0]的内容送入ax中，内存单元的长度为2字节（即一个字单元，因为是送入到ax）中。 move al, [0]：将ds:[0]的内容送入al中，内存单元的长度为1字节（即一个字节单元）。 转移指令：修改CS或IP的指令成为转移指令。只修改IP时称为段内转移（近转移），同时修改CS和IP的指令称为段间转移（远转移）。 无条件转移指令，如jmp。 jmp不需要转移的目的地址，而是通过相对位移来跳转。 条件转移指令。 循环指令，如loop。 过程 call \u0026 ret 中断 CPU在执行完当前正在执行的指令后，检测到从外部或者内部产生的特殊信息（中断信息），便转去处理这个特殊信息 内中断 除法溢出 单步执行（debug） 执行into 执行int指令 外中断 可屏蔽中断 不可屏蔽中断 中断处理程序入口通过中断向量表（中断类型码）来索引到（存储中断程序地址），对于8086而言，中断向量表存储在内存地址0处。中断向量表中，一个表项占两个字节，高字节存放段地址，低字节存放偏移地址 中断过程 从中断信息中获取中断类型码 标志寄存器入栈 设置标志位TF和IF为0 CS内容入栈 IP内容入栈 根据中断类型码定位到中断程序的入口 ","date":"2021-06-27","objectID":"/posts/asm/:0:0","tags":["汇编"],"title":"汇编入门速记","uri":"/posts/asm/"},{"categories":["anecdotes"],"content":"晦涩的社会心理现象，别具一格的黏性表达。 ","date":"2021-06-20","objectID":"/posts/anecdote/:0:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"格雷欣法则（劣币驱逐良币） 含义：泛指价值不高的东西会把价值较高的东西挤出流通领域。 起源：古时候，因为铸造货币工艺的差别很大，人们就倾向于将那些足值的“良币”收藏起来，并试图将那些成色差的“劣币”转让给他人，这样市场上就同时流通着劣币和良币，且劣币得不到有效地制止和惩罚，如此循环，良币就会被驱逐出市场，质量差的货币反而在市场上流通了。 示例：当你的钱包里既有新钱又有旧钱时，你总是倾向于把那张破旧的先花出去。久而久之，流通中的货币越来越破旧，新钱却没有花出去，仍然呆在钱包里。 ","date":"2021-06-20","objectID":"/posts/anecdote/:1:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"马太效应 马太效应 凡有的，还要加给他，叫他有余；凡没有的，连他所有的也要夺去 —— 太：13·12 含义：只要获得了每一点的成功，就会产生累积优势，使之拥有更大的机会获得更卓越的成就。 起源：有一个贵胄往远方去，要得国回来。便叫了他的十个仆人来，交给他们十锭银子（锭原文作弥拿，一弥拿约银十两），说，你们去作生意，直等我回来。他本国的人却恨他，打发使者随后去说，我们不愿意这个人作我们的王。他既得国回来，就吩咐叫那领银子的仆人来，要知道他们作生意赚了多少。头一个上来说，主阿，你的一锭银子，已经赚了十锭。主人说，好良善的仆人。你既在最小的事上有忠心，可以有权柄管十座城。第二个来说，主阿，你的一锭银子，已经赚了五锭。主人说，你也可以管五座城。又有一个来说，主阿，看哪，你的一锭银子在这里，我把他包在手巾里存着。我原是怕你，因为你是严厉的人。没有放下的还要去拿，没有种下的还要去收。主人对他说，你这恶仆，我要凭你的口，定你的罪。你既知道我是严厉的人，没有放下的还要去拿，没有种下的还要去收。为什么不把我的银子交给银行，等我来的时候，连本带利都可以要回来呢？就对旁边站着的人说，夺过他这一锭来，给那有十锭的。他们说，主阿，他已经有十锭了。主人说，我告诉你们，凡有的，还要加给他。没有的，连他所有的，也要夺过来。——路：19·12-26 示例：强者愈强弱者越弱，贫富差距大，赢家通吃。 ","date":"2021-06-20","objectID":"/posts/anecdote/:2:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"幸存者偏差 幸存者偏差 含义：当取得资讯的渠道，仅来自于幸存者时，此资讯可能会与实际情况存在偏差。 起源：1941年,第二次世界大战中，美国哥伦比亚大学统计学沃德教授应军方要求，利用其在统计方面的专业知识来提供关于《飞机应该如何加强防护，才能降低被炮火击落的几率》的相关建议。沃德教授针对联军的轰炸机遭受攻击后返回营地的轰炸机数据，进行研究后发现：机翼是最容易被击中的位置,机尾则是最少被击中的位置。沃德教授的结论是“我们应该强化机尾的防护”，而军方指挥官认为“应该加强机翼的防护，因为这是最容易被击中的位置”。教授认为： 统计的样本，只涵盖平安返回的轰炸机； 被多次击中机翼的轰炸机，似乎还是能够安全返航； 并非是机尾不易被击中，而是因为机尾被击中的飞机早已无法返航，寥寥几架返航的飞机都依赖引擎尚好。 示例：邻家小孩啥都比你强；炒股的大部分人赚得盆满钵满；SLA计算工具的可靠性（当系统由于断电等缘故大范围崩溃时，SLA计算工具也不work了，此时的不可用时间未被统计到）。 ","date":"2021-06-20","objectID":"/posts/anecdote/:3:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"路径依赖 马屁股决定火箭推进器 含义：人类社会中的技术演进或制度变迁均有类似于物理学中的惯性，即一旦进入某一路径（无论是“好”还是“坏”）就可能对这种路径产生依赖，并在此基础上进行自我强化，初始时的细微差异会被放大，从而不成比例地引发了后来的境况，即过去的决定对未来产生较强的影响。 示例1——键盘的构造：肖尔斯发明的打字机，首先是按字母顺序排列，后来发现，经过专业训练的打字员正常击键时老是出故障，因为打字员打得太快，使得支撑键盘的机械杆相互碰撞产生故障，反而严重影响了打字速度。有一位工程师提议，打字机绞键的原因，一方面是字键的弹回速度慢，另一方面也是打字员的击键速度太快了，由于当时的机械工艺水平，无法再提高字键弹回的速度。他想，为什么不想法降低打字员的击键速度呢？而降低打字员的击键速度，最简单的方法给打字制造难度，打乱26个字母的排列顺序，把较常用的字母摆在较笨拙的手指下。比如，字母“O”是英语中第三个使用频率最高的字母，但却把它放在右手的无名指下；字母“S”和“A”，也是使用频率很高的字母，却被交给了最笨拙的左手无名指和小指来击打。同样理由，使用频率较低的“V”“J”“U”等字母却由最灵活的食指来负责。这样，这种毫无规律可寻专门为增加打字难度的打字机键盘设计出来了，并且逐渐定型下来。后来，即使制造工艺的发展，字键的弹回的速度远大于打字员的击键速度，但是键盘字母顺序却无法改动，这种“QWERTY”式组合的键盘就一直沿用至今。后来也有许多人发明了更方便科学的键盘，但是一直没有推广开来。 示例2——火箭推进器与马屁股：如果火箭需要运输更大的重量，就要直径更大、箭体更长，但火箭的推进器直径，美国国家的标准是四英尺八点五英寸，这个尺寸的选定的依赖链如下： 当时火箭推进器，是通过火车来的运输的，火箭推进器的大小由火车的两轨之间的距离决定； 火车轨道的距离参照英国老路上的辙迹； 辙迹间距是罗马战车的两个轮子之间的距离； 战车轮子间据依赖于拉动战车两匹马的屁股宽度。 ","date":"2021-06-20","objectID":"/posts/anecdote/:4:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"肥皂水效应 含义：是将批评夹在赞美中，将对他人的批评夹裹在前后肯定的话语之中，减少批评的负面效应，使被批评者愉快地接受对自己的批评。 起源：1923年，约翰·卡尔文·柯立芝成为美国总统，他有一位漂亮的女秘书，人虽长得很好，但工作中却常因粗心而出错。一天早晨，柯立芝看见秘书走进办公室，便对她说：“今天你穿的这身衣服真漂亮，正适合你这样漂亮的小姐。”这句话出自柯立芝口中，简直让女秘书受宠若惊。柯立芝接着说：“但也不要骄傲，我相信你同样能把公文处理得像你一样漂亮的。”果然从那天起，女秘书在处理公文时很少出错了。 解释：你见过理发师给人刮胡子吗？他要先给人涂些肥皂水，为什么呀，就是为了刮起来使人不觉痛。 ","date":"2021-06-20","objectID":"/posts/anecdote/:5:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"旁观者效应 旁观者效应 含义：在紧急情况下，一个人在有其他人在场时，出手帮助之机会降低，援助的几率与旁观者人数负相关。换句话说，旁观者数量越多，他们当中任何一人进行援助之机会越低。 起源：1964年，纽约昆斯区，28岁的Kitty Genovese在经受了长达35分钟的性侵犯后最终被谋杀致死，共有38个本地区人性正常的居民经过，但没有一人提供帮助。 示例：项目中如果将责任明确到个人时，其可以完成的很好，但是如果没有明确的责任划分，项目往往会停滞不前，所谓三个和尚没水吃。比如明知陈旧代码存在问题，我们总是在假设别人会来修补这些问题。如果这些问题出现在我们的代码库中，我们很可能对之无动于衷，因为“这事儿跟我无关”。程序员对这样的问题通常的反应：这是别的程序员造成的问题，我才不管呢。这种“这事儿跟我无关”的态度很流行。 ","date":"2021-06-20","objectID":"/posts/anecdote/:6:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"拆屋效应 含义：你想让某人达成你的一个对你有利要求，如果你直接说出与自己的要求，那么你的要求也许会被对方压制下去，最后形成了一个“折衷”的结果。这个提议实际上没有达到你预期的效果。但是，如果你提一个相对自己想要的结果更大的要求，那么对方也会调和，但是你可以步步退让，最后退到自己的要求上，就能达到效果。其最大的好处在于让人无法拒绝你 起源：鲁迅（周树人）在1927年所写的《无声的中国》中写道：“中国人的性情总是喜欢调和、折中的，譬如你说，这屋子太暗，说在这里开一个天窗，大家一定是不允许的。但如果你主张拆掉屋顶，他们就会来调和，愿意开天窗了。 示例：《左耳听风》中提到 当你面对做不到的需求时，给出另一个你可以做到的方案，而不是把对方的方案直接回绝掉 。 当你面对过于复杂的需求时，我不说我不能完全满足你，但我说我可以部分满足你 。 我不能说不，但是我要有条件地说是。 ","date":"2021-06-20","objectID":"/posts/anecdote/:7:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"棘轮效应 棘轮(ratchet) 由俭入奢易 由奢入俭难 ——司马光《训俭示康》 含义：人的消费习惯形成之后有不可逆性，即易于向上调整，而难于向下调整。尤其是在短期内消费是不可逆的，其习惯效应较大。这种习惯效应，使消费取决于相对收入，即相对于自己过去的高峰收入。消费者易于随收入的提高增加消费，但不易于收入降低而减少消费，以致产生有正截距的短期消费函数。 示例：一般来讲，领导在年初为下属设置业绩目标，在年底通过一些关键指标（KPI）来度量下属的表现。下属的表现主要取决于其能力和努力程度。这两个都不易被领导所见。领导往往根据对下属能力和努力程度的预估为其设置业绩目标。为提供激励，往往许诺超额重奖。下属为了争取奖金，加倍努力。年底业绩超额，奖金兑现，领导下属皆大欢喜。来年开工，领导心想，去年业绩这么好，可见这下属能力不一般，今年他的业绩目标就调高一点。面对更高的业绩要求，下属发现同样的努力程度今年刚好达标，想拿奖金，只能比去年更努力。下属今年的优异表现推高了领导来年对其的业绩期望，导致下属为拿奖金所付出的努力越来越高，这就是职场中的棘轮效应。给定这个现象，聪明的下属努力时，不仅考虑它对今年奖金的影响，还考虑它对明年领导期望的影响。有了动态的眼光，才能用最少的努力换来最大的收获。 ","date":"2021-06-20","objectID":"/posts/anecdote/:8:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"眼镜蛇效应 含义：指针对某问题的解决方案，反而使得该问题恶化。 起源：英国政府计划要减少眼镜蛇的数量，因而颁布法令说每打死一条眼镜蛇都可以领取赏金。然而印度人为了赏金反而开始养殖眼镜蛇。当英国政府意识到这种情况而取消赏金后，养殖蛇的人把蛇都放了；放出去的蛇继而大量繁殖，结果眼镜蛇族群数量不减反增。 ","date":"2021-06-20","objectID":"/posts/anecdote/:9:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"墨菲定律 面包落地的时候，永远是抹黃油的一面着地 含义：指的是任何一个事件，只要具有大于零的几率，就可确定它终有一天会发生。墨菲定律的原句是：如果有两种或两种以上的方式去做某件事情，而其中一种选择方式将导致灾难，则必定有人会做出这种选择，常解释为担心的事情总会发生，凡是会出错的事一定会出错。 示例：一粒米掉入轮盘上任意一点的概率是0，但落在轮盘上某点是个必然事件。 ","date":"2021-06-20","objectID":"/posts/anecdote/:10:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"羊群效应 含义：指盲目从众跟风。 起源：在一个竞争非常激烈的行业上，而且这个行业上有一个领先者（领头羊）占据了主要的注意力，那么整个羊群就会不断摹仿这个领头羊的一举一动，领头羊到哪里去吃草，其它的羊也去哪里吃草。 他用：zookeeper作为分布式锁，大量客户端注册监听事件抢锁，当锁被释放以后，会向所有的监听器发送消息，引起羊群效应。此处的羊群效应含义有所不同，指代信息风暴。 ","date":"2021-06-20","objectID":"/posts/anecdote/:11:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"皮格马利翁效应 含义：指人在被赋予更高期望以后，他们会表现的更好的一种现象。 起源：希腊神话故事里面的一位名为皮格马利翁的雕刻家，他爱上了自己用象牙雕刻出来的女神雕像，由于他每天对着雕像说话，最后那座女神雕像变成一位真正的女神。 示例：内心常常带着负面期望的人们将会失败；而内心常常带着正面期望的人们将会成功。 ","date":"2021-06-20","objectID":"/posts/anecdote/:12:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"海格力斯效应 以彼之道，还施彼身 含义：人与人之间或群体之间存在的冤冤相报、致使仇恨越来越深的社会心理效应。 起源：希腊神话故事中有位英雄大力士，叫海格力斯，一天，他走在坎坷不平的路上，看见脚边有个像鼓起的袋子样的东西，很难看，海格力斯便踩了那东西一脚。谁知那东西不但没被海格力斯一脚踩破，反而膨胀起来，并成倍成倍地加大，这激怒了英雄海格力斯。他顺手操起—根碗口粗的木棒砸那个怪东西，好家伙，那东西竟膨胀到把路也堵死了。海格力斯奈何不了他，正在纳闷，一位圣者走到海格力斯跟前对他说：“朋友．快别动它了，忘了它，离它远去吧。它叫仇恨袋，你不惹它，它便会小如当初；你若侵犯它，它就会膨胀起来与你敌对到底。”仇恨正如海格力斯所遇到的这个袋子，开始很小，如果你忽略它，矛盾化解，它会自然消失；如果你与它过不去，加恨于它，它会加倍地报复。 ","date":"2021-06-20","objectID":"/posts/anecdote/:13:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"破窗效应 破窗效应 含义：环境中的不良现象如果被放任存在，会诱使人们仿效，甚至变本加厉。 起源：以一幢有少许破窗的建筑为例，如果那些窗不被修理好，可能将会有破坏者破坏更多的窗户。面对“第一扇破窗”，我们常常自我暗示：窗是可以被打破的，没有惩罚。这样想着，不知不觉，我们就成了第二双手、第三双手……“环境早就脏了，我扔的这点儿垃圾根本起不到关键性作用”、“反正也不是我先这么做的”，不少人会这样为自己辩解。 示例： 一面墙，如果出现一些涂鸦没有被清洗掉，很快的，墙上就布满了乱七八糟、不堪入目的东西； 一条人行道有些许纸屑，不久后就会有更多垃圾，最终人们会视若理所当然地将垃圾顺手丢弃在地上； 出轨只有零次和无数次。 ","date":"2021-06-20","objectID":"/posts/anecdote/:14:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"刻板印象 你以为你以为的就是你以为的吗？ 含义：人类对于某些特定类型人、事或物的一种概括的看法，看法可能是来自于同一类型的人事物之中的某一个个体给旁人的观感。 类比：晕轮效应是指人们对他人的认知首先根据初步印象，然后再从这个印象推论出认知对象的其他特质，以点概面（由局部到整体）。 ","date":"2021-06-20","objectID":"/posts/anecdote/:15:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"边际效用递减 边际效用递减 含义：在其他商品的消费数量保持不变的条件下，当一个人连续消费某种物品时，随着所消费的该物品的数量增加，其总效用（total utility）虽然相应增加，但物品的边际效用（marginalutility），即每消费一个单位的该物品，其所带来的效用的增加量）有递减的趋势。 起源：一个人右手举着300克的砝码，这时在其左手上放305克的砝码，他并不会觉得有多少差别，直到左手砝码的重量加至306克时才会觉得有些重;如果右手举着600克，这时左手上的重量要达到612克才能感觉到重了。也就是说，原来的砝码越重，后来就必须加更大的量才能感觉到差别（贝勃定律）。 示例： 当人经历强烈的刺激后，再施予的刺激对他（她）来说也就变得微不足道。就心理感受来说，第一次大刺激能冲淡第二次的小刺激。比如，原本一元钱的报纸变成了十元一份，你定会感到无法接受；而原本10000元的电脑涨了100元，你一定不会有什么大的反应。 你很喜欢巧克力，在吃第一块巧克力的时候会获得一种味蕾和精神上的满足感，这就是经济学家所说的“效用”。但是随着你越吃越多，你所获得的满足感会变得不那么强烈渐渐趋于平淡。第五块巧克力带给你的满足感远不及第一块，吃到第十块的时候，你甚至感到厌倦，更别说第十五块第二十块了。新增加的每一块糖所带来的满足感就是边际效用，“边际”的意思是事物的边缘，巧克力效用的边际是最后一块巧克力的效用。 ","date":"2021-06-20","objectID":"/posts/anecdote/:16:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"帕累托优化 婚前状态 婚后状态(预想) 含义：在一个系统中，存在一种分配状态，在不损失所有人的利益下，至少提升一人利益，此分配方向称为帕累托优化。在不损失所有人的利益下，无法找到一种策略使得至少一人利益提升时，系统达到帕累托最优。 本质：多目标优化的资源分配问题 ","date":"2021-06-20","objectID":"/posts/anecdote/:17:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"损失厌恶 损失厌恶 含义：人们面对同样数量的收益和损失时，认为损失更加令他们难以忍受。损失带来的负效用为收益正效用的2至2.5倍。 实验 实验团体先行持有1000单位的现金。在此基础上做出选择。 A.50%的概率将持有的现金增加为2000。 B.100%的概率将持有的现金增加为1500。 此实验中，被实验团体的16%选择了A，84%选择了B。 同实验团体先行持有2000单位的现金。在此基础上做出选择。 C.50%的概率损失1000单位现金。 D.100%的概率损失500单位现金。 此实验中，同实验团体的69%选择了C，31%选择了D。 ","date":"2021-06-20","objectID":"/posts/anecdote/:18:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"沉没成本 含义：指发生在过去，但对当前决策无关的成本，来自于对过去投入的执着，以此来说服自己。也是作用在人的心理因素层面，心理成本意识大过本身付出的经济成本。 示例：即便本行干不下去也不愿意转行；即便没有爱了也不愿意分手；来都来了… ","date":"2021-06-20","objectID":"/posts/anecdote/:19:0","tags":["思辨"],"title":"有趣的经济学/心理学效应","uri":"/posts/anecdote/"},{"categories":["anecdotes"],"content":"一本让创意有粘性的好书","date":"2021-06-19","objectID":"/posts/stickiness/","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"让创意具有黏性，打破知识的诅咒。 ","date":"2021-06-19","objectID":"/posts/stickiness/:0:0","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"知识的诅咒 一旦我们知道某样东西，我们就会发现很难想象不知道它是什么样子。我们的知识诅咒了我们，对于我们来说，同别人分享我们的知识变得很困难，因为我们不易重造我们听众的心境。 ","date":"2021-06-19","objectID":"/posts/stickiness/:1:0","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"黏性创意的六大原则 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:0","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"简约 寻找核心，从中提炼，切忌淹没导语。 谚语：从丰富的经验中提炼出的短句。 通过类比化繁为简。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:1","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"意外 吸引注意力，并使注意力持久。 缺口理论：知识的缺口产生好奇心。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:2","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"具体 “具体“的辨别：能够凭感觉审视某样东西。 例子，寓言，故事是具体的，便于理解，易于流传。 语言经常是抽象的，但生活不是。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:3","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"可信 细节与数据会增强事物的可信度。 可获得性偏差：直觉往往会由于一些共同的偏差造成缺陷。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:4","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"情感 当我面对人群的时候，我束手无策；但如果面对的只是一个人，我便有了办法。 ——特蕾莎修女 理性会抑制情感的自然流露，感性促使人们产生共情。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:5","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"故事 宏观定义：能在脑海中产生画面的事物。 故事以更少的直觉，更多戏剧化的方式让我们参与其中。 故事的魔力就在于如果自己的某些经历与故事中的主人公相似，那么就会在大脑中产生深深的共鸣，依靠着这种共鸣，故事就可以产生巨大的传播力。 ","date":"2021-06-19","objectID":"/posts/stickiness/:2:6","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"},{"categories":["anecdotes"],"content":"书评扩展 ","date":"2021-06-19","objectID":"/posts/stickiness/:3:0","tags":["思辨"],"title":"《粘住》有感","uri":"/posts/stickiness/"}]