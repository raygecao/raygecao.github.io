<!doctype html><html lang=zh-CN><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=X-UA-Compatible content="IE=edge, chrome=1"><title>SFT 指令数据生产 - 拾遗之途</title><meta name=Description content><meta property="og:url" content="https://raygecao.github.io/posts/data-generator/">
<meta property="og:site_name" content="拾遗之途"><meta property="og:title" content="SFT 指令数据生产"><meta property="og:description" content="数据质量决定了模型的质量，模型需要大量指令数据进行微调以提升效果。"><meta property="og:locale" content="zh-CN"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-04T14:23:13+08:00"><meta property="article:modified_time" content="2024-05-04T14:23:13+08:00"><meta property="article:tag" content="Data"><meta property="article:tag" content="Llm"><meta property="og:image" content="https://raygecao.github.io/posts/data-generator/featured-image.jpg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://raygecao.github.io/posts/data-generator/featured-image.jpg"><meta name=twitter:title content="SFT 指令数据生产"><meta name=twitter:description content="数据质量决定了模型的质量，模型需要大量指令数据进行微调以提升效果。"><meta name=application-name content="拾遗之途"><meta name=apple-mobile-web-app-title content="拾遗之途"><meta name=google-site-verification content="OzXJ6LYRvjcbIN3W9Dn7TA7in9F1AYZjd2pciC8sexc"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://raygecao.github.io/posts/data-generator/><link rel=prev href=https://raygecao.github.io/posts/busalgo/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"SFT 指令数据生产","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/raygecao.github.io\/posts\/data-generator\/"},"image":[{"@type":"ImageObject","url":"https:\/\/raygecao.github.io\/posts\/data-generator\/featured-image.jpg","width":1794,"height":586}],"genre":"posts","keywords":"data, llm","wordcount":2320,"url":"https:\/\/raygecao.github.io\/posts\/data-generator\/","datePublished":"2024-05-04T14:23:13+08:00","dateModified":"2024-05-04T14:23:13+08:00","publisher":{"@type":"Organization","name":"raygecao"},"author":{"@type":"Person","name":"raygecao"},"description":""}</script></head><body header-desktop header-mobile><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":""==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:""==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=拾遗之途>拾遗之途</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder=搜索文章标题或内容... id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin"></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题><i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=拾遗之途>拾遗之途</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder=搜索文章标题或内容... id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=搜索><i class="fas fa-search fa-fw"></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=清空><i class="fas fa-times-circle fa-fw"></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin"></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>取消</a></div><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">SFT 指令数据生产</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>raygecao</a></span>&nbsp;<span class=post-category>收录于 <a href=/categories/%E5%B7%A5%E4%BD%9C%E9%A1%B9%E7%9B%AE/><i class="far fa-folder fa-fw"></i>工作项目</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2024-05-04>2024-05-04</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 2320 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 5 分钟&nbsp;<span id=/posts/data-generator/ class=leancloud_visitors data-flag-title="SFT 指令数据生产">
<i class="far fa-eye fa-fw"></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;次阅读
</span>&nbsp;</div></div><div class=featured-image><img class=lazyload src=/svg/loading.min.svg data-src=/posts/data-generator/featured-image.jpg data-srcset="/posts/data-generator/featured-image.jpg, /posts/data-generator/featured-image.jpg 1.5x, /posts/data-generator/featured-image.jpg 2x" data-sizes=auto alt=/posts/data-generator/featured-image.jpg title=/posts/data-generator/featured-image.jpg></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#算法算力数据>算法？算力？数据！</a></li><li><a href=#数据生产>数据生产</a><ul><li><a href=#合成数据>合成数据</a></li><li><a href=#sft-数据>SFT 数据</a></li><li><a href=#数据生产方案>数据生产方案</a></li></ul></li><li><a href=#总结>总结</a></li></ul></nav></div></div><div class=content id=content><p>数据质量决定了模型的质量，模型需要大量指令数据进行微调以提升效果。</p><h2 id=算法算力数据>算法？算力？数据！</h2><p>ChatGPT 掀起了大模型的研究浪潮，将传统 AI 推向了 AI 2.0 的时代。AI 的三大要素：算法、算力、数据又再一次活跃在技术领域中。</p><p>ChatGPT 高质量的对话带火了 Transformer 神经网络结构，它所蕴含的两个核心特性大幅提升 NLP 的表达效果：注意力机制使得模型在文本处理时可以关注到句子中的关键信息，前馈神经网络使得模型可以处理长序列的文本。</p><p>算力无疑是这一轮 AI 爆点的关键因素。大模型参数动辄几十亿、成百上千亿，这对显卡的需求大幅提升。虽然已经有数据并行、模型并行、流水线并行等多卡训练的方式来解决训练速度慢、模型过大无法加载在单 GPU 上的问题，
但大模型消耗的大量算力仍然限制了其发展速度。比如最近 Meta 开源的 LLaMa3，训练过程耗费 <strong>770k GPU Hour</strong>，大模型的研发逐渐脱离了普通玩家，一定程度上演变成了算力竞赛。</p><p>考虑到大模型高昂的成本和落地可行性，目前的风向标飘向了垂直领域的小模型，各种对大模型减量不减质的方案方案如雨后春笋般出现，如知识蒸馏、模型压缩、模型剪枝等。这些方式保证了小模型在关键子空间的表达质量，大幅提升模型落地可行性。</p><p>神经网络算法目前的发展平缓，很难一夜研制出一套颠覆性的算法，小模型的商业模式又缓解了算力短缺的问题，这就使得高质量的数据成为了此轮模型战的决定因素。</p><h2 id=数据生产>数据生产</h2><p>自从 LLaMa 开源后，很多小公司不再选择高成本的从头训练大模型的方案，而是选择在一个基础模型上进行训练，可以细化为两个训练过程：</p><ul><li><strong>预训练</strong>：使用无监督的方式对基础模型进行训练，使用的是未经标注的数据。</li><li><strong>SFT</strong>：使用监督方式对预训练的模型进行微调，使用的是已标注的指令数据。</li></ul><h3 id=合成数据>合成数据</h3><p>预训练的数据通常来自于某一领域的非结构化数据，经过清洗后喂给基础模型，此类数据通常来自于网络爬虫、客户知识库等。而 SFT 所需的数据是需要经过标注的，通常是人工标注的数据。考虑到人工标注的成本与效率，合成数据得到了广泛的应用。</p><p>合成数据是运用计算机等手段模拟生成的人造数据，它不包含真实数据，但从统计角度上反映了真实数据。使用合成数据进行 SFT 存在如下好处：</p><ul><li>可以短期内产生大量数据、可以进行自动标注，减少人工成本。</li><li>合成数据避免了真实数据中涉及用户隐私的问题。</li><li>很容易地制造出现实中小概率出现的 corner data。</li></ul><h3 id=sft-数据>SFT 数据</h3><p>SFT 通常采用标注的数据进行监督训练，这类数据被称为指令数据。指令数据本质上是一个问答对，但根据问题的具体程度，将指令分为以下三部分：</p><ul><li><strong>Instruction</strong>：描述具体任务的指令，如“请根据给定内容提炼摘要”。</li><li><strong>Input</strong>：作为指令的补充，如上例中的给定内容。如果指令中的任务已明确，则其应为空。</li><li><strong>Output</strong>：任务的解答。</li></ul><h3 id=数据生产方案>数据生产方案</h3><p>对于 SFT 这种标注数据，通常<strong>借助 LLM 进行批量生成</strong>，下面详细介绍数据生产方案。</p><h4 id=self-instruct>Self-Instruct</h4><p>使用 LLM 生成指令数据的最早的方案是 <a href=https://arxiv.org/pdf/2212.10560 target=_blank rel="noopener noreffer">SELF-INSTRUCT</a>，该方案使用 GPT3 生成了大量指令数据并对 GPT3 进行了微调，指令生成流程如下：</p><figure><a class=lightgallery href=/posts/data-generator/self-instruct.jpg title=/posts/data-generator/self-instruct.jpg data-thumbnail=/posts/data-generator/self-instruct.jpg data-sub-html="<h2>Self-Instruct 流程图</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=self-instruct.jpg data-srcset="/posts/data-generator/self-instruct.jpg, self-instruct.jpg 1.5x, /posts/data-generator/self-instruct.jpg 2x" data-sizes=auto alt=/posts/data-generator/self-instruct.jpg height=400 width=800></a><figcaption class=image-caption>Self-Instruct 流程图</figcaption></figure><ul><li>使用 175 个人工标注的指令数据作为种子数据放入任务池。</li><li>随机选择种子数据，并结合指令生成的 Prompt，使用 LLM 批量生成指令。</li><li>对生成指令进行划分，分为分类指令与生成式指令。</li><li>借助 LLM 生成 Input 与 Output，分类指令采用 Output-first方法，生成式指令采用 Input-first方法。</li><li>对生成的指令进行过滤，如问答长度过滤、RoughL 关联性过滤、黑词过滤等，并将过滤后的指令放入到任务池中进行迭代。</li></ul><p>Self-Instruct 使用一个通用的 prompt 与种子数据，通过 GPT3 生成新的指令，进而生成相关的输入输出。种子数据用作示例，可以便于 LLM 返回标准的解析格式并以续写模拟的方式完成新指令的生成。</p><p>每次随机从种子数据中筛选出几个示例，这种随机性使得生成的指令具有丰富的多样性。</p><p>论文中给出了生产的数据质量，<strong>通过这些高质量的数据对 GPT3 进行微调，评测效果提升了33.1%</strong>。</p><figure><a class=lightgallery href=/posts/data-generator/data-quality.jpg title=/posts/data-generator/data-quality.jpg data-thumbnail=/posts/data-generator/data-quality.jpg data-sub-html="<h2>Self-Instruct 生成数据的质量</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=data-quality.jpg data-srcset="/posts/data-generator/data-quality.jpg, data-quality.jpg 1.5x, /posts/data-generator/data-quality.jpg 2x" data-sizes=auto alt=/posts/data-generator/data-quality.jpg height=200 width=400></a><figcaption class=image-caption>Self-Instruct 生成数据的质量</figcaption></figure><h4 id=alpaca>Alpaca</h4><p>Stanford 在此基础上进行了进一步扩展，使用 GPT3.5 生产出 52k 条数据并对开源的 LLaMa 模型进行微调，训练出 <a href=https://github.com/tatsu-lab/stanford_alpaca target=_blank rel="noopener noreffer">Alpaca</a> 模型，这也是利用大模型训练小模型的经典之作。</p><p>Alpaca 在指令生成上沿袭了 Self-Instruct 的套路，但对此方法进行了简化：</p><ul><li>指令不再区分分类指令与生成式指令。</li><li>使用 GPT3.5 一次性生成指令、输入与输出，避免指令和输入输出的两阶段生成。</li></ul><p>借助于更强大的 LLM，Alpaca 简化的方法生成的策略仍然具备丰富的多样性，仅通过几百美元的成本快速训练出一个性能大幅提升的语言模型。</p><h4 id=chinese-llama-alpaca>Chinese-LLaMA-Alpaca</h4><p><a href=https://github.com/ymcui/Chinese-LLaMA-Alpaca/tree/main target=_blank rel="noopener noreffer">Chinese-LLaMA-Alpaca</a> 将 Alpaca 的数据生产应用到中文领域，借助 GPT3.5 生成中文指令去微调 LLaMa。</p><p>该项目进一步简化的指令生产的流程，相对于 Alpaca 移除掉了种子数据，使用更通用的 prompt 生成多样性的中文指令数据。</p><h4 id=alpagasus>ALPAGASUS</h4><p><a href=https://lichang-chen.github.io/AlpaGasus/ target=_blank rel="noopener noreffer">AlpaGasus</a> 在 Alpaca 指令集基础上，借助 LLM 进行了进一步过滤，将低质量的指令数据过滤掉。</p><p>其主要的思路是将 Alpaca 生成的指令逐条送给 ChatGPT 进行评分，选择评分高的一小批数据对 LLaMa 进行微调。</p><p>使用 LLM 评分过滤的方式将 52k 指令集过滤到 9k，微调后的性能有显著的提升。</p><figure><a class=lightgallery href=/posts/data-generator/alpagasus.png title=/posts/data-generator/alpagasus.png data-thumbnail=/posts/data-generator/alpagasus.png data-sub-html="<h2>alpagasus vs alpaca</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=alpagasus.png data-srcset="/posts/data-generator/alpagasus.png, alpagasus.png 1.5x, /posts/data-generator/alpagasus.png 2x" data-sizes=auto alt=/posts/data-generator/alpagasus.png height=400 width=800></a><figcaption class=image-caption>alpagasus vs alpaca</figcaption></figure><p>除了性能的提升，训练的成本也降低了很多，节省了大量训练时间。这表明数据质量对于 SFT 影响巨大，而数据数量反而对模型性能的影响有限。</p><p>与 AlpaGasus 类似，<a href=https://github.com/zjunlp/EasyInstruct target=_blank rel="noopener noreffer">EasyInstruct</a> 引入了插件式的指令清洗组件，将数据生产与数据清洗流程打通。</p><h4 id=self-qa>Self-QA</h4><p><a href=https://arxiv.org/pdf/2305.11952 target=_blank rel="noopener noreffer">Self-QA</a> 使用无监督的方式代替手工编写的种子指令，将非结构化的数据喂给 LLM，并让其根据此数据上下文生成指令数据。</p><figure><a class=lightgallery href=/posts/data-generator/self-qa.jpg title=/posts/data-generator/self-qa.jpg data-thumbnail=/posts/data-generator/self-qa.jpg data-sub-html="<h2>Self-QA 使用无监督数据生成指令</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=self-qa.jpg data-srcset="/posts/data-generator/self-qa.jpg, self-qa.jpg 1.5x, /posts/data-generator/self-qa.jpg 2x" data-sizes=auto alt=/posts/data-generator/self-qa.jpg height=400 width=800></a><figcaption class=image-caption>Self-QA 使用无监督数据生成指令</figcaption></figure><figure><a class=lightgallery href=/posts/data-generator/qa-prompt.jpg title=/posts/data-generator/qa-prompt.jpg data-thumbnail=/posts/data-generator/qa-prompt.jpg data-sub-html="<h2>指令生成 Prompt</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=qa-prompt.jpg data-srcset="/posts/data-generator/qa-prompt.jpg, qa-prompt.jpg 1.5x, /posts/data-generator/qa-prompt.jpg 2x" data-sizes=auto alt=/posts/data-generator/qa-prompt.jpg height=200 width=400></a><figcaption class=image-caption>指令生成 Prompt</figcaption></figure><h2 id=总结>总结</h2><p>大模型对数据数量和质量提出了进一步要求，合成数据以其低成本、高灵活性在模型微调中扮演了重要的角色。
本文讨论了借助 LLM 进行数据生产的方案，顺着这个思路我们看到 LLM 在智能化、自动化方向上的有着广泛的应用，甚至在某些环节中可以做到自我完善。</p><p>从策略演进的过程我们看到，数据生产一方面依赖于大模型本身能力的提升，另一方面源于对 Prompt 工程的探索。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2024-05-04</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/data/>Data</a>,&nbsp;<a href=/tags/llm/>Llm</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/busalgo/ class=prev rel=prev title=算法量产中的业务算法平台><i class="fas fa-angle-left fa-fw"></i>算法量产中的业务算法平台</a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank></a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=/lib/lightgallery/lightgallery.min.css><script type=text/javascript src=/lib/valine/Valine.min.js></script><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/autocomplete/autocomplete.min.js></script><script type=text/javascript src=/lib/algoliasearch/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/lightgallery/lightgallery.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-thumbnail.min.js></script><script type=text/javascript src=/lib/lightgallery/lg-zoom.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{valine:{appId:"9CFga5qIsIai65AeVF4i0mAH-gzGzoHsz",appKey:"WgXjMQASwreiUmnY5dHWWx0Y",avatar:"mp",el:"#valine",emojiCDN:"https://cdn.jsdelivr.net/npm/emoji-datasource-google@5.0.1/img/google/64/",emojiMaps:{100:"1f4af.png",alien:"1f47d.png",anger:"1f4a2.png",angry:"1f620.png",anguished:"1f627.png",astonished:"1f632.png",black_heart:"1f5a4.png",blue_heart:"1f499.png",blush:"1f60a.png",bomb:"1f4a3.png",boom:"1f4a5.png",broken_heart:"1f494.png",brown_heart:"1f90e.png",clown_face:"1f921.png",cold_face:"1f976.png",cold_sweat:"1f630.png",confounded:"1f616.png",confused:"1f615.png",cry:"1f622.png",crying_cat_face:"1f63f.png",cupid:"1f498.png",dash:"1f4a8.png",disappointed:"1f61e.png",disappointed_relieved:"1f625.png",dizzy:"1f4ab.png",dizzy_face:"1f635.png",drooling_face:"1f924.png",exploding_head:"1f92f.png",expressionless:"1f611.png",face_vomiting:"1f92e.png",face_with_cowboy_hat:"1f920.png",face_with_hand_over_mouth:"1f92d.png",face_with_head_bandage:"1f915.png",face_with_monocle:"1f9d0.png",face_with_raised_eyebrow:"1f928.png",face_with_rolling_eyes:"1f644.png",face_with_symbols_on_mouth:"1f92c.png",face_with_thermometer:"1f912.png",fearful:"1f628.png",flushed:"1f633.png",frowning:"1f626.png",ghost:"1f47b.png",gift_heart:"1f49d.png",green_heart:"1f49a.png",grimacing:"1f62c.png",grin:"1f601.png",grinning:"1f600.png",hankey:"1f4a9.png",hear_no_evil:"1f649.png",heart:"2764-fe0f.png",heart_decoration:"1f49f.png",heart_eyes:"1f60d.png",heart_eyes_cat:"1f63b.png",heartbeat:"1f493.png",heartpulse:"1f497.png",heavy_heart_exclamation_mark_ornament:"2763-fe0f.png",hole:"1f573-fe0f.png",hot_face:"1f975.png",hugging_face:"1f917.png",hushed:"1f62f.png",imp:"1f47f.png",innocent:"1f607.png",japanese_goblin:"1f47a.png",japanese_ogre:"1f479.png",joy:"1f602.png",joy_cat:"1f639.png",kiss:"1f48b.png",kissing:"1f617.png",kissing_cat:"1f63d.png",kissing_closed_eyes:"1f61a.png",kissing_heart:"1f618.png",kissing_smiling_eyes:"1f619.png",laughing:"1f606.png",left_speech_bubble:"1f5e8-fe0f.png",love_letter:"1f48c.png",lying_face:"1f925.png",mask:"1f637.png",money_mouth_face:"1f911.png",nauseated_face:"1f922.png",nerd_face:"1f913.png",neutral_face:"1f610.png",no_mouth:"1f636.png",open_mouth:"1f62e.png",orange_heart:"1f9e1.png",partying_face:"1f973.png",pensive:"1f614.png",persevere:"1f623.png",pleading_face:"1f97a.png",pouting_cat:"1f63e.png",purple_heart:"1f49c.png",rage:"1f621.png",relaxed:"263a-fe0f.png",relieved:"1f60c.png",revolving_hearts:"1f49e.png",right_anger_bubble:"1f5ef-fe0f.png",robot_face:"1f916.png",rolling_on_the_floor_laughing:"1f923.png",scream:"1f631.png",scream_cat:"1f640.png",see_no_evil:"1f648.png",shushing_face:"1f92b.png",skull:"1f480.png",skull_and_crossbones:"2620-fe0f.png",sleeping:"1f634.png",sleepy:"1f62a.png",slightly_frowning_face:"1f641.png",slightly_smiling_face:"1f642.png",smile:"1f604.png",smile_cat:"1f638.png",smiley:"1f603.png",smiley_cat:"1f63a.png",smiling_face_with_3_hearts:"1f970.png",smiling_imp:"1f608.png",smirk:"1f60f.png",smirk_cat:"1f63c.png",sneezing_face:"1f927.png",sob:"1f62d.png",space_invader:"1f47e.png",sparkling_heart:"1f496.png",speak_no_evil:"1f64a.png",speech_balloon:"1f4ac.png","star-struck":"1f929.png",stuck_out_tongue:"1f61b.png",stuck_out_tongue_closed_eyes:"1f61d.png",stuck_out_tongue_winking_eye:"1f61c.png",sunglasses:"1f60e.png",sweat:"1f613.png",sweat_drops:"1f4a6.png",sweat_smile:"1f605.png",thinking_face:"1f914.png",thought_balloon:"1f4ad.png",tired_face:"1f62b.png",triumph:"1f624.png",two_hearts:"1f495.png",unamused:"1f612.png",upside_down_face:"1f643.png",weary:"1f629.png",white_frowning_face:"2639-fe0f.png",white_heart:"1f90d.png",wink:"1f609.png",woozy_face:"1f974.png",worried:"1f61f.png",yawning_face:"1f971.png",yellow_heart:"1f49b.png",yum:"1f60b.png",zany_face:"1f92a.png",zipper_mouth_face:"1f910.png",zzz:"1f4a4.png"},enableQQ:!1,highlight:!0,lang:"zh-cn",pageSize:10,placeholder:"发表评论...",recordIP:!0,visitor:!0}},lightGallery:{actualSize:!1,exThumbImage:"data-thumbnail",hideBarsDelay:2e3,selector:".lightgallery",speed:400,thumbContHeight:80,thumbWidth:80,thumbnail:!0},search:{algoliaAppID:"LX1FO3SR9Q",algoliaIndex:"blog",algoliaSearchKey:"e313cef40f53bf69fe7e593c2369a811",highlightTag:"em",maxResultLength:10,noResultsFound:"没有找到结果",snippetLength:50,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>